{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Livrable 3\n",
    "\n",
    "## 1. Introduction\n",
    "\n",
    "Dans le cadre du projet **Leyenda**, l'objectif de ce livrable est de développer un système automatique capable de générer des légendes descriptives pour des photographies, en utilisant des techniques avancées d'apprentissage profond. Ce livrable s'appuie sur l'un des datasets de référence dans le domaine, **MS COCO**, qui contient des images annotées manuellement avec des descriptions textuelles détaillées.\n",
    "\n",
    "Pour accomplir cette tâche, nous avons conçu un modèle de **réseau de neurones convolutionnel (CNN)** couplé à un **réseau de neurones récurrent (RNN)**. Le CNN est utilisé pour extraire les caractéristiques visuelles des images, tandis que le RNN, notamment un **modèle de type LSTM (Long Short-Term Memory)**, traite ces caractéristiques pour générer des séquences de mots, formant ainsi des légendes.\n",
    "\n",
    "Le modèle suit un pipeline en plusieurs étapes, commençant par le prétraitement des images et des légendes textuelles, suivi de l'entraînement du modèle à partir des données étiquetées. Ce document détaille les différentes phases de développement, l'architecture du modèle, ainsi que les résultats obtenus lors des phases de test. \n",
    "\n",
    "L'objectif final de ce livrable est de présenter une solution automatisée, capable d'être déployée pour une utilisation dans des environnements réels, où elle pourrait analyser de nouvelles images et générer des légendes précises, en s'appuyant sur l'expérience acquise lors de l'entraînement sur MS COCO."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Importation et exploration des données\n",
    "\n",
    "### 2.1. Code d'importation\n",
    "\n",
    "Pour la réalisation de ce projet de captioning d'images, plusieurs librairies sont nécessaires afin de gérer les différentes étapes du traitement des données, de l'entraînement du modèle et de la visualisation des résultats."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "import os, zipfile, random, json, collections, urllib, time\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Configuration et initialisation des répertoires et paramètres"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comme pour les autres Livrables, nous commençont par définir des chemins pour organiser les fichiers et dossiers d'un projet, puis crée les répertoires s'ils n'existent pas déjà. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_path = os.getcwd()\n",
    "\n",
    "paths = {\n",
    "    \"data_dir\": f\"{project_path}/data\",\n",
    "    \"train_images_dir\": f\"{project_path}/data/train2014\",\n",
    "    \"annotations_dir\": f\"{project_path}/data/annotations\",\n",
    "    \"train_images_zip\": f\"{project_path}/data/train2014.zip\",\n",
    "    \"annotations_zip\": f\"{project_path}/data/annotations_trainval2014.zip\",\n",
    "    \"annotation_file\": f\"{project_path}/data/annotations/captions_train2014.json\",\n",
    "    \"tokenizer_file\": f\"{project_path}/tokenizer.json\",\n",
    "    \"checkpoints_path\": f\"{project_path}/checkpoints\",\n",
    "    \"models_path\": f\"{project_path}/models\",\n",
    "}\n",
    "\n",
    "if not os.path.exists(paths[\"data_dir\"]):\n",
    "    os.makedirs(paths[\"data_dir\"])\n",
    "\n",
    "if not os.path.exists(paths[\"checkpoints_path\"]):\n",
    "    os.makedirs(paths[\"checkpoints_path\"])\n",
    "\n",
    "if not os.path.exists(paths[\"models_path\"]):\n",
    "    os.makedirs(paths[\"models_path\"])\n",
    "\n",
    "COCO_URLs = {\n",
    "    \"images\": \"http://images.cocodataset.org/zips/train2014.zip\",\n",
    "    \"annotations\": \"http://images.cocodataset.org/annotations/annotations_trainval2014.zip\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3. Paramètres du modèle\n",
    "\n",
    "Dans cette section, nous définissons plusieurs hyperparamètres essentiels pour l'entraînement du modèle de captioning d'images. Ces hyperparamètres contrôlent différents aspects du modèle et de son processus d'apprentissage.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "batch_size = 32\n",
    "buffer_size = 1000\n",
    "\n",
    "embedding_dim = 256\n",
    "\n",
    "units = 512\n",
    "\n",
    "top_k = 10000\n",
    "vocab_size = top_k + 1\n",
    "\n",
    "features_shape = 2048\n",
    "attention_features_shape = 64\n",
    "\n",
    "epoch_var = tf.Variable(0, name='epoch', dtype=tf.int64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- **`epochs`** : Définit le nombre total d'**époques d'entraînement**. Une époque correspond à un passage complet de l'ensemble de données à travers le modèle. Nous avons fixé ce nombre à `100` pour permettre au modèle de voir suffisamment de données pour apprendre efficacement.\n",
    "\n",
    "- **`batch_size`** : Nombre d'exemples d'entraînement traités à la fois pendant l'entraînement. Ici, nous utilisons des batches de taille `32`, ce qui signifie que le modèle ajuste ses poids après avoir traité 32 exemples d'images et leurs légendes. Un batch plus grand peut accélérer l'entraînement mais nécessite plus de mémoire GPU.\n",
    "\n",
    "- **`buffer_size`** : Ce paramètre détermine la taille du **buffer de mélange** utilisé lors de la création du dataset TensorFlow. Un buffer de `1000` permet un mélange aléatoire des exemples pour éviter les corrélations entre les données d'entraînement et améliorer la généralisation du modèle.\n",
    "\n",
    "- **`embedding_dim`** : Dimension des vecteurs d'**embedding** pour les mots dans les légendes. Les embeddings transforment chaque mot en un vecteur dense de taille `256`, qui capture des informations sémantiques. Plus la dimension est grande, plus le modèle peut capturer des relations complexes entre les mots, mais cela augmente aussi le temps de calcul.\n",
    "\n",
    "- **`units`** : Nombre d'unités dans la **LSTM** (Long Short-Term Memory) du décodeur. Avec `512` unités, nous avons une mémoire cachée de grande capacité pour stocker l'information à long terme pendant la génération des légendes.\n",
    "\n",
    "- **`top_k`** : Ce paramètre limite le vocabulaire aux `10 000` mots les plus fréquents dans le dataset d'entraînement. Cela réduit la complexité du modèle en excluant les mots rares qui n'ont pas d'impact significatif sur la performance globale.\n",
    "\n",
    "- **`vocab_size`** : Taille totale du vocabulaire, fixée à `top_k + 1` pour inclure les mots les plus fréquents et des tokens spéciaux comme `<pad>`, `<start>`, et `<end>`.\n",
    "\n",
    "- **`features_shape`** : Taille des **caractéristiques visuelles** extraites par le modèle CNN (InceptionV3). Ce nombre, `2048`, correspond aux dimensions des vecteurs caractéristiques produits par le modèle de CNN, qui capture les informations importantes des images.\n",
    "\n",
    "- **`attention_features_shape`** : Taille des caractéristiques d'attention utilisée pour modéliser l'importance des différentes parties d'une image. Ici, nous fixons cette valeur à `64`.\n",
    "\n",
    "- **`epoch_var`** : Variable TensorFlow qui garde la trace de l'époque actuelle pendant l'entraînement. Cela permet de reprendre l'entraînement à partir du dernier point sauvegardé si nécessaire."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4. Téléchargement et extraction des données COCO\n",
    "\n",
    "L’un des prérequis essentiels pour entraîner un modèle de captioning est de disposer d'un dataset riche en images et légendes correspondantes. Dans ce projet, nous utilisons le dataset COCO 2014 qui contient un grand nombre d’images d’entraînement et des annotations sous forme de légendes textuelles. Ce bloc de code permet de gérer automatiquement le téléchargement et l'extraction de ces données dans l'environnement de travail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_and_extract(zip_path, extract_dir, url, description):\n",
    "    # Download the zip file if it's not already present\n",
    "    if not os.path.exists(zip_path):\n",
    "        print(f\"Downloading COCO {description} dataset...\")\n",
    "        urllib.request.urlretrieve(url, zip_path)\n",
    "        print(f\"Download of {description} completed.\")\n",
    "    else:\n",
    "        print(f\"The zip file for {description} already exists.\")\n",
    "\n",
    "    # Check and extract if necessary\n",
    "    if not os.path.exists(extract_dir):\n",
    "        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "            print(f\"Extracting {description} files...\")\n",
    "            zip_ref.extractall(os.path.dirname(extract_dir))\n",
    "            print(f\"Extraction of {description} completed.\")\n",
    "    else:\n",
    "        print(f\"The {description} files have already been extracted.\")\n",
    "\n",
    "    # Display the number of extracted files\n",
    "    if os.path.exists(extract_dir):\n",
    "        extracted_files = os.listdir(extract_dir)\n",
    "        print(f\"{len(extracted_files)} files extracted in {extract_dir}\")\n",
    "    else:\n",
    "        print(f\"Error: the extraction folder for {description} does not exist.\")\n",
    "\n",
    "# Download and extract the images and annotations by generating the paths on the fly\n",
    "download_and_extract(\n",
    "    paths[\"train_images_zip\"],\n",
    "    paths[\"train_images_dir\"],\n",
    "    COCO_URLs[\"images\"],\n",
    "    \"images\"\n",
    ")\n",
    "\n",
    "download_and_extract(\n",
    "    paths[\"annotations_zip\"],\n",
    "    paths[\"annotations_dir\"],\n",
    "    COCO_URLs[\"annotations\"],\n",
    "    \"annotations\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5. Création de la correspondance image-légende\n",
    "\n",
    "Ce bloc de code a pour objectif de créer une structure de données associant chaque image à ses légendes correspondantes à partir des annotations fournies dans le fichier JSON du dataset COCO. Cela permettra ensuite d’entraîner un modèle de captioning, où chaque image sera liée à une ou plusieurs légendes descriptives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_image_caption_mapping(annotation_file, image_dir):\n",
    "    # Read the annotation JSON file\n",
    "    with open(annotation_file, 'r') as f:\n",
    "        annotations = json.load(f)\n",
    "    \n",
    "    # Dictionary to associate each image with its captions\n",
    "    image_path_to_caption = collections.defaultdict(list)\n",
    "\n",
    "    # Loop through each annotation\n",
    "    for annotation in annotations['annotations']:\n",
    "        # Add start and end tags to each caption\n",
    "        caption = f\"<start> {annotation['caption']} <end>\"\n",
    "        \n",
    "        # Build the corresponding image path\n",
    "        image_id = annotation['image_id']\n",
    "        image_filename = f'COCO_train2014_{image_id:012d}.jpg'\n",
    "        image_path = os.path.join(image_dir, image_filename)\n",
    "        \n",
    "        # Associate the caption with the image\n",
    "        image_path_to_caption[image_path].append(caption)\n",
    "\n",
    "    return image_path_to_caption\n",
    "\n",
    "# Create the image to caption mapping\n",
    "image_path_to_caption = create_image_caption_mapping(\n",
    "    paths[\"annotation_file\"], \n",
    "    paths[\"train_images_dir\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6. Sélection d'images et de légendes aléatoires\n",
    "\n",
    "Ce bloc de code permet de sélectionner un échantillon aléatoire d'images et leurs légendes associées à partir du dictionnaire d'annotations précédemment généré. Cette étape est utile pour créer un sous-ensemble du dataset complet, souvent nécessaire lorsque l’entraînement est long ou que les ressources sont limitées."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_image_paths_and_captions(image_path_to_caption, num_samples):\n",
    "    # Get all the image paths\n",
    "    image_paths = list(image_path_to_caption.keys())\n",
    "    \n",
    "    # Select a random sample of images\n",
    "    train_image_paths = random.sample(image_paths, num_samples)\n",
    "\n",
    "    # Initialize lists for captions and duplicated image paths\n",
    "    train_captions = []\n",
    "    img_name_vector = []\n",
    "\n",
    "    # Fill the lists based on the annotations\n",
    "    for image_path in train_image_paths:\n",
    "        caption_list = image_path_to_caption[image_path]\n",
    "        # Add each caption to train_captions\n",
    "        train_captions.extend(caption_list)\n",
    "        # Add the image path duplicated according to the number of annotations\n",
    "        img_name_vector.extend([image_path] * len(caption_list))\n",
    "\n",
    "    return img_name_vector, train_captions\n",
    "\n",
    "# Get random image paths and captions\n",
    "img_name_vector, train_captions = get_random_image_paths_and_captions(image_path_to_caption, 5000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_captions), len(img_name_vector))\n",
    "print(train_captions[0])\n",
    "Image.open(img_name_vector[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.7. Création d’un extracteur de caractéristiques d’images\n",
    "\n",
    "La fonction `create_image_feature_extractor()` a pour objectif de créer un modèle capable d'extraire les **caractéristiques visuelles** des images en utilisant un réseau de neurones convolutionnel (CNN) pré-entraîné. Plus précisément, elle utilise le modèle **InceptionV3** pré-entraîné sur le dataset **ImageNet**, tout en excluant la couche de classification finale. Ce modèle d'extraction de caractéristiques est utilisé dans le cadre de tâches de génération de légendes d'images (image captioning), où les informations visuelles sont transformées en un vecteur de caractéristiques compact et riche en informations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_image_feature_extractor():\n",
    "    # Download the pre-trained InceptionV3 model on ImageNet without the top layer\n",
    "    image_model = tf.keras.applications.InceptionV3(include_top=False, weights='imagenet')\n",
    "\n",
    "    # Get the input of the InceptionV3 model\n",
    "    new_input = image_model.input\n",
    "\n",
    "    # Get the output of the last hidden (dense) layer\n",
    "    hidden_layer = image_model.layers[-1].output\n",
    "\n",
    "    # Create a new model that extracts dense features from images\n",
    "    image_features_extract_model = tf.keras.Model(inputs=new_input, outputs=hidden_layer)\n",
    "\n",
    "    return image_features_extract_model\n",
    "\n",
    "# Create the image feature extractor model\n",
    "image_features_extract_model = create_image_feature_extractor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.8. Prétraitement et extraction des caractéristiques des image\n",
    "\n",
    "L'objectif de ce bloc de code est de prétraiter les images, d’extraire leurs caractéristiques à l'aide du modèle InceptionV3, et de sauvegarder ces caractéristiques sous forme de fichiers .npy pour une utilisation ultérieure dans le modèle de captioning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(image_path):\n",
    "    img = tf.io.read_file(image_path)\n",
    "    img = tf.image.decode_jpeg(img, channels=3)\n",
    "    img = tf.image.resize(img, (299, 299))\n",
    "    img = tf.keras.applications.inception_v3.preprocess_input(img)\n",
    "\n",
    "    return img, image_path\n",
    "\n",
    "def save_image_features(batch_features, paths):\n",
    "    for bf, p in zip(batch_features, paths):\n",
    "        path_of_feature = p.numpy().decode(\"utf-8\")\n",
    "\n",
    "        np.save(path_of_feature, bf.numpy())\n",
    "\n",
    "# Preprocess the images\n",
    "encode_train = sorted(set(img_name_vector))\n",
    "\n",
    "# Create a TensorFlow Dataset to process the images in batches\n",
    "image_dataset = tf.data.Dataset.from_tensor_slices(encode_train)\n",
    "image_dataset = image_dataset.map(load_image, num_parallel_calls=tf.data.experimental.AUTOTUNE).batch(16)\n",
    "\n",
    "# Loop through the dataset batch by batch to extract image features\n",
    "for img, path in tqdm(image_dataset):\n",
    "    # Extract the features of the current batch using InceptionV3\n",
    "    batch_features = image_features_extract_model(img)\n",
    "    # Reshape the features from (16, 8, 8, 2048) to (16, 64, 2048)\n",
    "    batch_features = tf.reshape(batch_features, (batch_features.shape[0], -1, batch_features.shape[3]))\n",
    "    # Save the extracted features for each image in the batch\n",
    "    save_image_features(batch_features, path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.9. Prétraitement des légendes et sauvegarde du tokenizer\n",
    "\n",
    "Ce bloc de code s’occupe de convertir les légendes d’entraînement en séquences de tokens, de les normaliser à une longueur commune, et de sauvegarder le tokenizer pour une utilisation ultérieure. Ce processus est essentiel pour préparer les légendes dans un format compréhensible par un modèle de machine learning. Un tokenizer en deep learning est un outil qui convertit du texte brut en une séquence de tokens, généralement des mots ou des sous-mots, afin de préparer les données pour l'entraînement des modèles. Il attribue des identifiants numériques à chaque token, permettant ainsi aux modèles de traiter et d'analyser efficacement le langage naturel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_max_length(tensor):\n",
    "    return max(len(t) for t in tensor)\n",
    "\n",
    "def preprocess_captions(train_captions, top_k=5000):\n",
    "    # Initialize the tokenizer with specific parameters\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "        num_words=top_k, \n",
    "        oov_token=\"<unk>\", \n",
    "        filters=r'!\"#$%&()*+.,-/:;=?@[\\]^_`{|}~ '\n",
    "    )\n",
    "    \n",
    "    # Build the vocabulary based on the captions\n",
    "    tokenizer.fit_on_texts(train_captions)\n",
    "\n",
    "    # Add a special token to pad short captions\n",
    "    tokenizer.word_index['<pad>'] = 0\n",
    "    tokenizer.index_word[0] = '<pad>'\n",
    "\n",
    "    # Convert the captions into sequences of tokens\n",
    "    train_seqs = tokenizer.texts_to_sequences(train_captions)\n",
    "\n",
    "    # Pad the sequences to the maximum length\n",
    "    cap_vector = tf.keras.preprocessing.sequence.pad_sequences(train_seqs, padding='post')\n",
    "\n",
    "    # Calculate the maximum length of the sequences\n",
    "    max_length = calc_max_length(train_seqs)\n",
    "\n",
    "    return cap_vector, tokenizer, max_length\n",
    "\n",
    "def save_tokenizer(tokenizer, save_path):\n",
    "    tokenizer_json = tokenizer.to_json()\n",
    "    with open(save_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(json.dumps(tokenizer_json, ensure_ascii=False))\n",
    "    print(f'Tokenizer saved to {save_path}.')\n",
    "\n",
    "# Preprocess the training captions\n",
    "cap_vector, tokenizer, max_length = preprocess_captions(train_captions, top_k)\n",
    "\n",
    "# Save the tokenizer for later use\n",
    "tokenizer_save_path = paths[\"tokenizer_file\"]\n",
    "save_tokenizer(tokenizer, tokenizer_save_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.10. Création de la correspondance image-légende et division du dataset\n",
    "\n",
    "L’objectif de ce segment de code est de créer une structure de données qui associe chaque image à ses légendes correspondantes, puis de diviser le dataset en ensembles d’entraînement et de validation. Cette étape est essentielle pour évaluer les performances du modèle tout en évitant le surapprentissage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_image_caption_mapping(img_name_vector, cap_vector):\n",
    "    img_to_cap_vector = collections.defaultdict(list)\n",
    "    for img, cap in zip(img_name_vector, cap_vector):\n",
    "        img_to_cap_vector[img].append(cap)\n",
    "    \n",
    "    return img_to_cap_vector\n",
    "\n",
    "def split_dataset(img_to_cap_vector, split_ratio=0.8):\n",
    "    # Retrieve the keys (image paths, without duplicates)\n",
    "    img_keys = list(img_to_cap_vector.keys())\n",
    "    random.shuffle(img_keys)  # Shuffle the keys\n",
    "\n",
    "    # Split the indices according to the defined ratio\n",
    "    slice_index = int(len(img_keys) * split_ratio)\n",
    "    img_name_train_keys, img_name_val_keys = img_keys[:slice_index], img_keys[slice_index:]\n",
    "\n",
    "    # Build the training and validation lists\n",
    "    img_name_train, cap_train = [], []\n",
    "    img_name_val, cap_val = [], []\n",
    "\n",
    "    # Loop for the training set\n",
    "    for imgt in img_name_train_keys:\n",
    "        capt_len = len(img_to_cap_vector[imgt])\n",
    "        img_name_train.extend([imgt] * capt_len)\n",
    "        cap_train.extend(img_to_cap_vector[imgt])\n",
    "\n",
    "    # Loop for the validation set\n",
    "    for imgv in img_name_val_keys:\n",
    "        capv_len = len(img_to_cap_vector[imgv])\n",
    "        img_name_val.extend([imgv] * capv_len)\n",
    "        cap_val.extend(img_to_cap_vector[imgv])\n",
    "\n",
    "    return img_name_train, cap_train, img_name_val, cap_val\n",
    "\n",
    "# Associate images with their captions\n",
    "img_to_cap_vector = create_image_caption_mapping(img_name_vector, cap_vector)\n",
    "\n",
    "# Split the dataset into training and validation sets\n",
    "img_name_train, cap_train, img_name_val, cap_val = split_dataset(img_to_cap_vector, split_ratio=0.8)\n",
    "\n",
    "# Display the sizes of the training and validation sets\n",
    "len(img_name_train), len(cap_train), len(img_name_val), len(cap_val)\n",
    "\n",
    "num_steps = len(img_name_train) // batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.11. Création du dataset d'entraînement avec préchargement des caractéristiques\n",
    "\n",
    "Ce code gère la création d’un dataset TensorFlow à partir des chemins d’images et de leurs légendes. Les images ne sont pas directement chargées mais plutôt leurs caractéristiques extraites (sous forme de fichiers .npy), préalablement générées à l’aide du modèle InceptionV3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_func(img_name, cap):\n",
    "    img_path = img_name.decode('utf-8') + '.npy'\n",
    "    if os.path.exists(img_path):\n",
    "        img_tensor = np.load(img_path)\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"The file {img_path} does not exist.\")\n",
    "    \n",
    "    return img_tensor, cap\n",
    "\n",
    "def create_dataset(img_names, captions, batch_size=batch_size, buffer_size=buffer_size):\n",
    "    # Create the dataset from the images and captions\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((img_names, captions))\n",
    "\n",
    "    # Use map to load the numpy files\n",
    "    dataset = dataset.map(lambda item1, item2: tf.numpy_function(\n",
    "        map_func, [item1, item2], [tf.float32, tf.int32]),\n",
    "        num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "    # Shuffle the data and divide into batches\n",
    "    dataset = dataset.shuffle(buffer_size).batch(batch_size)\n",
    "    dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "# Create the training dataset\n",
    "dataset = create_dataset(img_name_train, cap_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Le modèle\n",
    "\n",
    "### 3.1. Classe : CNN_Encoder\n",
    "\n",
    "La classe CNN_Encoder est un encodeur d'images conçu pour transformer les caractéristiques extraites des images en une représentation adaptée à l'entrée dans un modèle séquentiel, tel qu'un réseau de neurones récurrent (RNN). Dans ce cas, les caractéristiques des images sont déjà extraites par InceptionV3 et ne nécessitent pas un traitement supplémentaire complexe. L'encodeur agit simplement comme un intermédiaire en réduisant la dimensionnalité et en appliquant une couche dense avec activation ReLU et un Dropout pour régulariser l'apprentissage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.keras.utils.register_keras_serializable(package=\"MyCustomModels\")\n",
    "class CNN_Encoder(tf.keras.Model):\n",
    "    # Since the images are already preprocessed by InceptionV3 and represented in compact form,\n",
    "    # the CNN encoder will only pass these features through a dense layer\n",
    "    def __init__(self, embedding_dim, dropout_rate=0.5):\n",
    "        super(CNN_Encoder, self).__init__()\n",
    "        self.fc = tf.keras.layers.Dense(embedding_dim)\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.fc(x)\n",
    "        x = tf.nn.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Classe : BahdanauAttention\n",
    "\n",
    "La classe BahdanauAttention implémente le mécanisme d'attention proposé par Bahdanau et al. en 2015. L'attention permet au modèle de se concentrer sur certaines parties des données d'entrée (dans ce cas, les caractéristiques des images) en fonction de l'état caché courant du décodeur (par exemple, un RNN ou un LSTM). Cela améliore l'apprentissage en pondérant différemment les caractéristiques en fonction de leur pertinence pour générer la légende à chaque étape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.keras.utils.register_keras_serializable(package=\"MyCustomModels\")\n",
    "class BahdanauAttention(tf.keras.Model):\n",
    "    def __init__(self, units, dropout_rate=0.5):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.W1 = tf.keras.layers.Dense(units)\n",
    "        self.W2 = tf.keras.layers.Dense(units)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
    "\n",
    "    def call(self, features, hidden, training=False):\n",
    "        hidden_with_time_axis = tf.expand_dims(hidden, 1)\n",
    "        \n",
    "        attention_hidden_layer = tf.nn.tanh(self.W1(features) + self.W2(hidden_with_time_axis))\n",
    "\n",
    "        score = self.V(attention_hidden_layer)\n",
    "\n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "        attention_weights = self.dropout(attention_weights, training=training)\n",
    "\n",
    "        context_vector = attention_weights * features\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "\n",
    "        return context_vector, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Classe : RNN_Decoder\n",
    "\n",
    "La classe RNN_Decoder implémente un décodeur basé sur un réseau de neurones récurrents (RNN), plus précisément un LSTM (Long Short-Term Memory), utilisé pour générer des séquences de texte à partir de caractéristiques d'images extraites et du vecteur de contexte fourni par un mécanisme d'attention. Cette classe fait partie d'un modèle de captioning d'images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.keras.utils.register_keras_serializable(package=\"MyCustomModels\")\n",
    "class RNN_Decoder(tf.keras.Model):\n",
    "    def __init__(self, embedding_dim, units, vocab_size):\n",
    "        super(RNN_Decoder, self).__init__()\n",
    "        self.units = units\n",
    "\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        # Replacing the GRU layer with LSTM\n",
    "        self.lstm = tf.keras.layers.LSTM(\n",
    "            self.units,\n",
    "            return_sequences=True,\n",
    "            return_state=True,\n",
    "            recurrent_initializer='glorot_uniform'\n",
    "        )\n",
    "\n",
    "        self.fc1 = tf.keras.layers.Dense(self.units)\n",
    "        self.fc2 = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "        self.attention = BahdanauAttention(self.units)\n",
    "\n",
    "    def call(self, x, features, hidden):\n",
    "        context_vector, attention_weights = self.attention(features, hidden[0])\n",
    "\n",
    "        # Passing the current word to the embedding layer\n",
    "        x = self.embedding(x)\n",
    "        \n",
    "        # Concatenating the context vector and the embedding\n",
    "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "\n",
    "        # Passing the concatenated vector to the LSTM\n",
    "        output, hidden_state, cell_state = self.lstm(x, initial_state=hidden)\n",
    "\n",
    "        # Dense layer after LSTM\n",
    "        y = self.fc1(output)\n",
    "\n",
    "        # Reshaping the result\n",
    "        y = tf.reshape(y, (-1, y.shape[2]))\n",
    "\n",
    "        # Final dense layer to obtain the vocabulary prediction\n",
    "        y = self.fc2(y)\n",
    "\n",
    "        return y, [hidden_state, cell_state], attention_weights\n",
    "\n",
    "    def reset_state(self, batch_size):\n",
    "        # For LSTM, initialize two states: hidden state and cell state\n",
    "        return [tf.zeros((batch_size, self.units)), tf.zeros((batch_size, self.units))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4. Création de l'encodeur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the encoder\n",
    "encoder = CNN_Encoder(embedding_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5. Création du décodeur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the decoder\n",
    "decoder = RNN_Decoder(embedding_dim, units, vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6. Optimiseur et fonction de perte\n",
    "\n",
    "Ces éléments jouent un rôle clé dans la phase d'entraînement du modèle de captioning. L'optimiseur ajuste les poids du modèle pour minimiser la perte, et la fonction de perte mesure la différence entre les prédictions du modèle et les légendes réelles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adam optimizer\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "# Loss function\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_mean(loss_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.7. Gestion des checkpoints\n",
    "\n",
    "Ce code permet de sauvegarder et de restaurer l'état du modèle à différents points pendant l'entraînement. Les checkpoints contiennent les poids du modèle (pour l'encodeur et le décodeur), ainsi que l'état de l'optimiseur, ce qui permet de reprendre l'entraînement à partir du dernier point de sauvegarde en cas d'interruption."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_checkpoint(encoder, decoder, optimizer, epoch_var, max_to_keep=5):\n",
    "    ckpt = tf.train.Checkpoint(\n",
    "        encoder=encoder,\n",
    "        decoder=decoder,\n",
    "        optimizer=optimizer,\n",
    "        epoch=epoch_var\n",
    "    )\n",
    "    \n",
    "    ckpt_manager = tf.train.CheckpointManager(\n",
    "        ckpt,\n",
    "        paths[\"checkpoints_path\"],\n",
    "        max_to_keep=max_to_keep\n",
    "    )\n",
    "    \n",
    "    # Restore the latest checkpoint if it exists\n",
    "    if ckpt_manager.latest_checkpoint:\n",
    "        ckpt.restore(ckpt_manager.latest_checkpoint)\n",
    "        start_epoch = epoch_var.numpy()\n",
    "        print(f\"Restoring the latest checkpoint at epoch {start_epoch}.\")\n",
    "    else:\n",
    "        start_epoch = 0\n",
    "        print(\"No checkpoint found, training from scratch.\")\n",
    "    \n",
    "    return ckpt_manager, start_epoch\n",
    "\n",
    "# Set up checkpoints\n",
    "ckpt_manager, start_epoch = setup_checkpoint(encoder, decoder, optimizer, epoch_var)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.8. Fonction : train_step\n",
    "\n",
    "La fonction train_step représente une étape unique d'entraînement du modèle, où une image et sa légende cible sont traitées pour mettre à jour les poids du modèle. Cette étape comprend la propagation avant (encodage des images, génération des légendes), le calcul de la perte, et la rétropropagation pour ajuster les poids à l'aide des gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(img_tensor, target):\n",
    "    loss = 0\n",
    "\n",
    "    # Initialize the hidden state for each batch\n",
    "    hidden = decoder.reset_state(batch_size=target.shape[0])\n",
    "\n",
    "    # Initialize the decoder input with the <start> token for each sequence\n",
    "    dec_input = tf.expand_dims([tokenizer.word_index['<start>']] * target.shape[0], 1)\n",
    "\n",
    "    with tf.GradientTape() as tape:  # Compute gradients to optimize model weights\n",
    "        # Encode the image (extract features with the encoder)\n",
    "        features = encoder(img_tensor)\n",
    "\n",
    "        # Loop through each step of the sequence (each word)\n",
    "        for i in range(1, target.shape[1]):\n",
    "            # Predict the i-th word with the decoder\n",
    "            predictions, hidden, _ = decoder(dec_input, features, hidden)\n",
    "\n",
    "            # Calculate loss for this step (comparison between the truth and the prediction)\n",
    "            loss += loss_function(target[:, i], predictions)\n",
    "\n",
    "            # The correct word is used as input for the next prediction\n",
    "            dec_input = tf.expand_dims(target[:, i], 1)\n",
    "\n",
    "    # Calculate the average loss per sequence\n",
    "    total_loss = loss / int(target.shape[1])\n",
    "\n",
    "    # List of trainable variables (encoder and decoder)\n",
    "    trainable_variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "\n",
    "    # Compute gradients based on the loss\n",
    "    gradients = tape.gradient(loss, trainable_variables)\n",
    "    \n",
    "    # Apply gradients to update the model weights\n",
    "    optimizer.apply_gradients(zip(gradients, trainable_variables))\n",
    "\n",
    "    return loss, total_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.9. Fonction : train_model()\n",
    "\n",
    "La fonction train_model() exécute l'entraînement du modèle de captioning d'images sur un nombre spécifié d'époques. À chaque époque, elle parcourt le dataset d'entraînement, met à jour les poids du modèle à l'aide des gradients, et sauvegarde les checkpoints périodiquement. Cette fonction affiche également la progression et le temps d'exécution pour chaque époque."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(epochs, start_epoch, dataset, num_steps, ckpt_manager, epoch_var):\n",
    "    loss_plot = []\n",
    "\n",
    "    for epoch in range(start_epoch, epochs):\n",
    "        start = time.time()\n",
    "        total_loss = 0\n",
    "\n",
    "        # Update epoch_var\n",
    "        epoch_var.assign(epoch + 1)\n",
    "\n",
    "        # Progress bar for the current epoch\n",
    "        with tqdm(total=num_steps, desc=f\"Epoch {epoch + 1}/{epochs}\", unit=\"batch\") as pbar:\n",
    "\n",
    "            # Iterate over training batches\n",
    "            for batch, (img_tensor, target) in enumerate(dataset):\n",
    "                # Perform a training step\n",
    "                batch_loss, t_loss = train_step(img_tensor, target)\n",
    "                total_loss += t_loss\n",
    "\n",
    "                # Update the progress bar\n",
    "                pbar.set_postfix({\"Batch Loss\": f\"{batch_loss.numpy() / int(target.shape[1]):.4f}\"})\n",
    "                pbar.update(1)\n",
    "\n",
    "            # Save the average loss for the epoch\n",
    "            loss_plot.append(total_loss / num_steps)\n",
    "\n",
    "            # Save checkpoints every 5 epochs\n",
    "            if (epoch + 1) % 5 == 0:\n",
    "                ckpt_manager.save()\n",
    "                print(f'Checkpoint saved at epoch {epoch + 1}')\n",
    "\n",
    "        # Display the average loss and time taken for the epoch\n",
    "        print(f'\\nEpoch {epoch + 1} Loss {total_loss / num_steps:.6f}')\n",
    "        print(f'Time taken for epoch {epoch + 1}: {time.time() - start:.2f} seconds\\n')\n",
    "\n",
    "    return loss_plot\n",
    "\n",
    "# Start the training\n",
    "loss_plot = train_model(epochs, start_epoch, dataset, num_steps, ckpt_manager, epoch_var)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.10. Save models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder.save(os.path.join(paths[\"models_path\"], 'encoder_livrable_3.model.keras'))\n",
    "decoder.save(os.path.join(paths[\"models_path\"], 'decoder_livrable_3.model.keras'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.11. Fonction : plot_metrics()\n",
    "\n",
    "La fonction plot_metrics() est utilisée pour tracer la courbe de la perte d'entraînement au fil des époques. Bien que dans cet exemple seule la perte soit tracée, la fonction est conçue pour permettre l'ajout d'autres courbes, comme la précision ou le score BLEU, si nécessaire."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metrics(loss_plot):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    # Plot the loss curve\n",
    "    plt.plot(loss_plot, label='Training Loss', color='r')\n",
    "    \n",
    "    # Add titles and labels\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Metrics')\n",
    "    plt.title('Training Loss Over Epochs')\n",
    "\n",
    "    # Add a grid for better readability\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Add the legend\n",
    "    plt.legend()\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "\n",
    "# Call the function to display the curves\n",
    "plot_metrics(loss_plot)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Test\n",
    "\n",
    "L’objectif final est d’évaluer et tester notre modèle à partir de nouvelles images inconnues. Tout d’abord, nous évaluons notre modèle en utilisant la fonction evaluate. Cette fonction prend une image en entrée, la pré-traite, et passe les caractéristiques extraites à travers l’encodeur et le décodeur pour générer une légende prédite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(image):\n",
    "    # Initialize the attention plot with zeros\n",
    "    attention_plot = np.zeros((max_length, attention_features_shape))\n",
    "\n",
    "    # Reset the hidden state for the decoder\n",
    "    hidden = decoder.reset_state(batch_size=1)\n",
    "\n",
    "    # Load and preprocess the image\n",
    "    temp_input = tf.expand_dims(load_image(image)[0], 0)  # Load the image and add a batch dimension\n",
    "    img_tensor_val = image_features_extract_model(temp_input)  # Extract features from the image\n",
    "    img_tensor_val = tf.reshape(img_tensor_val, (img_tensor_val.shape[0], -1, img_tensor_val.shape[3]))\n",
    "\n",
    "    # Pass the image features to the encoder\n",
    "    features = encoder(img_tensor_val)\n",
    "\n",
    "    # Initialize the decoder input with the <start> token\n",
    "    dec_input = tf.expand_dims([tokenizer.word_index['<start>']], 0)\n",
    "    result = []\n",
    "\n",
    "    for i in range(max_length):\n",
    "        # Predict the next word\n",
    "        predictions, hidden, attention_weights = decoder(dec_input, features, hidden)\n",
    "\n",
    "        # Save the attention weights\n",
    "        attention_plot[i] = tf.reshape(attention_weights, (-1, )).numpy()\n",
    "\n",
    "        # Get the predicted word ID and append it to the result\n",
    "        predicted_id = tf.argmax(predictions, axis=-1).numpy()[0]\n",
    "        result.append(tokenizer.index_word.get(predicted_id, '<unk>'))  # Use '<unk>' for unknown words\n",
    "\n",
    "        # Check if the predicted word is <end>\n",
    "        if tokenizer.index_word.get(predicted_id) == '<end>':\n",
    "            attention_plot = attention_plot[:len(result), :]  # Trim attention plot to the length of the result\n",
    "            return result, attention_plot\n",
    "\n",
    "        # Update the input for the next word\n",
    "        dec_input = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "    attention_plot = attention_plot[:len(result), :]  # Trim the attention plot to the length of the result\n",
    "    return result, attention_plot\n",
    "\n",
    "def plot_attention(image, result, attention_plot):\n",
    "\n",
    "    temp_image = np.array(Image.open(image))\n",
    "    fig = plt.figure(figsize=(10, 10))\n",
    "    len_result = len(result)\n",
    "    grid_size = int(np.ceil(np.sqrt(len_result)))\n",
    "\n",
    "    for l in range(len_result):\n",
    "        temp_att = np.resize(attention_plot[l], (8, 8))\n",
    "        ax = fig.add_subplot(grid_size, grid_size, l+1)\n",
    "        ax.set_title(result[l])\n",
    "        img = ax.imshow(temp_image)\n",
    "        ax.imshow(temp_att, cmap='gray', alpha=0.6, extent=img.get_extent())\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensuite, nous visualisons les poids d’attention pour chaque mot prédit afin de mieux comprendre quelles parties de l’image influencent chaque mot. Pour ce faire, nous utilisons la fonction display_random_annotation qui sélectionne aléatoirement une image et sa légende réelle, puis génère une légende prédite avec notre modèle. Nous affichons ensuite la légende réelle et la légende prédite, ainsi que la visualisation des poids d’attention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_random_annotation(img_name_val, cap_val, tokenizer, num_samples=1):\n",
    "    for _ in range(num_samples):\n",
    "        # Select a random index\n",
    "        rid = np.random.randint(0, len(img_name_val))\n",
    "        image = img_name_val[rid]\n",
    "\n",
    "        # Generate the real caption by converting token IDs to words\n",
    "        real_caption = ' '.join([tokenizer.index_word[i] for i in cap_val[rid] if i not in [0]])\n",
    "\n",
    "        # Predict the caption using the model\n",
    "        result, attention_plot = evaluate(image)\n",
    "\n",
    "        # Display the real and predicted captions\n",
    "        print(f\"\\nImage ID: {rid}\")\n",
    "        print(f\"Real Caption: {real_caption}\")\n",
    "        print(f\"Prediction Caption: {' '.join(result)}\")\n",
    "\n",
    "        # Visualize the attention weights\n",
    "        plot_attention(image, result, attention_plot)\n",
    "\n",
    "# Call the function to display a random annotation\n",
    "display_random_annotation(img_name_val, cap_val, tokenizer, num_samples=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Conclusion\n",
    "Dans ce notebook, nous avons mis en œuvre un pipeline complet pour la **génération de légendes d'images** en utilisant des techniques avancées de **deep learning**. Le projet repose sur le dataset **COCO 2014**, qui associe des images à des légendes textuelles, et se concentre sur l'entraînement d'un modèle capable de générer des descriptions automatiques d'images.\n",
    "\n",
    "Nous avons d'abord téléchargé et préparé les données en extrayant les images et leurs légendes associées. Les images ont été prétraitées à l'aide du modèle **InceptionV3**, un réseau de neurones convolutionnel pré-entraîné sur ImageNet. Ce modèle a servi à extraire des caractéristiques visuelles compactes et riches en informations. Ces caractéristiques ont ensuite été utilisées comme entrée dans le modèle de génération de légendes.\n",
    "\n",
    "Le traitement des légendes a nécessité l'utilisation d'un **tokenizer** pour convertir les textes en séquences de tokens numériques exploitables par le modèle. Le vocabulaire a été limité aux `10 000` mots les plus fréquents, afin de conserver une taille de modèle gérable tout en capturant les informations sémantiques importantes. Les légendes ont également été complétées (padding) pour avoir une longueur uniforme, facilitant ainsi leur traitement en batchs lors de l'entraînement.\n",
    "\n",
    "Ensuite, nous avons mis en place une architecture de type **Encoder-Décodeur** avec un **mécanisme d'attention Bahdanau**. L'encodeur, basé sur InceptionV3, a été utilisé pour transformer les images en vecteurs de caractéristiques. Le décodeur, une LSTM (Long Short-Term Memory), a ensuite pris ces vecteurs comme entrée pour générer les légendes séquentiellement, un mot à la fois. Le mécanisme d'attention a permis au modèle de se concentrer sur différentes parties de l'image à chaque étape de la génération, améliorant ainsi la qualité des légendes générées.\n",
    "\n",
    "L'entraînement du modèle s'est déroulé sur `100` époques, avec une taille de batch de `32`. La fonction de perte utilisée était la **Sparse Categorical Crossentropy**, qui a permis de mesurer l'écart entre les légendes générées et les légendes réelles. L'optimiseur **Adam** a été employé pour ajuster les poids du modèle, garantissant une convergence efficace. Un système de **checkpoints** a également été mis en place pour sauvegarder régulièrement l'état du modèle, permettant de reprendre l'entraînement en cas d'interruption.\n",
    "\n",
    "Une fois l'entraînement terminé, le modèle a été évalué sur des images non vues. La fonction `evaluate()` a été utilisée pour générer des légendes prédictives à partir de nouvelles images. Ces légendes ont été comparées aux légendes réelles afin de mesurer la performance du modèle. De plus, la fonction de visualisation des **poids d'attention** a permis de comprendre quelles parties de l'image le modèle considérait comme importantes lors de la génération de chaque mot.\n",
    "\n",
    "Enfin, nous avons sauvegardé les poids du modèle et le tokenizer afin de permettre une **réutilisation** lors de l'inférence ou pour un éventuel nouvel entraînement. Cela garantit que les mêmes associations entre les mots et les indices sont utilisées, assurant ainsi la cohérence des légendes générées à partir de nouvelles images.\n",
    "\n",
    "En résumé, ce projet a démontré l'efficacité d'un modèle **Encoder-Décodeur avec attention** pour la génération de légendes d'images. Ce type de modèle est essentiel pour des applications dans des domaines tels que la vision par ordinateur et le traitement du langage naturel. Les applications potentielles incluent des systèmes de description automatique d'images pour les personnes malvoyantes, des moteurs de recherche d'images basés sur les légendes générées, ou encore des assistants virtuels capables de comprendre et décrire des scènes visuelles.\n",
    "\n",
    "Des **perspectives d'amélioration** existent, telles que l'exploration de modèles plus avancés comme les **Transformers**, ou l'augmentation du dataset pour améliorer la généralisation du modèle. En affinant le vocabulaire ou en ajustant les hyperparamètres, il est possible de rendre le modèle plus performant et de minimiser les erreurs dues aux mots inconnus. Ce projet constitue une base solide pour de futures explorations dans le domaine de la génération de légendes d'images et peut être étendu pour intégrer de nouvelles architectures ou de nouveaux cas d'usage."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
