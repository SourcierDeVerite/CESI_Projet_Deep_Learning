{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Livrable 3\n",
    "\n",
    "## 1. Introduction\n",
    "\n",
    "Dans le cadre du projet **Leyenda**, l'objectif de ce livrable est de développer un système automatique capable de générer des légendes descriptives pour des photographies, en utilisant des techniques avancées d'apprentissage profond. Ce livrable s'appuie sur l'un des datasets de référence dans le domaine, **MS COCO**, qui contient des images annotées manuellement avec des descriptions textuelles détaillées.\n",
    "\n",
    "Pour accomplir cette tâche, nous avons conçu un modèle de **réseau de neurones convolutionnel (CNN)** couplé à un **réseau de neurones récurrent (RNN)**. Le CNN est utilisé pour extraire les caractéristiques visuelles des images, tandis que le RNN, notamment un **modèle de type LSTM (Long Short-Term Memory)**, traite ces caractéristiques pour générer des séquences de mots, formant ainsi des légendes.\n",
    "\n",
    "Le modèle suit un pipeline en plusieurs étapes, commençant par le prétraitement des images et des légendes textuelles, suivi de l'entraînement du modèle à partir des données étiquetées. Ce document détaille les différentes phases de développement, l'architecture du modèle, ainsi que les résultats obtenus lors des phases de test. \n",
    "\n",
    "L'objectif final de ce livrable est de présenter une solution automatisée, capable d'être déployée pour une utilisation dans des environnements réels, où elle pourrait analyser de nouvelles images et générer des légendes précises, en s'appuyant sur l'expérience acquise lors de l'entraînement sur MS COCO."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Importation et exploration des données\n",
    "\n",
    "### 2.1. Code d'importation\n",
    "\n",
    "Pour la réalisation de ce projet de captioning d'images, plusieurs librairies sont nécessaires afin de gérer les différentes étapes du traitement des données, de l'entraînement du modèle et de la visualisation des résultats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, zipfile, random, json, collections, urllib, time\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Configuration et initialisation des répertoires et paramètres"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Chemin du projet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_path = os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Définition des chemins pour les données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = {\n",
    "    \"data_dir\": os.path.join(os.getcwd(), \"data\"),\n",
    "    \"train_images_dir\": os.path.join(os.getcwd(), \"data\", \"train2014\"),\n",
    "    \"annotations_dir\": os.path.join(os.getcwd(), \"data\", \"annotations\"),\n",
    "    #JE SAIS QUE T AIME PAS ÇA MAIS VOILA DESOLE\n",
    "    \"train_images_zip\": os.path.join(os.getcwd(), \"data\", \"train2014.zip\"),\n",
    "    \"annotations_zip\": os.path.join(os.getcwd(), \"data\", \"annotations_trainval2014.zip\"),\n",
    "    \"annotation_file\": os.path.join(os.getcwd(), \"data\", \"annotations\", \"captions_train2014.json\")\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Création du répertoire de données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(paths[\"data_dir\"]):\n",
    "    os.makedirs(paths[\"data_dir\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. URL des fichiers du dataset COCO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COCO_URLs = {\n",
    "    \"images\": \"http://images.cocodataset.org/zips/train2014.zip\",\n",
    "    \"annotations\": \"http://images.cocodataset.org/annotations/annotations_trainval2014.zip\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Paramètres du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 30\n",
    "batch_size = 64\n",
    "buffer_size = 1000\n",
    "embedding_dim = 256\n",
    "units = 512\n",
    "top_k = 7500\n",
    "vocab_size = top_k + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Téléchargement et extraction des données COCO\n",
    "\n",
    "L’un des prérequis essentiels pour entraîner un modèle de captioning est de disposer d'un dataset riche en images et légendes correspondantes. Dans ce projet, nous utilisons le dataset COCO 2014 qui contient un grand nombre d’images d’entraînement et des annotations sous forme de légendes textuelles. Ce bloc de code permet de gérer automatiquement le téléchargement et l'extraction de ces données dans l'environnement de travail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_and_extract(zip_path, extract_dir, url, description):\n",
    "    # Téléchargement du fichier zip si non présent\n",
    "    if not os.path.exists(zip_path):\n",
    "        print(f\"Téléchargement du dataset COCO {description} en cours...\")\n",
    "        urllib.request.urlretrieve(url, zip_path)\n",
    "        print(f\"Téléchargement des {description} terminé.\")\n",
    "    else:\n",
    "        print(f\"Le fichier zip des {description} existe déjà.\")\n",
    "\n",
    "    # Vérification et extraction si nécessaire\n",
    "    if not os.path.exists(extract_dir):\n",
    "        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "            print(f\"Extraction des fichiers {description} en cours...\")\n",
    "            zip_ref.extractall(os.path.dirname(extract_dir))\n",
    "            print(f\"Extraction des {description} terminée.\")\n",
    "    else:\n",
    "        print(f\"Les fichiers {description} ont déjà été extraits.\")\n",
    "\n",
    "    # Affichage du nombre de fichiers extraits\n",
    "    if os.path.exists(extract_dir):\n",
    "        extracted_files = os.listdir(extract_dir)\n",
    "        print(f\"{len(extracted_files)} fichiers extraits dans {extract_dir}\")\n",
    "    else:\n",
    "        print(f\"Erreur : le dossier d'extraction des {description} n'existe pas.\")\n",
    "\n",
    "# Téléchargement et extraction des images et annotations en générant les chemins à la volée\n",
    "download_and_extract(\n",
    "    paths[\"train_images_zip\"],\n",
    "    paths[\"train_images_dir\"],\n",
    "    COCO_URLs[\"images\"],\n",
    "    \"images\"\n",
    ")\n",
    "\n",
    "download_and_extract(\n",
    "    paths[\"annotations_zip\"],\n",
    "    paths[\"annotations_dir\"],\n",
    "    COCO_URLs[\"annotations\"],\n",
    "    \"annotations\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5. Création de la correspondance image-légende\n",
    "\n",
    "Ce bloc de code a pour objectif de créer une structure de données associant chaque image à ses légendes correspondantes à partir des annotations fournies dans le fichier JSON du dataset COCO. Cela permettra ensuite d’entraîner un modèle de captioning, où chaque image sera liée à une ou plusieurs légendes descriptives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_image_caption_mapping(annotation_file, image_dir):\n",
    "    # Lire le fichier d'annotation JSON\n",
    "    with open(annotation_file, 'r') as f:\n",
    "        annotations = json.load(f)\n",
    "    \n",
    "    # Dictionnaire pour associer chaque image à ses légendes\n",
    "    image_path_to_caption = collections.defaultdict(list)\n",
    "\n",
    "    # Parcourir chaque annotation\n",
    "    for annotation in annotations['annotations']:\n",
    "        # Ajouter les balises de début et de fin à chaque légende\n",
    "        caption = f\"<start> {annotation['caption']} <end>\"\n",
    "        \n",
    "        # Construire le chemin de l'image correspondant\n",
    "        image_id = annotation['image_id']\n",
    "        image_filename = f'COCO_train2014_{image_id:012d}.jpg'\n",
    "        image_path = os.path.join(image_dir, image_filename)\n",
    "        \n",
    "        # Associer la légende à l'image\n",
    "        image_path_to_caption[image_path].append(caption)\n",
    "\n",
    "    return image_path_to_caption\n",
    "\n",
    "image_path_to_caption = create_image_caption_mapping(\n",
    "    paths[\"annotation_file\"], \n",
    "    paths[\"train_images_dir\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6. Sélection d'images et de légendes aléatoires\n",
    "\n",
    "Ce bloc de code permet de sélectionner un échantillon aléatoire d'images et leurs légendes associées à partir du dictionnaire d'annotations précédemment généré. Cette étape est utile pour créer un sous-ensemble du dataset complet, souvent nécessaire lorsque l’entraînement est long ou que les ressources sont limitées."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_image_paths_and_captions(image_path_to_caption, num_samples=2000):\n",
    "    # Prendre tous les chemins d'images\n",
    "    image_paths = list(image_path_to_caption.keys())\n",
    "    \n",
    "    # Sélectionner un échantillon aléatoire d'images\n",
    "    train_image_paths = random.sample(image_paths, num_samples)\n",
    "\n",
    "    # Initialiser les listes pour les légendes et les chemins d'images dupliqués\n",
    "    train_captions = []\n",
    "    img_name_vector = []\n",
    "\n",
    "    # Remplir les listes en fonction des annotations\n",
    "    for image_path in train_image_paths:\n",
    "        caption_list = image_path_to_caption[image_path]\n",
    "        # Ajouter chaque légende dans train_captions\n",
    "        train_captions.extend(caption_list)\n",
    "        # Ajouter le chemin de l'image dupliqué selon le nombre d'annotations\n",
    "        img_name_vector.extend([image_path] * len(caption_list))\n",
    "\n",
    "    return img_name_vector, train_captions\n",
    "\n",
    "img_name_vector, train_captions = get_random_image_paths_and_captions(image_path_to_caption, 2000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_captions), len(img_name_vector))\n",
    "print(train_captions[0])\n",
    "Image.open(img_name_vector[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.7. Création d’un extracteur de caractéristiques d’images\n",
    "\n",
    "L'extraction des caractéristiques d'une image est une étape cruciale dans un pipeline de captioning d'images. Le modèle InceptionV3, pré-entraîné sur le dataset ImageNet, est couramment utilisé pour cette tâche car il est capable d'extraire des représentations denses et informatives d'images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_image_feature_extractor():\n",
    "    # Téléchargement du modèle InceptionV3 pré-entraîné sur ImageNet sans la couche supérieure\n",
    "    image_model = tf.keras.applications.InceptionV3(include_top=False, weights='imagenet')\n",
    "\n",
    "    # Récupérer l'entrée du modèle InceptionV3\n",
    "    new_input = image_model.input\n",
    "\n",
    "    # Récupérer la sortie de la dernière couche cachée (dense)\n",
    "    hidden_layer = image_model.layers[-1].output\n",
    "\n",
    "    # Créer un nouveau modèle qui extrait les caractéristiques denses des images\n",
    "    image_features_extract_model = tf.keras.Model(inputs=new_input, outputs=hidden_layer)\n",
    "\n",
    "    return image_features_extract_model\n",
    "\n",
    "image_features_extract_model = create_image_feature_extractor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.8. Prétraitement et extraction des caractéristiques des image\n",
    "\n",
    "L'objectif de ce bloc de code est de prétraiter les images, d’extraire leurs caractéristiques à l'aide du modèle InceptionV3, et de sauvegarder ces caractéristiques sous forme de fichiers .npy pour une utilisation ultérieure dans le modèle de captioning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(image_path):\n",
    "    img = tf.io.read_file(image_path)\n",
    "    img = tf.image.decode_jpeg(img, channels=3)\n",
    "    img = tf.image.resize(img, (299, 299))\n",
    "    img = tf.keras.applications.inception_v3.preprocess_input(img)\n",
    "\n",
    "    return img, image_path\n",
    "\n",
    "def save_image_features(batch_features, paths):\n",
    "    for bf, p in zip(batch_features, paths):\n",
    "        path_of_feature = p.numpy().decode(\"utf-8\") \n",
    "\n",
    "        np.save(path_of_feature, bf.numpy())\n",
    "\n",
    "# Pré-traitement des images\n",
    "encode_train = sorted(set(img_name_vector))\n",
    "\n",
    "# Créer un Dataset TensorFlow pour traiter les images par lots\n",
    "image_dataset = tf.data.Dataset.from_tensor_slices(encode_train)\n",
    "image_dataset = image_dataset.map(load_image, num_parallel_calls=tf.data.experimental.AUTOTUNE).batch(16)\n",
    "\n",
    "# Parcourir le dataset batch par batch pour extraire les caractéristiques des images\n",
    "for img, path in tqdm(image_dataset):\n",
    "    # Extraire les caractéristiques du batch actuel avec InceptionV3\n",
    "    batch_features = image_features_extract_model(img)\n",
    "    # Redimensionner les caractéristiques (16, 8, 8, 2048) en (16, 64, 2048)\n",
    "    batch_features = tf.reshape(batch_features, (batch_features.shape[0], -1, batch_features.shape[3]))\n",
    "    # Sauvegarder les caractéristiques extraites pour chaque image dans le batch\n",
    "    save_image_features(batch_features, path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.9. Prétraitement des légendes et sauvegarde du tokenizer\n",
    "\n",
    "Ce bloc de code s’occupe de convertir les légendes d’entraînement en séquences de tokens, de les normaliser à une longueur commune, et de sauvegarder le tokenizer pour une utilisation ultérieure. Ce processus est essentiel pour préparer les légendes dans un format compréhensible par un modèle de machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_max_length(tensor):\n",
    "    return max(len(t) for t in tensor)\n",
    "\n",
    "def preprocess_captions(train_captions, top_k=5000):\n",
    "    # Initialiser le tokenizer avec les paramètres spécifiques\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "        num_words=top_k, \n",
    "        oov_token=\"<unk>\", \n",
    "        filters=r'!\"#$%&()*+.,-/:;=?@[\\]^_`{|}~ '\n",
    "        )\n",
    "    \n",
    "    # Construire le vocabulaire basé sur les légendes\n",
    "    tokenizer.fit_on_texts(train_captions)\n",
    "\n",
    "    # Ajouter un token spécial pour remplir les légendes courtes\n",
    "    tokenizer.word_index['<pad>'] = 0\n",
    "    tokenizer.index_word[0] = '<pad>'\n",
    "\n",
    "    # Convertir les légendes en séquences de tokens\n",
    "    train_seqs = tokenizer.texts_to_sequences(train_captions)\n",
    "\n",
    "    # Remplir les séquences à la longueur maximale\n",
    "    cap_vector = tf.keras.preprocessing.sequence.pad_sequences(train_seqs, padding='post')\n",
    "\n",
    "    # Calculer la longueur maximale des séquences\n",
    "    max_length = calc_max_length(train_seqs)\n",
    "\n",
    "    return cap_vector, tokenizer, max_length\n",
    "\n",
    "def save_tokenizer(tokenizer, save_path):\n",
    "    tokenizer_json = tokenizer.to_json()\n",
    "    with open(save_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(json.dumps(tokenizer_json, ensure_ascii=False))\n",
    "    print(f'Tokenizer sauvegardé dans {save_path}.')\n",
    "\n",
    "# Prétraitement des légendes d'entraînement\n",
    "cap_vector, tokenizer, max_length = preprocess_captions(train_captions, top_k)\n",
    "\n",
    "# Sauvegarder le tokenizer pour un usage ultérieur\n",
    "tokenizer_save_path = os.path.join(os.getcwd(), 'tokenizer.json') #TODO: CHARLES CHANGE ÇA STP\n",
    "save_tokenizer(tokenizer, tokenizer_save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.10. Création de la correspondance image-légende et division du dataset\n",
    "\n",
    "L’objectif de ce segment de code est de créer une structure de données qui associe chaque image à ses légendes correspondantes, puis de diviser le dataset en ensembles d’entraînement et de validation. Cette étape est essentielle pour évaluer les performances du modèle tout en évitant le surapprentissage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_image_caption_mapping(img_name_vector, cap_vector):\n",
    "    img_to_cap_vector = collections.defaultdict(list)\n",
    "    for img, cap in zip(img_name_vector, cap_vector):\n",
    "        img_to_cap_vector[img].append(cap)\n",
    "    \n",
    "    return img_to_cap_vector\n",
    "\n",
    "def split_dataset(img_to_cap_vector, split_ratio=0.8):\n",
    "    # Récupérer les clés (chemins des images, sans doublons)\n",
    "    img_keys = list(img_to_cap_vector.keys())\n",
    "    random.shuffle(img_keys)  # Mélanger les clés\n",
    "\n",
    "    # Diviser les indices selon le ratio défini\n",
    "    slice_index = int(len(img_keys) * split_ratio)\n",
    "    img_name_train_keys, img_name_val_keys = img_keys[:slice_index], img_keys[slice_index:]\n",
    "\n",
    "    # Construire les listes d'entrainement et de validation\n",
    "    img_name_train, cap_train = [], []\n",
    "    img_name_val, cap_val = [], []\n",
    "\n",
    "    # Boucle pour l'ensemble d'entraînement\n",
    "    for imgt in img_name_train_keys:\n",
    "        capt_len = len(img_to_cap_vector[imgt])\n",
    "        img_name_train.extend([imgt] * capt_len)\n",
    "        cap_train.extend(img_to_cap_vector[imgt])\n",
    "\n",
    "    # Boucle pour l'ensemble de validation\n",
    "    for imgv in img_name_val_keys:\n",
    "        capv_len = len(img_to_cap_vector[imgv])\n",
    "        img_name_val.extend([imgv] * capv_len)\n",
    "        cap_val.extend(img_to_cap_vector[imgv])\n",
    "\n",
    "    return img_name_train, cap_train, img_name_val, cap_val\n",
    "\n",
    "# Association des images avec leurs légendes\n",
    "img_to_cap_vector = create_image_caption_mapping(img_name_vector, cap_vector)\n",
    "\n",
    "# Division du dataset en ensembles d'entraînement et de validation\n",
    "img_name_train, cap_train, img_name_val, cap_val = split_dataset(img_to_cap_vector, split_ratio=0.8)\n",
    "\n",
    "# Affichage des tailles des jeux d'entraînement et de validation\n",
    "len(img_name_train), len(cap_train), len(img_name_val), len(cap_val)\n",
    "\n",
    "num_steps = len(img_name_train) // batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.11. Création du dataset d'entraînement avec préchargement des caractéristiques\n",
    "\n",
    "Ce code gère la création d’un dataset TensorFlow à partir des chemins d’images et de leurs légendes. Les images ne sont pas directement chargées mais plutôt leurs caractéristiques extraites (sous forme de fichiers .npy), préalablement générées à l’aide du modèle InceptionV3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Les formes des vecteurs extraits de InceptionV3\n",
    "features_shape = 2048\n",
    "attention_features_shape = 64\n",
    "\n",
    "def map_func(img_name, cap):\n",
    "    img_path = img_name.decode('utf-8') + '.npy'\n",
    "    if os.path.exists(img_path):\n",
    "        img_tensor = np.load(img_path)\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"Le fichier {img_path} n'existe pas.\")\n",
    "    \n",
    "    return img_tensor, cap\n",
    "\n",
    "def create_dataset(img_names, captions, batch_size=batch_size, buffer_size=buffer_size):\n",
    "    # Création du dataset à partir des images et des légendes\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((img_names, captions))\n",
    "\n",
    "    # Utiliser map pour charger les fichiers numpy\n",
    "    dataset = dataset.map(lambda item1, item2: tf.numpy_function(\n",
    "        map_func, [item1, item2], [tf.float32, tf.int32]),\n",
    "        num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "    # Mélanger les données et les diviser en batchs\n",
    "    dataset = dataset.shuffle(buffer_size).batch(batch_size)\n",
    "    dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "# Création du dataset d'entraînement\n",
    "dataset = create_dataset(img_name_train, cap_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Le modèle\n",
    "\n",
    "### 3.1. Classe : CNN_Encoder\n",
    "\n",
    "La classe CNN_Encoder est un encodeur d'images conçu pour transformer les caractéristiques extraites des images en une représentation adaptée à l'entrée dans un modèle séquentiel, tel qu'un réseau de neurones récurrent (RNN). Dans ce cas, les caractéristiques des images sont déjà extraites par InceptionV3 et ne nécessitent pas un traitement supplémentaire complexe. L'encodeur agit simplement comme un intermédiaire en réduisant la dimensionnalité et en appliquant une couche dense avec activation ReLU et un Dropout pour régulariser l'apprentissage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_Encoder(tf.keras.Model):\n",
    "    # Comme les images sont déjà prétraités par InceptionV3 est représenté sous forme compacte\n",
    "    # L'encodeur CNN ne fera que transmettre ces caractéristiques à une couche dense\n",
    "    def __init__(self, embedding_dim, dropout_rate=0.5):\n",
    "        super(CNN_Encoder, self).__init__()\n",
    "        self.fc = tf.keras.layers.Dense(embedding_dim)\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.fc(x)\n",
    "        x = tf.nn.relu(x)\n",
    "        x = self.dropout(x)  # Ajoute un Dropout ici\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Classe : BahdanauAttention\n",
    "\n",
    "La classe BahdanauAttention implémente le mécanisme d'attention proposé par Bahdanau et al. en 2015. L'attention permet au modèle de se concentrer sur certaines parties des données d'entrée (dans ce cas, les caractéristiques des images) en fonction de l'état caché courant du décodeur (par exemple, un RNN ou un LSTM). Cela améliore l'apprentissage en pondérant différemment les caractéristiques en fonction de leur pertinence pour générer la légende à chaque étape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention(tf.keras.Model):\n",
    "    def __init__(self, units, dropout_rate=0.5):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.W1 = tf.keras.layers.Dense(units)\n",
    "        self.W2 = tf.keras.layers.Dense(units)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
    "\n",
    "    def call(self, features, hidden, training=False):\n",
    "        hidden_with_time_axis = tf.expand_dims(hidden, 1)\n",
    "        attention_hidden_layer = tf.nn.tanh(self.W1(features) + self.W2(hidden_with_time_axis))\n",
    "        score = self.V(attention_hidden_layer)\n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "\n",
    "        # Ajout de Dropout sur les poids d'attention\n",
    "        attention_weights = self.dropout(attention_weights, training=training)\n",
    "\n",
    "        context_vector = attention_weights * features\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "\n",
    "        return context_vector, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Classe : RNN_Decoder\n",
    "\n",
    "La classe RNN_Decoder implémente un décodeur basé sur un réseau de neurones récurrents (RNN), plus précisément un LSTM (Long Short-Term Memory), utilisé pour générer des séquences de texte à partir de caractéristiques d'images extraites et du vecteur de contexte fourni par un mécanisme d'attention. Cette classe fait partie d'un modèle de captioning d'images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN_Decoder(tf.keras.Model):\n",
    "    def __init__(self, embedding_dim, units, vocab_size):\n",
    "        super(RNN_Decoder, self).__init__()\n",
    "        self.units = units\n",
    "\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        # Remplacement de la couche GRU par LSTM\n",
    "        self.lstm = tf.keras.layers.LSTM(self.units,\n",
    "                                         return_sequences=True,\n",
    "                                         return_state=True,\n",
    "                                         recurrent_initializer='glorot_uniform')\n",
    "\n",
    "        # Couche dense qui aura pour entrée la sortie du LSTM\n",
    "        self.fc1 = tf.keras.layers.Dense(self.units)\n",
    "        # Dernière couche dense\n",
    "        self.fc2 = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "        self.attention = BahdanauAttention(self.units)\n",
    "\n",
    "    def call(self, x, features, hidden):\n",
    "        # L'attention est défini par un modèle à part\n",
    "        context_vector, attention_weights = self.attention(features, hidden[0])\n",
    "\n",
    "        # Passage du mot courant à la couche embedding\n",
    "        x = self.embedding(x)\n",
    "        \n",
    "        # Concaténation du vecteur de contexte et de l'embedding\n",
    "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "\n",
    "        # Passage du vecteur concaténé à la LSTM\n",
    "        output, hidden_state, cell_state = self.lstm(x, initial_state=hidden)\n",
    "\n",
    "        # Couche dense après LSTM\n",
    "        y = self.fc1(output)\n",
    "\n",
    "        # Reshape du résultat\n",
    "        y = tf.reshape(y, (-1, y.shape[2]))\n",
    "\n",
    "        # Dernière couche dense pour obtenir la prédiction du vocabulaire\n",
    "        y = self.fc2(y)\n",
    "\n",
    "        return y, [hidden_state, cell_state], attention_weights\n",
    "\n",
    "    def reset_state(self, batch_size):\n",
    "        # Pour LSTM, initialiser deux états : hidden state et cell state\n",
    "        return [tf.zeros((batch_size, self.units)), tf.zeros((batch_size, self.units))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4. Création de l'encodeur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création de l'encodeur\n",
    "encoder = CNN_Encoder(embedding_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5. Création du décodeur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création du décodeur\n",
    "decoder = RNN_Decoder(embedding_dim, units, vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6. Optimiseur et fonction de perte\n",
    "\n",
    "Ces éléments jouent un rôle clé dans la phase d'entraînement du modèle de captioning. L'optimiseur ajuste les poids du modèle pour minimiser la perte, et la fonction de perte mesure la différence entre les prédictions du modèle et les légendes réelles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimiseur ADAM\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "# La fonction de perte\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.7. Gestion des checkpoints\n",
    "\n",
    "Ce code permet de sauvegarder et de restaurer l'état du modèle à différents points pendant l'entraînement. Les checkpoints contiennent les poids du modèle (pour l'encodeur et le décodeur), ainsi que l'état de l'optimiseur, ce qui permet de reprendre l'entraînement à partir du dernier point de sauvegarde en cas d'interruption."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = os.path.abspath(\"data/checkpoints\")\n",
    "\n",
    "def setup_checkpoint(encoder, decoder, optimizer, max_to_keep=5):\n",
    "    # Créer le checkpoint\n",
    "    ckpt = tf.train.Checkpoint(encoder=encoder, decoder=decoder, optimizer=optimizer)\n",
    "    ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=max_to_keep)\n",
    "    \n",
    "    start_epoch = 0\n",
    "    \n",
    "    # Restaurer le dernier checkpoint s'il existe\n",
    "    if ckpt_manager.latest_checkpoint:\n",
    "        start_epoch = int(ckpt_manager.latest_checkpoint.split('-')[-1])\n",
    "        print(f\"Restoration du dernier checkpoint à l'époque {start_epoch}.\")\n",
    "        ckpt.restore(ckpt_manager.latest_checkpoint)\n",
    "    else:\n",
    "        print(\"Aucun checkpoint trouvé, entraînement à partir de zéro.\")\n",
    "    \n",
    "    return ckpt_manager, start_epoch\n",
    "\n",
    "ckpt_manager, start_epoch = setup_checkpoint(encoder, decoder, optimizer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.8. Fonction : train_step\n",
    "\n",
    "La fonction train_step représente une étape unique d'entraînement du modèle, où une image et sa légende cible sont traitées pour mettre à jour les poids du modèle. Cette étape comprend la propagation avant (encodage des images, génération des légendes), le calcul de la perte, et la rétropropagation pour ajuster les poids à l'aide des gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(img_tensor, target):\n",
    "    loss = 0\n",
    "\n",
    "    # Initialisation de l'état caché pour chaque batch\n",
    "    hidden = decoder.reset_state(batch_size=target.shape[0])\n",
    "\n",
    "    # Initialiser l'entrée du décodeur avec le token <start> pour chaque séquence\n",
    "    dec_input = tf.expand_dims([tokenizer.word_index['<start>']] * target.shape[0], 1)\n",
    "\n",
    "    with tf.GradientTape() as tape:  # Calcul des gradients pour optimiser les poids du modèle\n",
    "        # Encoder l'image (extraction des features avec l'encodeur)\n",
    "        features = encoder(img_tensor)\n",
    "\n",
    "        # Boucle sur chaque étape de la séquence (chaque mot)\n",
    "        for i in range(1, target.shape[1]):\n",
    "            # Prédiction du i-ème mot avec le décodeur\n",
    "            predictions, hidden, _ = decoder(dec_input, features, hidden)\n",
    "\n",
    "            # Calcul de la perte pour cette étape (comparaison entre la vérité et la prédiction)\n",
    "            loss += loss_function(target[:, i], predictions)\n",
    "\n",
    "            # Le mot correct est utilisé comme entrée pour la prochaine prédiction\n",
    "            dec_input = tf.expand_dims(target[:, i], 1)\n",
    "\n",
    "    # Calcul de la perte moyenne par séquence\n",
    "    total_loss = loss / int(target.shape[1])\n",
    "\n",
    "    # Liste des variables entraînables (encodeur et décodeur)\n",
    "    trainable_variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "\n",
    "    # Calcul des gradients en fonction de la perte\n",
    "    gradients = tape.gradient(loss, trainable_variables)\n",
    "    \n",
    "    # Application des gradients pour mettre à jour les poids du modèle\n",
    "    optimizer.apply_gradients(zip(gradients, trainable_variables))\n",
    "\n",
    "    return loss, total_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.9. Fonction : train_model()\n",
    "\n",
    "La fonction train_model() exécute l'entraînement du modèle de captioning d'images sur un nombre spécifié d'époques. À chaque époque, elle parcourt le dataset d'entraînement, met à jour les poids du modèle à l'aide des gradients, et sauvegarde les checkpoints périodiquement. Cette fonction affiche également la progression et le temps d'exécution pour chaque époque."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(epochs, start_epoch, dataset, num_steps, ckpt_manager):\n",
    "    loss_plot = []\n",
    "\n",
    "    for epoch in range(start_epoch, epochs):\n",
    "        start = time.time()\n",
    "        total_loss = 0\n",
    "\n",
    "        # Barre de progression pour l'epoch en cours\n",
    "        with tqdm(total=num_steps, desc=f\"Epoch {epoch + 1}/{epochs}\", unit=\"batch\") as pbar:\n",
    "\n",
    "            # Parcourir les batches d'entraînement\n",
    "            for batch, (img_tensor, target) in enumerate(dataset):\n",
    "                # Effectuer une étape d'entraînement\n",
    "                batch_loss, t_loss = train_step(img_tensor, target)\n",
    "                total_loss += t_loss\n",
    "\n",
    "                # Mise à jour de la barre de progression\n",
    "                pbar.set_postfix({\"Batch Loss\": f\"{batch_loss.numpy() / int(target.shape[1]):.4f}\"})\n",
    "                pbar.update(1)\n",
    "\n",
    "            # Sauvegarder la perte moyenne pour l'epoch\n",
    "            loss_plot.append(total_loss / num_steps)\n",
    "\n",
    "            # Sauvegarder les checkpoints tous les 5 epochs\n",
    "            if epoch % 5 == 0:\n",
    "                ckpt_manager.save()\n",
    "                print(f'Checkpoint sauvegardé à l\\'epoch {epoch + 1}')\n",
    "\n",
    "        # Afficher la perte moyenne et le temps pris pour l'epoch\n",
    "        print(f'\\nEpoch {epoch + 1} Loss {total_loss / num_steps:.6f}')\n",
    "        print(f'Temps pris pour l\\'epoch {epoch + 1}: {time.time() - start:.2f} secondes\\n')\n",
    "\n",
    "    return loss_plot\n",
    "\n",
    "# Lancer l'entraînement\n",
    "loss_plot = train_model(epochs, start_epoch, dataset, num_steps, ckpt_manager)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.10. Fonction : plot_metrics()\n",
    "\n",
    "La fonction plot_metrics() est utilisée pour tracer la courbe de la perte d'entraînement au fil des époques. Bien que dans cet exemple seule la perte soit tracée, la fonction est conçue pour permettre l'ajout d'autres courbes, comme la précision ou le score BLEU, si nécessaire."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metrics(loss_plot):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    # Tracer la courbe de la perte\n",
    "    plt.plot(loss_plot, label='Training Loss', color='r')\n",
    "    \n",
    "    # Tracer la courbe de la précision\n",
    "    # plt.plot(accuracy_plot, label='Training Accuracy', color='g')\n",
    "    \n",
    "    # Tracer la courbe du score BLEU\n",
    "    # plt.plot(bleu_plot, label='Training BLEU', color='b')\n",
    "\n",
    "    # Ajouter des titres et labels\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Metrics')\n",
    "    plt.title('Training Loss Over Epochs')\n",
    "\n",
    "    # Ajouter la grille pour faciliter la lecture\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Ajouter la légende\n",
    "    plt.legend()\n",
    "\n",
    "    # Afficher le plot\n",
    "    plt.show()\n",
    "\n",
    "# Appeler la fonction pour afficher les courbes\n",
    "plot_metrics(loss_plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(image):\n",
    "    attention_plot = np.zeros((max_length, attention_features_shape))\n",
    "\n",
    "    hidden = decoder.reset_state(batch_size=1)\n",
    "\n",
    "    # Charger et pré-traiter l'image\n",
    "    temp_input = tf.expand_dims(load_image(image)[0], 0)\n",
    "    img_tensor_val = image_features_extract_model(temp_input)\n",
    "    img_tensor_val = tf.reshape(img_tensor_val, (img_tensor_val.shape[0], -1, img_tensor_val.shape[3]))\n",
    "\n",
    "    # Passer l'image à l'encodeur\n",
    "    features = encoder(img_tensor_val)\n",
    "\n",
    "    # Initialiser l'entrée du décodeur avec le token <start>\n",
    "    dec_input = tf.expand_dims([tokenizer.word_index['<start>']], 0)\n",
    "    result = []\n",
    "\n",
    "    for i in range(max_length):\n",
    "        # Prédire le mot suivant\n",
    "        predictions, hidden, attention_weights = decoder(dec_input, features, hidden)\n",
    "\n",
    "        # Sauvegarder les poids d'attention\n",
    "        attention_plot[i] = tf.reshape(attention_weights, (-1, )).numpy()\n",
    "\n",
    "        # Sélectionner l'ID du mot prédit avec argmax pour des prédictions plus déterministes\n",
    "        predicted_id = tf.argmax(predictions, axis=-1).numpy()[0]\n",
    "        result.append(tokenizer.index_word.get(predicted_id, '<unk>'))\n",
    "\n",
    "        if tokenizer.index_word.get(predicted_id) == '<end>':\n",
    "            attention_plot = attention_plot[:len(result), :]\n",
    "            return result, attention_plot\n",
    "\n",
    "        # Mettre à jour l'entrée pour le prochain mot\n",
    "        dec_input = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "    attention_plot = attention_plot[:len(result), :]\n",
    "    return result, attention_plot\n",
    "\n",
    "\n",
    "def plot_attention(image, result, attention_plot):\n",
    "    temp_image = np.array(Image.open(image))\n",
    "\n",
    "    fig = plt.figure(figsize=(10, 10))\n",
    "\n",
    "    len_result = len(result)\n",
    "    grid_size = int(np.ceil(np.sqrt(len_result)))\n",
    "\n",
    "    for l in range(len_result):\n",
    "        temp_att = np.resize(attention_plot[l], (8, 8))\n",
    "        ax = fig.add_subplot(grid_size, grid_size, l+1) \n",
    "        ax.set_title(result[l])\n",
    "        img = ax.imshow(temp_image)\n",
    "        ax.imshow(temp_att, cmap='gray', alpha=0.6, extent=img.get_extent())\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_random_annotation(img_name_val, cap_val, tokenizer, num_samples=1):\n",
    "    for _ in range(num_samples):\n",
    "        rid = np.random.randint(0, len(img_name_val))\n",
    "        image = img_name_val[rid]\n",
    "\n",
    "        # Générer la légende réelle\n",
    "        real_caption = ' '.join([tokenizer.index_word[i] for i in cap_val[rid] if i not in [0]])\n",
    "\n",
    "        # Prédire la légende avec le modèle\n",
    "        result, attention_plot = evaluate(image)\n",
    "\n",
    "        # Afficher la légende réelle et la légende prédite\n",
    "        print(f\"\\nImage ID: {rid}\")\n",
    "        print(f\"Real Caption: {real_caption}\")\n",
    "        print(f\"Prediction Caption: {' '.join(result)}\")\n",
    "\n",
    "        # Visualiser l'attention\n",
    "        plot_attention(image, result, attention_plot)\n",
    "\n",
    "display_random_annotation(img_name_val, cap_val, tokenizer, num_samples=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
