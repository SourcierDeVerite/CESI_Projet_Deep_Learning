{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Livrable 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "\n",
    "Dans le cadre du projet Leyenda, dont l'objectif est de réaliser un modèle de DeepLearning permettant de faire du captionning, c'est à dire de créer un texte à partir d'une image. Ce livrable se concentre sur une étape cruciale du prétraitement des images : le débruitage. \n",
    "\n",
    "Le bruit numérique, qui peut résulter de diverses sources telles que des erreurs de capteurs ou des conditions de numérisation sous-optimales, affecte la qualité des images. Ce bruit altère les performances des algorithmes de traitement d'images en ajoutant des informations non désirées qui peuvent perturber l'analyse. Il est donc essentiel de débruiter ces images avant de les soumettre à d'autres algorithmes de classification ou de reconnaissance d'images.\n",
    "\n",
    "Dans cette phase, nous utiliserons des **auto-encodeurs à convolution** pour réaliser un débruitage efficace. Ces réseaux de neurones profonds sont particulièrement adaptés au traitement d'images grâce à leur capacité à capturer les caractéristiques spatiales de l'image. Le notebook documente chaque étape du processus, depuis le chargement des images, jusqu'à leur traitement et l'évaluation des performances du modèle.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Importation et exploration des données\n",
    "\n",
    "Avant de procéder au traitement des images, il est essentiel de configurer l'environnement en important les bibliothèques nécessaires. Cette section se concentre sur l'importation des modules et packages requis pour l'analyse et le traitement des images, ainsi que sur la configuration initiale. Nous importons également les données sous forme d'images, que nous explorerons avant d'entamer le processus de débruitage.\n",
    "\n",
    "### 2.1. Code d'importation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-16T14:07:45.552488Z",
     "start_time": "2024-10-16T14:07:45.548393Z"
    }
   },
   "outputs": [],
   "source": [
    "import warnings, os, datetime, zipfile, tqdm, gdown, glob, pytz\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Configuration des chemins et des paramètres\n",
    "\n",
    "Dans cette section, nous configurons les chemins vers les répertoires de données et les modèles, ainsi que plusieurs paramètres essentiels pour l'entraînement du modèle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-16T14:07:45.571638Z",
     "start_time": "2024-10-16T14:07:45.564951Z"
    }
   },
   "outputs": [],
   "source": [
    "timezone = pytz.timezone('Europe/Paris')\n",
    "now = datetime.datetime.now(timezone).strftime('%Y.%m.%d-%H.%M.%S')\n",
    "\n",
    "project_path = os.getcwd()\n",
    "\n",
    "paths = {\n",
    "    \"data_path\": f\"{project_path}/data\",\n",
    "    \"train_data_path\": f\"{project_path}/data/train\",\n",
    "    \"model_path\": f\"{project_path}/models\",\n",
    "    \"checkpoint_path\": f\"{project_path}/weights/model_early\",\n",
    "    \"log_path\": f\"{project_path}/logs/fit/{now}_model\"\n",
    "}\n",
    "\n",
    "for key, path in paths.items():\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "\n",
    "image_h = 224\n",
    "image_w = 224\n",
    "batch_s = 32\n",
    "ecpchs = 50\n",
    "\n",
    "encoding_dim = 256"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Téléchargement et extraction du dataset\n",
    "\n",
    "Pour effectuer le traitement des images, il est nécessaire de télécharger et d'extraire le dataset. Le dataset est stocké dans un fichier compressé `.zip` disponible sur Google Drive. La fonction suivante télécharge ce dataset, si nécessaire, et l'extrait dans le répertoire approprié."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-16T14:07:45.593539Z",
     "start_time": "2024-10-16T14:07:45.585719Z"
    }
   },
   "outputs": [],
   "source": [
    "def download_dataset(force=False):\n",
    "    # Construct the dataset path\n",
    "    dataset_path = os.path.join(paths['train_data_path'], 'dataset_livrable_2.zip')\n",
    "\n",
    "    # Check if the dataset is already downloaded\n",
    "    if os.path.exists(dataset_path) and not force:\n",
    "        print(\"Dataset is already downloaded.\")\n",
    "    else:\n",
    "        print(\"Downloading dataset...\")\n",
    "        url = 'https://drive.google.com/uc?export=download&id=190NL04KXMiUsnC-rdYDB9PLxCgk-MKuy'\n",
    "        gdown.download(url, dataset_path, quiet=False)\n",
    "    \n",
    "    # Check if the dataset is already extracted\n",
    "    if len(os.listdir(paths['train_data_path'])) >= 2 and not force:\n",
    "        print(\"Dataset is already extracted.\")\n",
    "    else:\n",
    "        print(\"Extracting dataset...\")\n",
    "        with zipfile.ZipFile(dataset_path, 'r') as zip_ref:\n",
    "            files = zip_ref.infolist()\n",
    "            with tqdm.tqdm(total=len(files), desc=\"Extracting\", unit=\"file\") as pbar:\n",
    "                for file in files:\n",
    "                    zip_ref.extract(file, paths['train_data_path'])\n",
    "                    pbar.update(1)\n",
    "            print(f\"Dataset extracted to {paths['train_data_path']}\")\n",
    "        \n",
    "download_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4. Vérification de l'intégrité des images dans le dataset\n",
    "\n",
    "Il est essentiel de s'assurer que les images du dataset sont valides avant de les utiliser pour l'entraînement d'un modèle. Cette section présente la fonction `check_images_in_dataset` qui permet de vérifier l'intégrité de chaque image et de supprimer les fichiers corrompus ou non valides."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-16T14:08:00.292942Z",
     "start_time": "2024-10-16T14:07:45.630460Z"
    }
   },
   "outputs": [],
   "source": [
    "def check_images_in_dataset(dataset_path):\n",
    "    # Get the paths of all image files (jpg, png, etc.)\n",
    "    img_paths = glob.glob(os.path.join(dataset_path, '*/*.*'))\n",
    "\n",
    "    for img_path in tqdm.tqdm(img_paths, desc=\"Checking images\"):\n",
    "        try:\n",
    "            # Read and decode the image to check if it is valid\n",
    "            img_bytes = tf.io.read_file(img_path)\n",
    "            tf.io.decode_image(img_bytes)\n",
    "\n",
    "        except tf.errors.InvalidArgumentError as e:\n",
    "            # Catch specific TensorFlow error and remove bad files\n",
    "            print(f\"Found corrupt image: {img_path}. Error: {str(e)}. Removing it.\")\n",
    "            os.remove(img_path)\n",
    "        except Exception as e:\n",
    "            # General exception handler for other possible issues\n",
    "            print(f\"Error processing image: {img_path}. Error: {str(e)}. Removing it.\")\n",
    "            os.remove(img_path)\n",
    "\n",
    "check_images_in_dataset(paths['train_data_path'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5. Chargement et division des images en ensembles d'entraînement et de test\n",
    "\n",
    "Dans cette section, nous chargeons les images du dataset et les divisons en deux sous-ensembles : l'un pour l'entraînement (80%) et l'autre pour la validation (20%). Cette étape est cruciale pour entraîner le modèle sur une partie des données tout en réservant une portion pour évaluer ses performances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-16T14:08:01.232422Z",
     "start_time": "2024-10-16T14:08:00.386752Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load and split images into train and test sets (80%-20%)\n",
    "train_set, validation_set = tf.keras.utils.image_dataset_from_directory(\n",
    "    paths['train_data_path'],\n",
    "    validation_split=0.2,\n",
    "    subset=\"both\",\n",
    "    seed=42,\n",
    "    image_size=(image_h, image_w),\n",
    "    batch_size=batch_s,\n",
    "    labels=None,\n",
    "    label_mode=None\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6. Prétraitement des images : Ajout de bruit gaussien et redimensionnement\n",
    "\n",
    "Dans cette phase, nous ajoutons une couche de redimensionnement pour normaliser les valeurs des pixels des images et appliquons un bruit gaussien à chaque lot d'images. Ce bruit simule des perturbations réelles présentes dans des images de mauvaise qualité, comme celles numérisées avec des artefacts, afin d'entraîner un modèle capable de restaurer des images bruitées."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-16T14:08:01.350179Z",
     "start_time": "2024-10-16T14:08:01.241649Z"
    }
   },
   "outputs": [],
   "source": [
    "# Adding the resizing layer\n",
    "rescale_layer = tf.keras.layers.Rescaling(1./255)\n",
    "\n",
    "# Adds Gaussian noise to images, keeping pixel values in [0, 1].\n",
    "def add_gaussian_noise(images, min_noise=0.1, max_noise=0.5):\n",
    "    batch_size = tf.shape(images)[0]\n",
    "    noise_factor = tf.random.uniform(shape=(batch_size, 1, 1, 1), minval=min_noise, maxval=max_noise)\n",
    "    noise = tf.random.normal(shape=tf.shape(images))\n",
    "    noisy_images = images + noise_factor * noise\n",
    "    noisy_images = tf.clip_by_value(noisy_images, 0.0, 1.0)\n",
    "    return noisy_images\n",
    "\n",
    "noisy_train_set = train_set.map(\n",
    "    lambda x: (add_gaussian_noise(rescale_layer(x)), rescale_layer(x)),\n",
    "    num_parallel_calls=tf.data.AUTOTUNE\n",
    ").cache().prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "\n",
    "noisy_validation_set = validation_set.map(\n",
    "    lambda x: (add_gaussian_noise(rescale_layer(x)), rescale_layer(x)),\n",
    "    num_parallel_calls=tf.data.AUTOTUNE\n",
    ").cache().prefetch(buffer_size=tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.7. Visualisation des images originales et bruitées\n",
    "\n",
    "La visualisation est une étape importante pour s'assurer que le bruit est correctement appliqué aux images d'entraînement et pour comparer les images d'origine avec leurs versions bruitées. La fonction `display_noisy_train_images` permet d'afficher côte à côte un échantillon d'images originales et bruitées à partir du dataset d'entraînement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-16T14:08:03.964968Z",
     "start_time": "2024-10-16T14:08:01.358372Z"
    }
   },
   "outputs": [],
   "source": [
    "# Function for displaying original and noisy images\n",
    "def display_noisy_train_images(noisy_dataset, num_images=5):\n",
    "    plt.figure(figsize=(15, 6))\n",
    "\n",
    "    for noisy_images, original_images in noisy_dataset.take(1):\n",
    "        for i in range(num_images):\n",
    "            # Original pictures\n",
    "            plt.subplot(2, num_images, i + 1)\n",
    "            plt.imshow(original_images[i].numpy())\n",
    "            plt.title('Original')\n",
    "            plt.axis('off')\n",
    "\n",
    "            # Noisy images\n",
    "            plt.subplot(2, num_images, num_images + i + 1)\n",
    "            plt.imshow(noisy_images[i].numpy())\n",
    "            plt.title('Noisy')\n",
    "            plt.axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "display_noisy_train_images(noisy_train_set, num_images=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Construction de l'Auto-encodeur\n",
    "\n",
    "Un auto-encodeur est un type de réseau de neurones utilisé pour apprendre une représentation compacte (encodée) des données en entrée. Il est constitué de deux parties principales : l'encodeur et le décodeur. L'encodeur réduit les dimensions des données d'entrée, tandis que le décodeur tente de reconstruire les données originales à partir de cette représentation réduite. Ce modèle est particulièrement utile pour des tâches comme le débruitage d'images.\n",
    "\n",
    "### Explication générale : Comment fonctionne un auto-encodeur ?\n",
    "\n",
    "Un auto-encodeur prend une image en entrée et passe par plusieurs couches pour compresser l'image en une représentation latente plus petite. Ensuite, cette représentation compressée est passée à travers le décodeur, qui reconstruit l'image. L'objectif du modèle est de minimiser la différence entre l'image d'origine et l'image reconstruite, ce qui signifie qu'il apprend à capturer les caractéristiques importantes de l'image tout en éliminant le bruit ou d'autres détails inutiles.\n",
    "\n",
    "Le modèle d'auto-encodeur est optimisé avec une fonction de perte telle que l'erreur quadratique moyenne (MSE) pour comparer les pixels de l'image originale à ceux de l'image reconstruite.\n",
    "\n",
    "### 3.1. Construction de l'Auto-encodeur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-17T08:57:26.618282Z",
     "start_time": "2024-10-17T08:57:26.542695Z"
    }
   },
   "outputs": [],
   "source": [
    "# def build_autoencoder(input_shape):\n",
    "#     input_img = tf.keras.layers.Input(shape=input_shape)\n",
    "    \n",
    "#     # Encoder\n",
    "#     x = tf.keras.layers.Conv2D(32, (3, 3), activation='relu', padding='same')(input_img)\n",
    "#     x = tf.keras.layers.MaxPooling2D((2, 2), padding='same')(x)\n",
    "\n",
    "#     x = tf.keras.layers.Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
    "#     x = tf.keras.layers.MaxPooling2D((2, 2), padding='same')(x)\n",
    "\n",
    "#     x = tf.keras.layers.Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n",
    "    \n",
    "#     encoded = tf.keras.layers.MaxPooling2D((2, 2), padding='same')(x)\n",
    "\n",
    "#     # Decoder\n",
    "#     x = tf.keras.layers.Conv2D(128, (3, 3), activation='relu', padding='same')(encoded)\n",
    "#     x = tf.keras.layers.UpSampling2D((2, 2))(x)\n",
    "    \n",
    "#     x = tf.keras.layers.Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
    "#     x = tf.keras.layers.UpSampling2D((2, 2))(x)\n",
    "\n",
    "#     x = tf.keras.layers.Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n",
    "#     x = tf.keras.layers.UpSampling2D((2, 2))(x)\n",
    "\n",
    "#     decoded = tf.keras.layers.Conv2D(3, (3, 3), activation='sigmoid', padding='same')(x)\n",
    "    \n",
    "#     # Self-encoding model\n",
    "#     autoencoder = tf.keras.Model(input_img, decoded)\n",
    "#     return autoencoder\n",
    "\n",
    "# # Model construction\n",
    "# input_shape = (image_h, image_w, 3)\n",
    "# autoencoder = build_autoencoder(input_shape)\n",
    "\n",
    "# def psnr(y_true, y_pred):\n",
    "#     return tf.image.psnr(y_true, y_pred, max_val=1.0)\n",
    "\n",
    "# def ssim(y_true, y_pred):\n",
    "#     return tf.image.ssim(y_true, y_pred, max_val=1.0)\n",
    "\n",
    "# autoencoder.compile(optimizer='adam', loss='mse', metrics=[psnr, ssim])\n",
    "# autoencoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Explication du modèle couche par couche\n",
    "\n",
    "#### 1. **Encodeur** \n",
    "\n",
    "L'encodeur est responsable de la compression des données d'entrée en une représentation plus compacte. Cela se fait à l'aide de couches convolutives et de pooling pour réduire les dimensions tout en capturant les caractéristiques importantes de l'image.\n",
    "\n",
    "- **Première couche Conv2D :**\n",
    "  - **Type de couche** : Convolution 2D\n",
    "  - **Nombre de filtres** : 32 filtres\n",
    "  - **Taille du noyau (kernel)** : (3, 3)\n",
    "  - **Fonction d'activation** : ReLU\n",
    "  - **Padding** : `same` (cela conserve les dimensions d'entrée)\n",
    "  - **Rôle** : Appliquer 32 filtres de convolution pour extraire des caractéristiques basiques des images (bords, textures).\n",
    "\n",
    "- **MaxPooling2D :**\n",
    "  - **Type de couche** : Pooling (sous-échantillonnage)\n",
    "  - **Taille du pool** : (2, 2)\n",
    "  - **Padding** : `same` (cela conserve les dimensions tout en réduisant la résolution)\n",
    "  - **Rôle** : Réduit la taille de l'image de moitié, en prenant la valeur maximale dans chaque fenêtre de 2x2 pixels, tout en préservant les caractéristiques les plus saillantes.\n",
    "\n",
    "- **Deuxième couche Conv2D :**\n",
    "  - **Nombre de filtres** : 64 filtres\n",
    "  - **Taille du noyau** : (3, 3)\n",
    "  - **Fonction d'activation** : ReLU\n",
    "  - **Padding** : `same`\n",
    "  - **Rôle** : Augmenter la profondeur des caractéristiques extraites avec 64 filtres, capturant ainsi des détails plus complexes.\n",
    "\n",
    "- **MaxPooling2D :**\n",
    "  - Identique à la première couche de MaxPooling2D, cette couche réduit encore la résolution tout en conservant les caractéristiques importantes.\n",
    "\n",
    "- **Troisième couche Conv2D :**\n",
    "  - **Nombre de filtres** : 128 filtres\n",
    "  - **Taille du noyau** : (3, 3)\n",
    "  - **Fonction d'activation** : ReLU\n",
    "  - **Padding** : `same`\n",
    "  - **Rôle** : Extraire des caractéristiques encore plus fines avec plus de filtres, capturant des structures détaillées de l'image.\n",
    "\n",
    "- **MaxPooling2D (encoded layer)** :\n",
    "  - Cette couche finale de MaxPooling dans l'encodeur réduit encore l'image pour obtenir la représentation encodée. C'est la sortie compressée de l'image, appelée \"représentation latente\". Elle a une dimension beaucoup plus petite, mais conserve les informations essentielles nécessaires à la reconstruction.\n",
    "\n",
    "#### 2. **Décodeur**\n",
    "\n",
    "Le décodeur est la partie qui prend la représentation encodée et tente de reconstruire l'image d'origine. Il utilise des couches convolutives et d'up-sampling pour restaurer les dimensions de l'image à partir de la représentation latente.\n",
    "\n",
    "- **Première couche Conv2D :**\n",
    "  - **Nombre de filtres** : 128 filtres\n",
    "  - **Taille du noyau** : (3, 3)\n",
    "  - **Fonction d'activation** : ReLU\n",
    "  - **Padding** : `same`\n",
    "  - **Rôle** : Commence le processus de reconstruction de l'image avec les mêmes caractéristiques extraites par la dernière couche de convolution de l'encodeur.\n",
    "\n",
    "- **UpSampling2D :**\n",
    "  - **Type de couche** : Upsampling (sur-échantillonnage)\n",
    "  - **Taille de la fenêtre** : (2, 2)\n",
    "  - **Rôle** : Double les dimensions de l'image à chaque étape pour restaurer la résolution originale, inversant l'effet de MaxPooling.\n",
    "\n",
    "- **Deuxième couche Conv2D :**\n",
    "  - Identique à la précédente, mais avec 64 filtres, réduisant progressivement la complexité des caractéristiques au fur et à mesure que l'on s'approche de l'image reconstruite.\n",
    "\n",
    "- **UpSampling2D :**\n",
    "  - Identique à la première couche d'UpSampling, elle augmente encore les dimensions de l'image.\n",
    "\n",
    "- **Troisième couche Conv2D :**\n",
    "  - **Nombre de filtres** : 32 filtres\n",
    "  - **Taille du noyau** : (3, 3)\n",
    "  - **Fonction d'activation** : ReLU\n",
    "  - **Padding** : `same`\n",
    "  - **Rôle** : Applique des filtres moins complexes pour affiner la reconstruction de l'image.\n",
    "\n",
    "- **UpSampling2D :**\n",
    "  - Effectue une dernière opération de sur-échantillonnage pour restaurer l'image à sa taille originale.\n",
    "\n",
    "- **Dernière couche Conv2D :**\n",
    "  - **Nombre de filtres** : 3 (correspondant aux 3 canaux de couleur RGB)\n",
    "  - **Taille du noyau** : (3, 3)\n",
    "  - **Fonction d'activation** : Sigmoïde\n",
    "  - **Padding** : `same`\n",
    "  - **Rôle** : Reconstituer les valeurs de pixels de l'image reconstruite dans l'intervalle [0, 1].\n",
    "\n",
    "#### 3. **Compilation du modèle**\n",
    "\n",
    "Une fois le modèle créé, il est compilé avec les paramètres suivants :\n",
    "- **Optimiseur** : `adam` — Un optimiseur populaire basé sur la descente de gradient adaptative.\n",
    "- **Fonction de perte** : `mean squared error (mse)` — Calcul la différence entre l'image originale et l'image reconstruite pixel par pixel.\n",
    "\n",
    "Nous utilisons deux métriques pour évaluer la performance de l'auto-encodeur :\n",
    "- **PSNR** (Peak Signal-to-Noise Ratio) : Cette métrique mesure la qualité de l'image reconstruite par rapport à l'image originale. Une valeur de PSNR plus élevée indique une meilleure reconstruction.\n",
    "- **SSIM** (Structural Similarity Index) : Le SSIM est une métrique perceptuelle qui évalue la similarité structurale entre deux images. Elle est particulièrement utile pour juger de la qualité visuelle des images débruitées.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Configuration des callbacks pour l'entraînement du modèle\n",
    "\n",
    "Les callbacks sont des outils puissants permettant de contrôler le processus d'entraînement d'un modèle. Ils permettent de sauvegarder les poids, de surveiller les métriques, d'arrêter l'entraînement en cas de stagnation, et d'enregistrer des informations pour une analyse ultérieure. La fonction get_callbacks configure trois callbacks principaux : la sauvegarde des checkpoints, l'intégration avec TensorBoard, et l'arrêt anticipé (early stopping)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-17T08:57:26.742646Z",
     "start_time": "2024-10-17T08:57:26.735893Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_callbacks():\n",
    "    # Create a callback that saves the model's weights at each epoch where validation loss improves\n",
    "    checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath=paths['checkpoint_path'] + \"/weights-epoch-{epoch:02d}-{val_loss:.2f}.weights.h5\",\n",
    "        save_weights_only=True,\n",
    "        save_best_only=True,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    # Create a TensorBoard callback to log training metrics, model graphs, and images for visualization\n",
    "    tensorboard_callback = tf.keras.callbacks.TensorBoard(\n",
    "        log_dir=paths['log_path'],\n",
    "        histogram_freq=1,\n",
    "        write_graph=True,\n",
    "        write_images=True,\n",
    "        update_freq='epoch',\n",
    "        profile_batch=0,\n",
    "        embeddings_freq=0\n",
    "    )\n",
    "\n",
    "    # Set up early stopping to halt training if validation loss stops improving for a set number of epochs\n",
    "    early_callback = tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        min_delta=0,\n",
    "        patience=5,\n",
    "        verbose=0,\n",
    "        mode='auto',\n",
    "        baseline=None,\n",
    "        restore_best_weights=True,\n",
    "        start_from_epoch=0\n",
    "    )\n",
    "\n",
    "    return [checkpoint_callback, tensorboard_callback, early_callback]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### 3.4. Entraînement du modèle\n",
    "Cette étape vise à ajuster les poids du modèle pour optimiser la reconstruction d'images bruitées. \n",
    "Nous utiliserons la méthode fit pour entraîner le modèle sur `noisy_train_set` tout en validant ses performances sur `noisy_test_set`. \n",
    "Grâce aux callbacks configurés précédemment, nous garantirons un entraînement efficace, \n",
    "avec une surveillance des métriques et la sauvegarde des poids les plus performants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-10-17T08:57:26.774839Z"
    },
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# # Model training\n",
    "# history = autoencoder.fit(\n",
    "#     noisy_train_set,\n",
    "#     epochs=ecpchs,\n",
    "#     validation_data=noisy_validation_set,\n",
    "#     callbacks=get_callbacks()\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le code ci-dessus lance l'entraînement de l'auto-encodeur en utilisant la méthode ``fit``. Il entraîne le modèle sur l'ensemble d'entraînement, ``noisy_train_set``, composé d'images bruitées, afin que le modèle apprenne à reconstruire les images originales. L'entraînement se déroule sur 50 époques, ce qui représente le nombre d'itérations sur les données. Pour évaluer la performance du modèle pendant l'entraînement, l'ensemble de validation ``noisy_test_set`` est utilisé. De plus, des callbacks sont intégrés via la fonction ``get_callbacks()``, permettant de gérer le processus d'entraînement en sauvegardant les poids du modèle et en activant l'arrêt anticipé si nécessaire. En résumé, ce code permet d'entraîner le modèle tout en surveillant sa performance et en optimisant le processus.\n",
    "\n",
    "### 3.5. Visualisation des courbes de perte\n",
    "Les courbes de perte sont des graphiques représentant l'évolution de la fonction de perte au fil des époques pendant l'entraînement d'un modèle. Elles permettent d'évaluer la performance du modèle en comparant les pertes d'entraînement et de validation. Analyser ces courbes aide à identifier des problèmes tels que le surapprentissage, lorsque la perte d'entraînement diminue mais que la perte de validation commence à augmenter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-16T15:32:20.481270Z",
     "start_time": "2024-10-16T15:32:20.364520Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_training_curves(history, metrics=['loss'], figsize=(10, 5)):\n",
    "    plt.figure(figsize=figsize)\n",
    "    \n",
    "    for metric in metrics:\n",
    "        train_metric = history.history.get(metric)\n",
    "        val_metric = history.history.get(f'val_{metric}')\n",
    "        \n",
    "        if train_metric and val_metric:\n",
    "            plt.plot(train_metric, label=f'Training {metric.capitalize()}')\n",
    "            plt.plot(val_metric, label=f'Validation {metric.capitalize()}')\n",
    "        else:\n",
    "            print(f\"Warning: Metric '{metric}' not found in history. Skipping.\")\n",
    "\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Metric Value')\n",
    "    plt.title('Training and Validation Metrics Evolution')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# plot_training_curves(history, metrics=['loss', 'psnr', 'ssim'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le code trace les courbes de perte d'entraînement et de validation pour évaluer la performance du modèle au cours des époques. La courbe de perte d'entraînement, représentée par ``history.history['loss']``, montre comment le modèle s'adapte aux données d'entraînement, tandis que la courbe de perte de validation, représentée par ``history.history['val_loss']``, indique la capacité du modèle à généraliser sur des données invisibles. L'évolution de ces courbes permet de détecter des phénomènes tels que le surapprentissage, où la perte d'entraînement continue de diminuer alors que la perte de validation commence à augmenter. L'ajout d'étiquettes et de légendes facilite l'interprétation des résultats, permettant ainsi d'analyser la convergence et l'efficacité du modèle durant l'entraînement.\n",
    "\n",
    "\n",
    "### 3.6. Visualisation des images débruitées\n",
    "La visualisation des images débruitées permet de comparer visuellement les images bruitées, débruitées et originales. Cela aide à évaluer l'efficacité du modèle d'auto-encodeur dans la restauration des détails perdus en raison du bruit. En observant les résultats, on peut déterminer si le modèle parvient à conserver les caractéristiques importantes de l'image tout en éliminant le bruit indésirable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-16T15:32:21.704301Z",
     "start_time": "2024-10-16T15:32:20.489670Z"
    }
   },
   "outputs": [],
   "source": [
    "def display_denoised_images(model, dataset, num_images=5):\n",
    "    plt.figure(figsize=(15, 8))\n",
    "\n",
    "    for noisy_images, original_images in dataset.take(1):\n",
    "        # Predicting denoised pictures\n",
    "        denoised_images = model.predict(noisy_images)\n",
    "        \n",
    "        for i in range(num_images):\n",
    "            # Noisy images\n",
    "            plt.subplot(3, num_images, i + 1)\n",
    "            plt.imshow(noisy_images[i].numpy())\n",
    "            plt.title('Noisy')\n",
    "            plt.axis('off')\n",
    "            \n",
    "            # Denoised images\n",
    "            plt.subplot(3, num_images, num_images + i + 1)\n",
    "            plt.imshow(denoised_images[i])\n",
    "            plt.title('Denoised')\n",
    "            plt.axis('off')\n",
    "            \n",
    "            # Original picture\n",
    "            plt.subplot(3, num_images, 2 * num_images + i + 1)\n",
    "            plt.imshow(original_images[i].numpy())\n",
    "            plt.title('Original')\n",
    "            plt.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Displaying results\n",
    "# display_denoised_images(autoencoder, noisy_validation_set, num_images=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.7. Plan d'expérience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get tf version\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import pandas as pd\n",
    "\n",
    "# Définition des hyperparamètres à tester\n",
    "num_filters_options = [\n",
    "    [16, 32, 64],\n",
    "    [32, 64, 128],\n",
    "    [64, 128, 256],\n",
    "    # Ajoutez d'autres configurations si nécessaire\n",
    "]\n",
    "\n",
    "optimizers = [\n",
    "    tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "    tf.keras.optimizers.Adam(learning_rate=0.0001),\n",
    "    # Ajoutez d'autres optimisateurs ou taux d'apprentissage\n",
    "]\n",
    "\n",
    "loss_functions = [\n",
    "    'mse',\n",
    "    'mae',\n",
    "    # Ajoutez d'autres fonctions de perte ou des fonctions personnalisées\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction modifiée pour construire l'auto-encodeur avec des hyperparamètres variables\n",
    "def build_autoencoder_test(input_shape, num_filters):\n",
    "    input_img = tf.keras.layers.Input(shape=input_shape)\n",
    "    \n",
    "    # Encoder\n",
    "    x = tf.keras.layers.Conv2D(num_filters[0], (3, 3), activation='relu', padding='same')(input_img)\n",
    "    x = tf.keras.layers.MaxPooling2D((2, 2), padding='same')(x)\n",
    "    \n",
    "    x = tf.keras.layers.Conv2D(num_filters[1], (3, 3), activation='relu', padding='same')(x)\n",
    "    x = tf.keras.layers.MaxPooling2D((2, 2), padding='same')(x)\n",
    "    \n",
    "    x = tf.keras.layers.Conv2D(num_filters[2], (3, 3), activation='relu', padding='same')(x)\n",
    "    encoded = tf.keras.layers.MaxPooling2D((2, 2), padding='same')(x)\n",
    "    \n",
    "    # Decoder\n",
    "    x = tf.keras.layers.Conv2D(num_filters[2], (3, 3), activation='relu', padding='same')(encoded)\n",
    "    x = tf.keras.layers.UpSampling2D((2, 2))(x)\n",
    "    \n",
    "    x = tf.keras.layers.Conv2D(num_filters[1], (3, 3), activation='relu', padding='same')(x)\n",
    "    x = tf.keras.layers.UpSampling2D((2, 2))(x)\n",
    "    \n",
    "    x = tf.keras.layers.Conv2D(num_filters[0], (3, 3), activation='relu', padding='same')(x)\n",
    "    x = tf.keras.layers.UpSampling2D((2, 2))(x)\n",
    "    \n",
    "    decoded = tf.keras.layers.Conv2D(3, (3, 3), activation='sigmoid', padding='same')(x)\n",
    "    \n",
    "    autoencoder = tf.keras.Model(input_img, decoded)\n",
    "    return autoencoder\n",
    "\n",
    "# Fonctions de calcul des métriques\n",
    "def psnr_metric(y_true, y_pred):\n",
    "    return tf.image.psnr(y_true, y_pred, max_val=1.0)\n",
    "\n",
    "def ssim_metric(y_true, y_pred):\n",
    "    return tf.image.ssim(y_true, y_pred, max_val=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combinaisons des hyperparamètres\n",
    "experiments = list(itertools.product(num_filters_options, optimizers, loss_functions))\n",
    "\n",
    "# Liste pour stocker les résultats\n",
    "results = []\n",
    "\n",
    "# Boucle sur chaque combinaison d'hyperparamètres\n",
    "for num_filters, optimizer, loss_function in experiments:\n",
    "    print(f\"Entraînement avec num_filters={num_filters}, optimizer={optimizer.name}, loss_function={loss_function}\")\n",
    "    \n",
    "    # Construction du modèle\n",
    "    autoencoder = build_autoencoder_test((image_h, image_w, 3), num_filters)\n",
    "    \n",
    "    # Compilation du modèle\n",
    "    autoencoder.compile(optimizer=optimizer, loss=loss_function, metrics=[psnr_metric, ssim_metric])\n",
    "    \n",
    "    # Entraînement du modèle\n",
    "    history = autoencoder.fit(\n",
    "        noisy_train_set,\n",
    "        epochs=1, \n",
    "        validation_data=noisy_validation_set,\n",
    "        callbacks=get_callbacks(),\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Évaluation du modèle sur le jeu de validation\n",
    "    eval_results = autoencoder.evaluate(noisy_validation_set, verbose=0)\n",
    "    eval_dict = dict(zip(autoencoder.metrics_names, eval_results))\n",
    "    \n",
    "    # Enregistrement des résultats\n",
    "    results.append({\n",
    "        'num_filters': num_filters,\n",
    "        'optimizer': optimizer.name,\n",
    "        'learning_rate': optimizer.learning_rate.numpy(),\n",
    "        'loss_function': loss_function,\n",
    "        'validation_loss': eval_dict['loss'],\n",
    "        'psnr': eval_dict['psnr'],\n",
    "        'ssim': eval_dict['ssim']\n",
    "    })\n",
    "\n",
    "# Conversion des résultats en DataFrame pour une analyse facile\n",
    "df_results = pd.DataFrame(results)\n",
    "print(df_results)\n",
    "\n",
    "# Libération de la session pour éviter les fuites de mémoire\n",
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Conclusion\n",
    "Le projet Leyenda vise à développer une solution pour améliorer la qualité des images numérisées, facilitant ainsi leur traitement par des algorithmes de Deep Learning. L'accent est mis sur le débruitage, une étape essentielle du prétraitement, qui permet de réduire le bruit numérique résultant de divers facteurs tels que les erreurs de capteurs ou des conditions de numérisation inadaptées.\n",
    "\n",
    "Pour atteindre cet objectif, des auto-encodeurs à convolution sont utilisés, exploitant leur capacité à capturer les caractéristiques spatiales des images tout en éliminant les informations non désirées. Le processus comprend plusieurs étapes clés, de l'importation et de l'exploration des données, à la vérification de l'intégrité des images, en passant par l'ajout de bruit gaussien pour simuler des conditions réelles.\n",
    "\n",
    "La construction et l'entraînement d'un modèle d'auto-encodeur, optimisé à l'aide de la fonction de perte d'erreur quadratique moyenne, permettent de reconstruire des images de haute qualité à partir de leurs versions bruitées. Des callbacks sont configurés pour superviser l'entraînement et sauvegarder les meilleures performances, garantissant ainsi l'efficacité du modèle.\n",
    "\n",
    "Les résultats de ce projet permettront d'améliorer significativement le traitement d'images par des algorithmes de classification et de reconnaissance, posant ainsi les bases pour des applications avancées dans divers domaines nécessitant une analyse d'images de qualité."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Données factices\n",
    "x_train = np.random.rand(100, 64, 64, 3).astype(np.float32)\n",
    "y_train = x_train.copy()\n",
    "\n",
    "# Modèle simple\n",
    "input_img = tf.keras.layers.Input(shape=(64, 64, 3))\n",
    "x = tf.keras.layers.Conv2D(32, (3,3), activation='relu', padding='same')(input_img)\n",
    "decoded = tf.keras.layers.Conv2D(3, (3,3), activation='sigmoid', padding='same')(x)\n",
    "autoencoder = tf.keras.Model(input_img, decoded)\n",
    "\n",
    "# Compilation du modèle avec une métrique intégrée\n",
    "autoencoder.compile(optimizer='adam', loss='mse', metrics=['mae', 'mse'])\n",
    "\n",
    "# Affichage des noms des métriques\n",
    "print(\"Metrics names:\", autoencoder.get_compile_config()['metrics'])\n",
    "\n",
    "# Entraînement du modèle\n",
    "autoencoder.fit(x_train, y_train, epochs=3, batch_size=10)\n",
    "\n",
    "print(\"Metrics names:\", autoencoder.get_metrics_result())\n",
    "\n",
    "# Évaluation du modèle\n",
    "eval_results = autoencoder.evaluate(x_train, y_train, verbose=0)\n",
    "eval_dict = dict(zip(autoencoder.metrics_names, eval_results))\n",
    "print(\"Evaluation results:\", eval_dict)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
