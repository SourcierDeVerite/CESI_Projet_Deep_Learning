{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Livrable 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "\n",
    "Dans le cadre du projet Leyenda, dont l'objectif est de réaliser un modèle de DeepLearning permettant de faire du captionning, c'est à dire de créer un texte à partir d'une image. Ce livrable se concentre sur une étape cruciale du prétraitement des images : le débruitage. \n",
    "\n",
    "Le bruit numérique, qui peut résulter de diverses sources telles que des erreurs de capteurs ou des conditions de numérisation sous-optimales, affecte la qualité des images. Ce bruit altère les performances des algorithmes de traitement d'images en ajoutant des informations non désirées qui peuvent perturber l'analyse. Il est donc essentiel de débruiter ces images avant de les soumettre à d'autres algorithmes de classification ou de reconnaissance d'images.\n",
    "\n",
    "Dans cette phase, nous utiliserons des **auto-encodeurs à convolution** pour réaliser un débruitage efficace. Ces réseaux de neurones profonds sont particulièrement adaptés au traitement d'images grâce à leur capacité à capturer les caractéristiques spatiales de l'image. Le notebook documente chaque étape du processus, depuis le chargement des images, jusqu'à leur traitement et l'évaluation des performances du modèle.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Importation et exploration des données\n",
    "\n",
    "Avant de procéder au traitement des images, il est essentiel de configurer l'environnement en important les bibliothèques nécessaires. Cette section se concentre sur l'importation des modules et packages requis pour l'analyse et le traitement des images, ainsi que sur la configuration initiale. Nous importons également les données sous forme d'images, que nous explorerons avant d'entamer le processus de débruitage.\n",
    "\n",
    "### 2.1. Code d'importation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-18T09:08:16.172718Z",
     "start_time": "2024-10-18T09:08:05.420846Z"
    }
   },
   "outputs": [],
   "source": [
    "import warnings, os, datetime, zipfile, tqdm, gdown, glob, pytz\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import wandb\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Configuration des chemins et des paramètres\n",
    "\n",
    "Dans cette section, nous configurons les chemins vers les répertoires de données et les modèles, ainsi que plusieurs paramètres essentiels pour l'entraînement du modèle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-16T14:07:45.571638Z",
     "start_time": "2024-10-16T14:07:45.564951Z"
    }
   },
   "outputs": [],
   "source": [
    "timezone = pytz.timezone('Europe/Paris')\n",
    "now = datetime.datetime.now(timezone).strftime('%Y.%m.%d-%H.%M.%S')\n",
    "\n",
    "project_path = os.getcwd()\n",
    "\n",
    "paths = {\n",
    "    \"data_path\": f\"{project_path}/data\",\n",
    "    \"train_data_path\": f\"{project_path}/data/train\",\n",
    "    \"model_path\": f\"{project_path}/models\",\n",
    "    \"checkpoint_path\": f\"{project_path}/weights/model_early\",\n",
    "    \"log_path\": f\"{project_path}/logs/fit/{now}_model\"\n",
    "}\n",
    "\n",
    "for key, path in paths.items():\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "\n",
    "image_h = 224\n",
    "image_w = 224\n",
    "batch_s = 32\n",
    "epochs = 50\n",
    "\n",
    "encoding_dim = 256"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Téléchargement et extraction du dataset\n",
    "\n",
    "Pour effectuer le traitement des images, il est nécessaire de télécharger et d'extraire le dataset. Le dataset est stocké dans un fichier compressé `.zip` disponible sur Google Drive. La fonction suivante télécharge ce dataset, si nécessaire, et l'extrait dans le répertoire approprié."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-18T09:08:31.357916Z",
     "start_time": "2024-10-18T09:08:29.730493Z"
    }
   },
   "outputs": [],
   "source": [
    "def download_dataset(force=False):\n",
    "    # Construct the dataset path\n",
    "    dataset_path = os.path.join(paths['train_data_path'], 'dataset_livrable_2.zip')\n",
    "\n",
    "    # Check if the dataset is already downloaded\n",
    "    if os.path.exists(dataset_path) and not force:\n",
    "        print(\"Dataset is already downloaded.\")\n",
    "    else:\n",
    "        print(\"Downloading dataset...\")\n",
    "        url = 'https://drive.google.com/uc?export=download&id=190NL04KXMiUsnC-rdYDB9PLxCgk-MKuy'\n",
    "        gdown.download(url, dataset_path, quiet=False)\n",
    "    \n",
    "    # Check if the dataset is already extracted\n",
    "    if len(os.listdir(paths['train_data_path'])) == 2 and not force:\n",
    "        print(\"Dataset is already extracted.\")\n",
    "    else:\n",
    "        print(\"Extracting dataset...\")\n",
    "        with zipfile.ZipFile(dataset_path, 'r') as zip_ref:\n",
    "            files = zip_ref.infolist()\n",
    "            with tqdm.tqdm(total=len(files), desc=\"Extracting\", unit=\"file\") as pbar:\n",
    "                for file in files:\n",
    "                    zip_ref.extract(file, paths['train_data_path'])\n",
    "                    pbar.update(1)\n",
    "            print(f\"Dataset extracted to {paths['train_data_path']}\")\n",
    "        \n",
    "download_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4. Vérification de l'intégrité des images dans le dataset\n",
    "\n",
    "Il est essentiel de s'assurer que les images du dataset sont valides avant de les utiliser pour l'entraînement d'un modèle. Cette section présente la fonction `check_images_in_dataset` qui permet de vérifier l'intégrité de chaque image et de supprimer les fichiers corrompus ou non valides."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-16T14:08:00.292942Z",
     "start_time": "2024-10-16T14:07:45.630460Z"
    }
   },
   "outputs": [],
   "source": [
    "def check_images_in_dataset(dataset_path):\n",
    "    # Get the paths of all image files (jpg, png, etc.)\n",
    "    img_paths = glob.glob(os.path.join(dataset_path, '*/*.*'))\n",
    "\n",
    "    for img_path in tqdm.tqdm(img_paths, desc=\"Checking images\"):\n",
    "        try:\n",
    "            # Read and decode the image to check if it is valid\n",
    "            img_bytes = tf.io.read_file(img_path)\n",
    "            tf.io.decode_image(img_bytes)\n",
    "\n",
    "        except tf.errors.InvalidArgumentError as e:\n",
    "            # Catch specific TensorFlow error and remove bad files\n",
    "            print(f\"Found corrupt image: {img_path}. Error: {str(e)}. Removing it.\")\n",
    "            os.remove(img_path)\n",
    "        except Exception as e:\n",
    "            # General exception handler for other possible issues\n",
    "            print(f\"Error processing image: {img_path}. Error: {str(e)}. Removing it.\")\n",
    "            os.remove(img_path)\n",
    "\n",
    "check_images_in_dataset(paths['train_data_path'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5. Chargement et division des images en ensembles d'entraînement et de test\n",
    "\n",
    "Dans cette section, nous chargeons les images du dataset et les divisons en deux sous-ensembles : l'un pour l'entraînement (80%) et l'autre pour la validation (20%). Cette étape est cruciale pour entraîner le modèle sur une partie des données tout en réservant une portion pour évaluer ses performances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-16T14:08:01.232422Z",
     "start_time": "2024-10-16T14:08:00.386752Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load and split images into train and test sets (80%-20%)\n",
    "train_set, validation_set = tf.keras.utils.image_dataset_from_directory(\n",
    "    paths['train_data_path'],\n",
    "    validation_split=0.2,\n",
    "    subset=\"both\",\n",
    "    seed=42,\n",
    "    image_size=(image_h, image_w),\n",
    "    batch_size=batch_s,\n",
    "    labels=None,\n",
    "    label_mode=None\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6. Prétraitement des images : Ajout de bruit gaussien et redimensionnement\n",
    "\n",
    "Dans cette phase, nous ajoutons une couche de redimensionnement pour normaliser les valeurs des pixels des images et appliquons un bruit gaussien à chaque lot d'images. Ce bruit simule des perturbations réelles présentes dans des images de mauvaise qualité, comme celles numérisées avec des artefacts, afin d'entraîner un modèle capable de restaurer des images bruitées."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-16T14:08:01.350179Z",
     "start_time": "2024-10-16T14:08:01.241649Z"
    }
   },
   "outputs": [],
   "source": [
    "# Adding the resizing layer\n",
    "rescale_layer = tf.keras.layers.Rescaling(1./255)\n",
    "\n",
    "# Adds Gaussian noise to images, keeping pixel values in [0, 1].\n",
    "def add_gaussian_noise(images, min_noise=0.1, max_noise=0.5):\n",
    "    batch_size = tf.shape(images)[0]\n",
    "    noise_factor = tf.random.uniform(shape=(batch_size, 1, 1, 1), minval=min_noise, maxval=max_noise)\n",
    "    noise = tf.random.normal(shape=tf.shape(images))\n",
    "    noisy_images = images + noise_factor * noise\n",
    "    noisy_images = tf.clip_by_value(noisy_images, 0.0, 1.0)\n",
    "    return noisy_images\n",
    "\n",
    "noisy_train_set = train_set.map(\n",
    "    lambda x: (add_gaussian_noise(rescale_layer(x)), rescale_layer(x)),\n",
    "    num_parallel_calls=tf.data.AUTOTUNE\n",
    ").cache().prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "\n",
    "noisy_validation_set = validation_set.map(\n",
    "    lambda x: (add_gaussian_noise(rescale_layer(x)), rescale_layer(x)),\n",
    "    num_parallel_calls=tf.data.AUTOTUNE\n",
    ").cache().prefetch(buffer_size=tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.7. Visualisation des images originales et bruitées\n",
    "\n",
    "La visualisation est une étape importante pour s'assurer que le bruit est correctement appliqué aux images d'entraînement et pour comparer les images d'origine avec leurs versions bruitées. La fonction `display_noisy_train_images` permet d'afficher côte à côte un échantillon d'images originales et bruitées à partir du dataset d'entraînement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-16T14:08:03.964968Z",
     "start_time": "2024-10-16T14:08:01.358372Z"
    }
   },
   "outputs": [],
   "source": [
    "# Function for displaying original and noisy images\n",
    "def display_noisy_train_images(noisy_dataset, num_images=5):\n",
    "    plt.figure(figsize=(15, 6))\n",
    "\n",
    "    for noisy_images, original_images in noisy_dataset.take(1):\n",
    "        for i in range(num_images):\n",
    "            # Original pictures\n",
    "            plt.subplot(2, num_images, i + 1)\n",
    "            plt.imshow(original_images[i].numpy())\n",
    "            plt.title('Original')\n",
    "            plt.axis('off')\n",
    "\n",
    "            # Noisy images\n",
    "            plt.subplot(2, num_images, num_images + i + 1)\n",
    "            plt.imshow(noisy_images[i].numpy())\n",
    "            plt.title('Noisy')\n",
    "            plt.axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "display_noisy_train_images(noisy_train_set, num_images=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Construction de l'Auto-encodeur\n",
    "\n",
    "Un auto-encodeur est un type de réseau de neurones utilisé pour apprendre une représentation compacte (encodée) des données en entrée. Il est constitué de deux parties principales : l'encodeur et le décodeur. L'encodeur réduit les dimensions des données d'entrée, tandis que le décodeur tente de reconstruire les données originales à partir de cette représentation réduite. Ce modèle est particulièrement utile pour des tâches comme le débruitage d'images.\n",
    "\n",
    "### Explication générale : Comment fonctionne un auto-encodeur ?\n",
    "\n",
    "Un auto-encodeur prend une image en entrée et passe par plusieurs couches pour compresser l'image en une représentation latente plus petite. Ensuite, cette représentation compressée est passée à travers le décodeur, qui reconstruit l'image. L'objectif du modèle est de minimiser la différence entre l'image d'origine et l'image reconstruite, ce qui signifie qu'il apprend à capturer les caractéristiques importantes de l'image tout en éliminant le bruit ou d'autres détails inutiles.\n",
    "\n",
    "Le modèle d'auto-encodeur est optimisé avec une fonction de perte telle que l'erreur quadratique moyenne (MSE) pour comparer les pixels de l'image originale à ceux de l'image reconstruite.\n",
    "\n",
    "### 3.1. Construction de l'Auto-encodeur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-17T08:57:26.618282Z",
     "start_time": "2024-10-17T08:57:26.542695Z"
    }
   },
   "outputs": [],
   "source": [
    "def build_autoencoder(input_shape, activation_function, dropout_rate):\n",
    "\n",
    "    input_img = tf.keras.layers.Input(shape=input_shape)\n",
    "\n",
    "    # Encoder\n",
    "    x = tf.keras.layers.Conv2D(32, (3, 3), activation=activation_function, padding='same')(input_img)\n",
    "    x = tf.keras.layers.MaxPooling2D((2, 2), padding='same')(x)\n",
    "    x = tf.keras.layers.Dropout(dropout_rate)(x)\n",
    "\n",
    "    x = tf.keras.layers.Conv2D(64, (3, 3), activation=activation_function, padding='same')(x)\n",
    "    x = tf.keras.layers.MaxPooling2D((2, 2), padding='same')(x)\n",
    "    x = tf.keras.layers.Dropout(dropout_rate)(x)\n",
    "\n",
    "    x = tf.keras.layers.Conv2D(128, (3, 3), activation=activation_function, padding='same')(x)\n",
    "    x = tf.keras.layers.Dropout(dropout_rate)(x)\n",
    "\n",
    "    encoded = tf.keras.layers.MaxPooling2D((2, 2), padding='same')(x)\n",
    "\n",
    "    # Decoder\n",
    "    x = tf.keras.layers.Conv2D(128, (3, 3), activation=activation_function, padding='same')(encoded)\n",
    "    x = tf.keras.layers.UpSampling2D((2, 2))(x)\n",
    "\n",
    "    x = tf.keras.layers.Conv2D(64, (3, 3), activation=activation_function, padding='same')(x)\n",
    "    x = tf.keras.layers.UpSampling2D((2, 2))(x)\n",
    "\n",
    "    x = tf.keras.layers.Conv2D(32, (3, 3), activation=activation_function, padding='same')(x)\n",
    "    x = tf.keras.layers.UpSampling2D((2, 2))(x)\n",
    "\n",
    "    decoded = tf.keras.layers.Conv2D(3, (3, 3), activation=activation_function, padding='same')(x)\n",
    "\n",
    "    # Self-encoding model\n",
    "    autoencoder = tf.keras.Model(input_img, decoded)\n",
    "    return autoencoder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Explication du modèle couche par couche\n",
    "\n",
    "#### 1. **Encodeur** \n",
    "\n",
    "L'encodeur est responsable de la compression des données d'entrée en une représentation plus compacte. Cela se fait à l'aide de couches convolutives et de pooling pour réduire les dimensions tout en capturant les caractéristiques importantes de l'image.\n",
    "\n",
    "- **Première couche Conv2D :**\n",
    "  - **Type de couche** : Convolution 2D\n",
    "  - **Nombre de filtres** : 32 filtres\n",
    "  - **Taille du noyau (kernel)** : (3, 3)\n",
    "  - **Fonction d'activation** : ReLU\n",
    "  - **Padding** : `same` (cela conserve les dimensions d'entrée)\n",
    "  - **Rôle** : Appliquer 32 filtres de convolution pour extraire des caractéristiques basiques des images (bords, textures).\n",
    "\n",
    "- **MaxPooling2D :**\n",
    "  - **Type de couche** : Pooling (sous-échantillonnage)\n",
    "  - **Taille du pool** : (2, 2)\n",
    "  - **Padding** : `same` (cela conserve les dimensions tout en réduisant la résolution)\n",
    "  - **Rôle** : Réduit la taille de l'image de moitié, en prenant la valeur maximale dans chaque fenêtre de 2x2 pixels, tout en préservant les caractéristiques les plus saillantes.\n",
    "\n",
    "- **Deuxième couche Conv2D :**\n",
    "  - **Nombre de filtres** : 64 filtres\n",
    "  - **Taille du noyau** : (3, 3)\n",
    "  - **Fonction d'activation** : ReLU\n",
    "  - **Padding** : `same`\n",
    "  - **Rôle** : Augmenter la profondeur des caractéristiques extraites avec 64 filtres, capturant ainsi des détails plus complexes.\n",
    "\n",
    "- **MaxPooling2D :**\n",
    "  - Identique à la première couche de MaxPooling2D, cette couche réduit encore la résolution tout en conservant les caractéristiques importantes.\n",
    "\n",
    "- **Troisième couche Conv2D :**\n",
    "  - **Nombre de filtres** : 128 filtres\n",
    "  - **Taille du noyau** : (3, 3)\n",
    "  - **Fonction d'activation** : ReLU\n",
    "  - **Padding** : `same`\n",
    "  - **Rôle** : Extraire des caractéristiques encore plus fines avec plus de filtres, capturant des structures détaillées de l'image.\n",
    "\n",
    "- **MaxPooling2D (encoded layer)** :\n",
    "  - Cette couche finale de MaxPooling dans l'encodeur réduit encore l'image pour obtenir la représentation encodée. C'est la sortie compressée de l'image, appelée \"représentation latente\". Elle a une dimension beaucoup plus petite, mais conserve les informations essentielles nécessaires à la reconstruction.\n",
    "\n",
    "#### 2. **Décodeur**\n",
    "\n",
    "Le décodeur est la partie qui prend la représentation encodée et tente de reconstruire l'image d'origine. Il utilise des couches convolutives et d'up-sampling pour restaurer les dimensions de l'image à partir de la représentation latente.\n",
    "\n",
    "- **Première couche Conv2D :**\n",
    "  - **Nombre de filtres** : 128 filtres\n",
    "  - **Taille du noyau** : (3, 3)\n",
    "  - **Fonction d'activation** : ReLU\n",
    "  - **Padding** : `same`\n",
    "  - **Rôle** : Commence le processus de reconstruction de l'image avec les mêmes caractéristiques extraites par la dernière couche de convolution de l'encodeur.\n",
    "\n",
    "- **UpSampling2D :**\n",
    "  - **Type de couche** : Upsampling (sur-échantillonnage)\n",
    "  - **Taille de la fenêtre** : (2, 2)\n",
    "  - **Rôle** : Double les dimensions de l'image à chaque étape pour restaurer la résolution originale, inversant l'effet de MaxPooling.\n",
    "\n",
    "- **Deuxième couche Conv2D :**\n",
    "  - Identique à la précédente, mais avec 64 filtres, réduisant progressivement la complexité des caractéristiques au fur et à mesure que l'on s'approche de l'image reconstruite.\n",
    "\n",
    "- **UpSampling2D :**\n",
    "  - Identique à la première couche d'UpSampling, elle augmente encore les dimensions de l'image.\n",
    "\n",
    "- **Troisième couche Conv2D :**\n",
    "  - **Nombre de filtres** : 32 filtres\n",
    "  - **Taille du noyau** : (3, 3)\n",
    "  - **Fonction d'activation** : ReLU\n",
    "  - **Padding** : `same`\n",
    "  - **Rôle** : Applique des filtres moins complexes pour affiner la reconstruction de l'image.\n",
    "\n",
    "- **UpSampling2D :**\n",
    "  - Effectue une dernière opération de sur-échantillonnage pour restaurer l'image à sa taille originale.\n",
    "\n",
    "- **Dernière couche Conv2D :**\n",
    "  - **Nombre de filtres** : 3 (correspondant aux 3 canaux de couleur RGB)\n",
    "  - **Taille du noyau** : (3, 3)\n",
    "  - **Fonction d'activation** : Sigmoïde\n",
    "  - **Padding** : `same`\n",
    "  - **Rôle** : Reconstituer les valeurs de pixels de l'image reconstruite dans l'intervalle [0, 1].\n",
    "\n",
    "#### 3. **Compilation du modèle**\n",
    "\n",
    "Une fois le modèle créé, il est compilé avec les paramètres suivants :\n",
    "- **Optimiseur** : `adam` — Un optimiseur populaire basé sur la descente de gradient adaptative.\n",
    "- **Fonction de perte** : `mean squared error (mse)` — Calcul la différence entre l'image originale et l'image reconstruite pixel par pixel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Configuration des callbacks pour l'entraînement du modèle\n",
    "\n",
    "Les callbacks permettent d'automatiser et d'optimiser l'entraînement du modèle en surveillant les performances, en sauvegardant les poids et en stoppant l'entraînement en cas de stagnation. La fonction ``get_callbacks()`` regroupe quatre callbacks essentiels :\n",
    "\n",
    "- **Sauvegarde des checkpoints :** Le callback ``ModelCheckpoint`` enregistre les poids du modèle à chaque amélioration de la perte de validation (``val_loss``). Cela permet de conserver la meilleure version du modèle.\n",
    "\n",
    "- **Suivi avec TensorBoard :** Le callback TensorBoard génère des logs pour visualiser l'évolution des métriques, le graphe du modèle et les images des données. C'est un outil de suivi visuel très utile.\n",
    "\n",
    "- **Arrêt anticipé (EarlyStopping) :** Ce callback stoppe l'entraînement si la perte de validation cesse de s'améliorer pendant 5 époques, évitant ainsi le surentraînement et optimisant la durée d'entraînement.\n",
    "\n",
    "- **Suivi en temps réel avec WandB :** Le callback WandbMetricsLogger enregistre les métriques dans Weights & Biases (WandB), facilitant la visualisation et le suivi des performances du modèle en temps réel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-18T09:12:27.934581Z",
     "start_time": "2024-10-18T09:12:27.925390Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_callbacks(wandb=False):\n",
    "    # Create a callback that saves the model's weights at each epoch where validation loss improves\n",
    "    checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath=paths['checkpoint_path'] + \"/weights-epoch-{epoch:02d}-{val_loss:.2f}.weights.h5\",\n",
    "        save_weights_only=True,\n",
    "        save_best_only=True,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    # Create a TensorBoard callback to log training metrics, model graphs, and images for visualization\n",
    "    tensorboard_callback = tf.keras.callbacks.TensorBoard(\n",
    "        log_dir=paths['log_path'],\n",
    "        histogram_freq=1,\n",
    "        write_graph=True,\n",
    "        write_images=True,\n",
    "        update_freq='epoch',\n",
    "        profile_batch=0,\n",
    "        embeddings_freq=0\n",
    "    )\n",
    "\n",
    "    # Set up early stopping to halt training if validation loss stops improving for a set number of epochs\n",
    "    early_callback = tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        min_delta=0,\n",
    "        patience=5,\n",
    "        verbose=0,\n",
    "        mode='auto',\n",
    "        baseline=None,\n",
    "        restore_best_weights=True,\n",
    "        start_from_epoch=0\n",
    "    )\n",
    "\n",
    "    if wandb:\n",
    "        # Wandb callback\n",
    "        wandb_callback = wandb.keras.WandbMetricsLogger()\n",
    "\n",
    "        return [\n",
    "            checkpoint_callback,\n",
    "            tensorboard_callback,\n",
    "            early_callback,\n",
    "            wandb_callback\n",
    "        ]\n",
    "    \n",
    "    return [\n",
    "        checkpoint_callback,\n",
    "        tensorboard_callback,\n",
    "        early_callback\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Plan d'Expériences avec Weights & Biases (Wandb)\n",
    "\n",
    "Pour optimiser les performances de notre autoencodeur, nous avons mis en place un **plan d'expériences** structuré à l'aide de **Wandb Sweep**. L'objectif est d'explorer systématiquement différentes configurations d'hyperparamètres afin de déterminer la combinaison optimale qui maximise les performances du modèle.\n",
    "\n",
    "L'optimisation des hyperparamètres est cruciale pour améliorer la précision, accélérer la convergence et renforcer la capacité de généralisation du modèle. Nous avons opté pour une approche d'optimisation bayésienne, qui, contrairement aux méthodes de recherche par grille ou aléatoire, utilise les résultats des expériences précédentes pour guider les sélections futures d'hyperparamètres. Cela permet de réduire le nombre total d'itérations nécessaires tout en améliorant l'efficacité de la recherche.\n",
    "\n",
    "#### 1. Définition du Plan d'Expériences\n",
    "\n",
    "Le plan d'expériences est défini par un ensemble d'hyperparamètres que nous souhaitons optimiser. Le code suivant établit la configuration du sweep avec des valeurs spécifiques pour chaque hyperparamètre :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_config = {\n",
    "    'method': 'bayes',\n",
    "    'metric': {\n",
    "        'name': 'val_loss',\n",
    "        'goal': 'minimize'\n",
    "    },\n",
    "    'parameters': {\n",
    "        'learning_rate': {\n",
    "            'values': [0.001, 0.0001, 0.00001]\n",
    "        },\n",
    "        'batch_size': {\n",
    "            'values': [32, 64, 128]\n",
    "        },\n",
    "        'optimizer': {\n",
    "            'values': ['adam', 'sgd', 'rmsprop']\n",
    "        },\n",
    "        'dropout_rate': {\n",
    "            'values': [0.0, 0.2, 0.5]\n",
    "        },\n",
    "        'activation_function': {\n",
    "            'values': ['relu', 'tanh', 'sigmoid']\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les hyperparamètres considérés dans ce plan d'expériences sont les suivants :\n",
    "\n",
    "- **Taux d'apprentissage (learning rate)** : Paramètre essentiel influençant la vitesse de convergence du modèle. Nous testons des taux de 0.001, 0.0001 et 0.00001 pour observer comment le modèle ajuste ses poids au cours de l'entraînement.\n",
    "\n",
    "- **Taille des lots (batch size)** : La taille du lot affecte la stabilité et la rapidité de l'apprentissage. Des lots plus petits peuvent favoriser une meilleure généralisation, mais nécessitent davantage d'itérations pour une mise à jour complète des poids.\n",
    "\n",
    "- **Optimiseur** : Nous évaluons trois optimiseurs — `adam`, `sgd` et `rmsprop` — chacun ayant des propriétés distinctes en termes de vitesse de convergence et de stabilité.\n",
    "\n",
    "- **Taux de dropout** : Le dropout est utilisé pour la régularisation en désactivant aléatoirement une fraction des neurones pendant l'entraînement, ce qui aide à prévenir le surapprentissage. Nous testons des valeurs de 0.0, 0.2 et 0.5.\n",
    "\n",
    "- **Fonction d'activation** : Les fonctions d'activation introduisent de la non-linéarité dans le modèle, essentielle pour l'apprentissage de représentations complexes. Nous testons les fonctions `relu`, `tanh` et `sigmoid`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Entraînement du Modèle et Optimisation avec Wandb Sweep\n",
    "\n",
    "La fonction suivante, `train_model`, est utilisée pour entraîner l'autoencodeur avec les hyperparamètres sélectionnés par Wandb Sweep. Cette fonction prend en entrée les hyperparamètres, construit et compile le modèle, puis l'entraîne sur les ensembles de données d'entraînement et de validation bruités."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-10-17T08:57:26.774839Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_model(config=None):\n",
    "    # Initialize W&B run\n",
    "    wandb.init(config=config)\n",
    "\n",
    "    # Use the config to set hyperparameters\n",
    "    optimizer = wandb.config.optimizer\n",
    "    learning_rate =  wandb.config.learning_rate\n",
    "    batch_size =  wandb.config.batch_size\n",
    "    activation_function =  wandb.config.activation_function\n",
    "    dropout_rate =  wandb.config.dropout_rate\n",
    "\n",
    "    noisy_train_set = train_set.map(\n",
    "        lambda x: (add_gaussian_noise(rescale_layer(x)), rescale_layer(x)),\n",
    "        num_parallel_calls=tf.data.AUTOTUNE\n",
    "    ).cache().prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "\n",
    "    # Adds Gaussian noise and rescales the test set images, optimizing with parallel processing.\n",
    "    noisy_test_set = validation_set.map(\n",
    "        lambda x: (add_gaussian_noise(rescale_layer(x)), rescale_layer(x)),\n",
    "        num_parallel_calls=tf.data.AUTOTUNE\n",
    "    ).cache().prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "\n",
    "\n",
    "    # Build and compile the autoencoder model\n",
    "    autoencoder = build_autoencoder((image_h, image_w, 3), activation_function, dropout_rate)\n",
    "    if optimizer == 'adam':\n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    elif optimizer == 'sgd':\n",
    "        optimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate)\n",
    "    elif optimizer == 'rmsprop':\n",
    "        optimizer = tf.keras.optimizers.RMSprop(learning_rate=learning_rate)\n",
    "\n",
    "    autoencoder.compile(optimizer=optimizer, loss='mse')\n",
    "\n",
    "    # Train the model\n",
    "    history = autoencoder.fit(\n",
    "        noisy_train_set,\n",
    "        validation_data=noisy_test_set,\n",
    "        batch_size=batch_size,\n",
    "        epochs=10,\n",
    "        callbacks=get_callbacks()\n",
    "    )\n",
    "\n",
    "    wandb.log({\"val_loss\": history.history['val_loss'][-1]})\n",
    "\n",
    "    return history\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Lancement du Wandb Sweep\n",
    "\n",
    "Une fois la fonction d'entraînement définie, nous lançons le **Wandb Sweep** pour exécuter jusqu'à 100 expériences avec différentes combinaisons d'hyperparamètres. Chaque expérience entraîne le modèle avec une configuration unique, et les résultats sont automatiquement enregistrés pour analyse et comparaison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_id = wandb.sweep(sweep_config, project='livrable_2')\n",
    "\n",
    "wandb.agent(sweep_id, train_model, count=100)\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Exemple d'image](./images/wandb.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Analyse des Hyperparamètres avec un Graphique Parallèle\n",
    "\n",
    "#### Visualisation des Résultats avec un Graphique Parallèle\n",
    "\n",
    "Afin d'analyser les performances de notre modèle en fonction des différentes combinaisons d'hyperparamètres, nous utilisons un **graphique parallèle**. Ce type de graphique est particulièrement utile pour visualiser les relations entre plusieurs variables et identifier les meilleures configurations d'hyperparamètres en termes de performance.\n",
    "\n",
    "Le graphique que nous avons généré présente les résultats du **sweep Wandb** réalisé avec différentes combinaisons d'hyperparamètres, en traçant leur relation avec la **perte de validation (val_loss)**. Chaque ligne du graphique correspond à une expérience unique avec une combinaison donnée d'hyperparamètres, et la couleur de la ligne reflète la performance de cette configuration.\n",
    "\n",
    "#### Lecture du Graphique Parallèle\n",
    "\n",
    "Le graphique est composé de plusieurs axes verticaux, chacun représentant un hyperparamètre différent :\n",
    "- **Activation function** : Les fonctions d'activation utilisées dans l'autoencodeur, telles que `sigmoid`, `tanh`, et `relu`.\n",
    "- **Batch size** : La taille des lots, qui détermine combien d'exemples sont passés à travers le réseau avant la mise à jour des poids.\n",
    "- **Dropout rate** : Le taux de dropout, qui représente la fraction de neurones ignorés durant l'entraînement pour éviter le surapprentissage.\n",
    "- **Learning rate** : Le taux d'apprentissage, qui détermine la rapidité avec laquelle le modèle ajuste ses poids.\n",
    "- **Optimizer** : L'optimiseur utilisé, parmi `adam`, `sgd` et `rmsprop`.\n",
    "- **Validation Loss (val_loss)** : Le dernier axe du graphique représente la **perte de validation**, qui est l'objectif que nous cherchons à minimiser.\n",
    "\n",
    "Les **lignes** du graphique traversent chaque axe et représentent une configuration unique d'hyperparamètres testée. Chaque ligne passe par une valeur spécifique sur chaque axe, montrant la configuration choisie pour cette expérience.\n",
    "\n",
    "#### Couleurs et Interprétation\n",
    "\n",
    "Les **couleurs** des lignes dans le graphique sont essentielles pour comprendre les performances des modèles :\n",
    "- Les lignes **violettes** représentent des combinaisons d'hyperparamètres ayant une faible perte de validation, ce qui indique une bonne performance du modèle.\n",
    "- Les lignes **jaunes** et **oranges** représentent des combinaisons d'hyperparamètres avec des pertes de validation plus élevées, indiquant des performances moins bonnes.\n",
    "\n",
    "L'objectif est d'identifier les lignes les plus violettes, car elles correspondent aux combinaisons d'hyperparamètres qui minimisent la perte de validation.\n",
    "\n",
    "#### Sélection des Meilleurs Hyperparamètres\n",
    "\n",
    "En examinant les lignes violettes, nous pouvons déduire les hyperparamètres qui mènent aux meilleures performances :\n",
    "- **Fonction d'activation** : La fonction **sigmoid** a produit des lignes plus violettes, ce qui suggère qu'elle donne de bonnes performances en termes de reconstruction d'images.\n",
    "- **Batch size** : Une **taille de lot de 32** est souvent associée aux lignes violettes, ce qui suggère que c'est un bon compromis entre stabilité et vitesse d'entraînement.\n",
    "- **Dropout rate** : Un **taux de dropout de 0.0** a montré les meilleures performances, suggérant que la régularisation par dropout n'est pas nécessaire dans ce cas.\n",
    "- **Learning rate** : Un **taux d'apprentissage de 0.0001** est optimal, permettant au modèle de converger lentement mais de manière stable.\n",
    "- **Optimizer** : L'optimiseur **RMSprop** semble être le meilleur choix, car il est souvent associé aux lignes violettes.\n",
    "\n",
    "#### Conclusion\n",
    "\n",
    "Le graphique parallèle nous a permis d'analyser visuellement l'impact des différentes combinaisons d'hyperparamètres sur les performances du modèle. Sur la base de cette analyse, nous avons sélectionné les hyperparamètres suivants comme étant les meilleurs pour notre autoencodeur :\n",
    "- **Fonction d'activation** : Sigmoid\n",
    "- **Batch size** : 32\n",
    "- **Dropout rate** : 0.0\n",
    "- **Learning rate** : 0.0001\n",
    "- **Optimizer** : Adam\n",
    "\n",
    "Ces hyperparamètres ont produit les meilleures performances avec une perte de validation minimale, et seront utilisés pour entraîner le modèle final."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_autoencoder = build_autoencoder((image_h, image_w, 3), \"sigmoid\", 0)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
    "final_autoencoder.compile(optimizer=optimizer, loss='mse')\n",
    "\n",
    "history = final_autoencoder.fit(\n",
    "        noisy_train_set,\n",
    "        validation_data=noisy_validation_set,\n",
    "        batch_size=32,\n",
    "        epochs=200,\n",
    "        callbacks=get_callbacks()\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Exemple d'image](./images/loss.png)\n",
    "\n",
    "Le graphique ci-dessus illustre l'évolution de la **perte d'entraînement** (ligne bleue) et de la **perte de validation** (ligne orange) au cours des 50 époques d'entraînement de notre modèle d'autoencodeur.\n",
    "\n",
    "- **Convergence des pertes** : On observe que les deux courbes de perte (entraînement et validation) diminuent régulièrement au fil des époques, ce qui indique que le modèle apprend efficacement à reconstruire les images bruitées. Après environ 25 époques, la perte commence à se stabiliser, suggérant que le modèle converge.\n",
    "  \n",
    "- **Bonne généralisation** : La courbe de perte de validation suit de très près la courbe de perte d'entraînement, sans signes de surapprentissage (*overfitting*). Cela montre que le modèle généralise bien aux données de validation, ce qui est crucial pour la robustesse du modèle.\n",
    "  \n",
    "- **Stabilité du modèle** : Vers la fin de l'entraînement, les courbes montrent une certaine stabilité, avec de légères fluctuations mineures, indiquant que le modèle atteint un plateau de performance sans surentraînement.\n",
    "\n",
    "En résumé, le modèle semble bien ajusté, avec une bonne convergence et une généralisation satisfaisante sur le jeu de validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Comparaison\n",
    "La fonction `display_denoised_images` sert à visualiser les résultats du modèle de débruitage. Elle commence par créer une figure avec plt.figure(figsize=(15, 8)) pour définir l'affichage. Ensuite, elle extrait un lot d'images bruitées et leurs images originales.\n",
    "\n",
    "La fonction prédit les images débruitées à l'aide du modèle, puis affiche trois versions pour chaque image : la version bruitée, la version débruitée et la version originale. Enfin, elle ajuste l'espacement des sous-figures et affiche le tout avec plt.show(). Cela permet de comparer visuellement les performances du modèle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-16T15:32:21.704301Z",
     "start_time": "2024-10-16T15:32:20.489670Z"
    }
   },
   "outputs": [],
   "source": [
    "def display_denoised_images(model, dataset, num_images=5):\n",
    "    plt.figure(figsize=(15, 8))\n",
    "\n",
    "    for noisy_images, original_images in dataset.take(1):\n",
    "        # Predicting denoised pictures\n",
    "        denoised_images = model.predict(noisy_images)\n",
    "        \n",
    "        for i in range(num_images):\n",
    "            # Noisy images\n",
    "            plt.subplot(3, num_images, i + 1)\n",
    "            plt.imshow(noisy_images[i].numpy())\n",
    "            plt.title('Noisy')\n",
    "            plt.axis('off')\n",
    "            \n",
    "            # Denoised images\n",
    "            plt.subplot(3, num_images, num_images + i + 1)\n",
    "            plt.imshow(denoised_images[i])\n",
    "            plt.title('Denoised')\n",
    "            plt.axis('off')\n",
    "            \n",
    "            # Original picture\n",
    "            plt.subplot(3, num_images, 2 * num_images + i + 1)\n",
    "            plt.imshow(original_images[i].numpy())\n",
    "            plt.title('Original')\n",
    "            plt.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Displaying results\n",
    "display_denoised_images(final_autoencoder, noisy_validation_set, num_images=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Exemple d'image](./images/final.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Conclusion\n",
    "Le projet Leyenda vise à développer une solution pour améliorer la qualité des images numérisées, facilitant ainsi leur traitement par des algorithmes de Deep Learning. L'accent est mis sur le débruitage, une étape essentielle du prétraitement, qui permet de réduire le bruit numérique résultant de divers facteurs tels que les erreurs de capteurs ou des conditions de numérisation inadaptées.\n",
    "\n",
    "Pour atteindre cet objectif, des auto-encodeurs à convolution sont utilisés, exploitant leur capacité à capturer les caractéristiques spatiales des images tout en éliminant les informations non désirées. Le processus comprend plusieurs étapes clés, de l'importation et de l'exploration des données, à la vérification de l'intégrité des images, en passant par l'ajout de bruit gaussien pour simuler des conditions réelles.\n",
    "\n",
    "La construction et l'entraînement d'un modèle d'auto-encodeur, optimisé à l'aide de la fonction de perte d'erreur quadratique moyenne, permettent de reconstruire des images de haute qualité à partir de leurs versions bruitées. Des callbacks sont configurés pour superviser l'entraînement et sauvegarder les meilleures performances, garantissant ainsi l'efficacité du modèle.\n",
    "\n",
    "Les résultats de ce projet permettront d'améliorer significativement le traitement d'images par des algorithmes de classification et de reconnaissance, posant ainsi les bases pour des applications avancées dans divers domaines nécessitant une analyse d'images de qualité."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
