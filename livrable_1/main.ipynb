{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2188a303",
   "metadata": {},
   "source": [
    "# **Projet de Classification d'Images - Livrable 1**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e10db5",
   "metadata": {},
   "source": [
    "## **1. Introduction du projet**\n",
    "\n",
    "Ce projet s'inscrit dans le cadre d'une solution de **classification d'images** pour l'entreprise TouNum, spécialisée dans la numérisation de documents. L'objectif principal de ce livrable est de développer un modèle de **réseau de neurones convolutif (CNN)** capable de distinguer automatiquement des photos d'autres types d'images (schémas, peintures, dessins, scans de textes, etc.). Cette classification permettra à l'entreprise d'automatiser le tri des images, améliorant ainsi l'efficacité des processus de traitement et de gestion de grandes quantités de données numérisées.\n",
    "\n",
    "La problématique de la classification d'images est un sujet central dans le domaine du **deep learning**, notamment dans les contextes où les datasets sont hétérogènes, comme c'est le cas ici. Les images à classifier présentent des caractéristiques visuelles très différentes : les photos et les peintures peuvent partager des similarités complexes, tandis que les schémas ou les textes scannés sont visuellement distincts. L'enjeu est donc de concevoir un modèle suffisamment robuste pour non seulement distinguer ces différentes classes mais aussi généraliser à des nouvelles données.\n",
    "\n",
    "Pour cela, nous allons nous appuyer sur un **modèle de réseau de neurones convolutif (CNN)**, une architecture qui a fait ses preuves pour les tâches de classification d'images, en raison de sa capacité à capturer efficacement les motifs locaux à différentes échelles. Dans ce livrable, nous nous concentrons sur une classification binaire qui vise à distinguer les photos du reste des images.\n",
    "\n",
    "Le modèle sera implémenté en **Python** à l'aide du framework **TensorFlow**, largement utilisé pour le développement de modèles de deep learning. Pour garantir une performance optimale, des techniques telles que l'**augmentation des données**, la **régularisation** et l'utilisation de **callbacks** (comme l'early stopping et le checkpointing) seront explorées. Nous examinerons également les performances du modèle via des métriques comme l'**accuracy** et la **perte**, en traçant leur évolution pendant l'entraînement.\n",
    "\n",
    "En fin de compte, ce projet vise à fournir à TouNum une solution automatisée qui s'intègre dans leur chaîne de production, tout en offrant une base solide pour des développements futurs dans des domaines connexes comme l'**annotation d'images** ou le **captioning automatique**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e4dac5e",
   "metadata": {},
   "source": [
    "## **2. Préparation des données**\n",
    "\n",
    "### **2.1. Importation des bibliothèques**\n",
    "Dans cette étape, nous importons un ensemble de bibliothèques Python nécessaires pour l'entraînement du modèle et la manipulation des données :\n",
    "\n",
    "- `warnings`: Pour filtrer les avertissements non critiques afin de rendre les sorties du notebook plus lisibles.\n",
    "- `os`: Permet de gérer les opérations liées au système d'exploitation, telles que la création de dossiers et la gestion des chemins.\n",
    "- `datetime`: Utilisée pour générer des horodatages et organiser les fichiers de logs et de modèles.\n",
    "- `zipfile`: Pour manipuler les fichiers compressés au format `.zip`, utile pour décompresser les datasets.\n",
    "- `tqdm`: Permet d'afficher des barres de progression pour suivre les opérations longues.\n",
    "- `gdown`: Utilisé pour télécharger des fichiers à partir de Google Drive (pratique pour récupérer des datasets).\n",
    "- `glob`: Permet de trouver des fichiers et des répertoires en fonction de motifs spécifiques.\n",
    "- `matplotlib.pyplot`: La bibliothèque de visualisation pour tracer les courbes et graphiques nécessaires à l'analyse des résultats.\n",
    "- `tensorflow`: La bibliothèque principale utilisée pour créer et entraîner les réseaux de neurones."
   ]
  },
  {
   "cell_type": "code",
   "id": "f341b0e7fefe1287",
   "metadata": {},
   "source": [
    "import warnings, os, datetime, zipfile, tqdm, gdown, glob, random, shutil, pytz\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "fcd4f28d",
   "metadata": {},
   "source": [
    "### **2.2. Configuration des chemins et des répertoires**\n",
    "\n",
    "Afin de bien organiser le projet, plusieurs chemins de fichiers et répertoires sont créés pour stocker les datasets, les modèles et les logs d'entraînement. Cela facilite la gestion des fichiers et permet de garder une structure claire."
   ]
  },
  {
   "cell_type": "code",
   "id": "859a5b08dffbeebc",
   "metadata": {},
   "source": [
    "timezone = pytz.timezone('Europe/Paris')\n",
    "now = datetime.datetime.now(timezone).strftime('%Y.%m.%d-%H.%M.%S')\n",
    "\n",
    "project_path = os.getcwd()\n",
    "\n",
    "paths = {\n",
    "    \"data_path\": f\"{project_path}/data\",\n",
    "    \"train_data_path\": f\"{project_path}/data/train\",\n",
    "    \"validation_data_path\": f\"{project_path}/data/validation\",\n",
    "    \"unsorted_validation_data_path\": f\"{project_path}/data/validation/unsorted\",\n",
    "    \"sorted_validation_data_path\": f\"{project_path}/data/validation/sorted\",\n",
    "    \"model_path\": f\"{project_path}/models\",\n",
    "    \"checkpoint_path\": f\"{project_path}/weights/model_early\",\n",
    "    \"log_path\": f\"{project_path}/logs/fit/{now}_model\",\n",
    "    \"plot_test_plan\": f\"{project_path}/test_plan\"\n",
    "}\n",
    "\n",
    "paths_to_delete = [\n",
    "    \"unsorted_validation_data_path\",\n",
    "    \"sorted_validation_data_path\"\n",
    "]\n",
    "\n",
    "for key, path in paths.items():\n",
    "    if key in paths_to_delete and os.path.exists(path):\n",
    "        print(f\"Deleting {path}\")\n",
    "        shutil.rmtree(path)\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "\n",
    "image_h = 180\n",
    "image_w = 180\n",
    "batch_s = 32"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "0733d349",
   "metadata": {},
   "source": [
    "### **2.3. Téléchargement et extraction du dataset**\n",
    "\n",
    "Pour entraîner le modèle de classification, nous avons besoin d'un dataset contenant les images à classer. Le dataset est stocké dans un fichier compressé `.zip` disponible sur Google Drive. La fonction suivante télécharge ce dataset, si nécessaire, et l'extrait dans le répertoire approprié."
   ]
  },
  {
   "cell_type": "code",
   "id": "4de95ce62a4da63f",
   "metadata": {},
   "source": [
    "def download_dataset(force=False):\n",
    "    \n",
    "    dataset_path = f\"{paths['train_data_path']}/dataset_livrable_1.zip\"\n",
    "    \n",
    "    if os.path.exists(dataset_path) and force == False:\n",
    "        print(\"Dataset already downloded\")\n",
    "    else:\n",
    "        print(\"Downloading dataset\")\n",
    "        url = 'https://drive.google.com/uc?export=download&id=1vhNmk5omNcX3g9enqETvFR_2vdVONsxo'\n",
    "        gdown.download(url, dataset_path, quiet=False)\n",
    "\n",
    "    if len(os.listdir(paths['train_data_path'])) == 6 and force == False:\n",
    "        print(\"Dataset already exported\")\n",
    "    else:\n",
    "        print(\"Exporting dataset\")\n",
    "        with zipfile.ZipFile(dataset_path, 'r') as zip_ref:\n",
    "            files = zip_ref.infolist()\n",
    "            with tqdm.tqdm(total=len(files), desc=\"Décompression\", unit=\"fichier\") as pbar:\n",
    "                    for file in files:\n",
    "                        zip_ref.extract(file, paths['train_data_path'])\n",
    "                        pbar.update(1)\n",
    "            zip_ref.extractall(paths['train_data_path'])\n",
    "            print(f\"Dataset downloaded {paths['train_data_path']}\")\n",
    "        \n",
    "download_dataset()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "705e4454",
   "metadata": {},
   "source": [
    "### **2.4. Vérification de l'intégrité des images**\n",
    "\n",
    "Il est important de s'assurer que toutes les images du dataset sont valides avant de les utiliser pour l'entraînement du modèle. Des fichiers corrompus ou non valides pourraient causer des erreurs lors du chargement ou de l'entraînement du modèle. La fonction suivante permet de parcourir toutes les images du dataset et de vérifier si elles peuvent être correctement lues par TensorFlow."
   ]
  },
  {
   "cell_type": "code",
   "id": "cdc85762dffd2eb3",
   "metadata": {},
   "source": [
    "def check_images_in_dataset(dataset_path):\n",
    "    img_paths = glob.glob(os.path.join(dataset_path, '*/*.*'))\n",
    "\n",
    "    for img_path in tqdm.tqdm(img_paths, desc=\"Checking images\"):\n",
    "        try:\n",
    "            img_bytes = tf.io.read_file(img_path)\n",
    "            tf.io.decode_image(img_bytes)\n",
    "            \n",
    "        except tf.errors.InvalidArgumentError as e:\n",
    "            print(f\"Found bad file: {img_path}. Removing it.\")\n",
    "            os.remove(img_path)\n",
    "\n",
    "check_images_in_dataset(paths['train_data_path'])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "45c0536e",
   "metadata": {},
   "source": [
    "### **2.5. Création du jeu de validation**\n",
    "\n",
    "Il est essentiel de disposer d'un jeu de validation distinct pour évaluer les performances du modèle sur des données qu'il n'a pas encore vues. La fonction suivante permet de copier un certain nombre d'images (par défaut 20) depuis chaque sous-dossier d'entraînement vers un dossier de validation.\n",
    "\n",
    "Cette étape est cruciale pour s'assurer que le modèle est capable de bien généraliser, c'est-à-dire d'effectuer des prédictions précises sur des images nouvelles, et non simplement de mémoriser les images d'entraînement."
   ]
  },
  {
   "cell_type": "code",
   "id": "49bce5c9ab1e70be",
   "metadata": {},
   "source": [
    "def create_validation_set(num_files_to_copy=20):\n",
    "    for folder in os.listdir(paths['train_data_path']):\n",
    "        if folder.endswith(\".zip\"):\n",
    "            continue\n",
    "        folder_path = os.path.join(paths['train_data_path'], folder)\n",
    "        files = os.listdir(folder_path)\n",
    "        files_to_copy = random.sample(files, num_files_to_copy)\n",
    "        for file in files_to_copy:\n",
    "            src_file = os.path.join(folder_path, file)\n",
    "            dest_file = os.path.join(paths['unsorted_validation_data_path'], file)\n",
    "            shutil.copy2(src_file, dest_file)\n",
    "\n",
    "create_validation_set()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "97abc7e1",
   "metadata": {},
   "source": [
    "### **2.6. Chargement des données d'entraînement et de test**\n",
    "\n",
    "Pour entraîner et évaluer le modèle, nous avons besoin de charger les images depuis les répertoires dans lesquels elles sont stockées. TensorFlow propose une méthode pratique avec `image_dataset_from_directory` qui permet de charger directement les images depuis un répertoire et de les transformer en un format compatible avec les réseaux de neurones."
   ]
  },
  {
   "cell_type": "code",
   "id": "93073c21b1dc2d70",
   "metadata": {},
   "source": [
    "train_set, test_set = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    paths['train_data_path'],\n",
    "    validation_split=0.2,\n",
    "    subset=\"both\",\n",
    "    seed=42,\n",
    "    image_size=(image_h, image_w),\n",
    "    batch_size=batch_s,\n",
    "    labels=\"inferred\",\n",
    "    label_mode=\"int\"\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "6e37354c",
   "metadata": {},
   "source": [
    "### **2.7. Extraction des noms des classes**\n",
    "\n",
    "Après avoir chargé les datasets d'entraînement et de validation, nous pouvons extraire les noms des classes à partir du dataset. TensorFlow fournit automatiquement les noms de classes en fonction des sous-dossiers du répertoire de données. Cela nous permet de mieux comprendre la répartition des catégories dans les données."
   ]
  },
  {
   "cell_type": "code",
   "id": "745d5f2f0b508a90",
   "metadata": {},
   "source": [
    "class_names = test_set.class_names\n",
    "class_num = len(class_names)\n",
    "print(class_names)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "c6f3471e",
   "metadata": {},
   "source": [
    "### **2.8. Prétraitement des images**\n",
    "\n",
    "Avant d'envoyer les images au modèle de réseau de neurones pour l'entraînement, il est important de les préparer de manière cohérente. Cette étape consiste à redimensionner toutes les images à une taille commune tout en conservant leur ratio d'aspect. Cela garantit que les images ont des dimensions uniformes (180*180 pixels) sans distorsion, grâce à l'ajout de padding si nécessaire."
   ]
  },
  {
   "cell_type": "code",
   "id": "a1d20d402e527a3e",
   "metadata": {},
   "source": [
    "def preprocess_image(image, label):\n",
    "    image = tf.image.resize_with_pad(image, image_h, image_w)\n",
    "    return image, label\n",
    "\n",
    "train_set = train_set.map(preprocess_image)\n",
    "test_set = test_set.map(preprocess_image)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "ba3e9c02",
   "metadata": {},
   "source": [
    "### **2.9. Visualisation des images**\n",
    "\n",
    "Avant d'entraîner le modèle, il est utile de visualiser quelques exemples d'images du dataset pour vérifier que le chargement et le prétraitement des données ont été correctement effectués. La fonction `print_img()` affiche un échantillon d'images avec leurs étiquettes respectives."
   ]
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "def print_img(dataset, num_images=9):\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    for images, labels in dataset.take(1):\n",
    "        for i in range(num_images):\n",
    "            ax = plt.subplot(3, 3, i + 1)\n",
    "            plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
    "            plt.title(f\"Label: {class_names[labels[i]]}\")\n",
    "            plt.axis(\"off\")\n",
    "    plt.show()\n",
    "        \n",
    "images, labels = iter(train_set).next()\n",
    "print(\"Image shape: \", images.shape)\n",
    "print(\"Labels shape: \", labels.shape)\n",
    "print_img(train_set, 9)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "11751da0",
   "metadata": {},
   "source": [
    "### **2.10. Optimisation des Performances des Données**\n",
    "\n",
    "Pour garantir un entraînement efficace et éviter les goulots d'étranglement en termes de performances, il est essentiel d'optimiser non seulement le modèle, mais aussi le traitement des données. Cette section se concentre sur l’utilisation de techniques spécifiques dans TensorFlow pour maximiser l’efficacité du flux de données pendant l’entraînement, notamment en gérant le cache, le préchargement et le mélange des données. Ces optimisations permettent de réduire les temps d’attente pendant l’entraînement tout en garantissant une meilleure généralisation du modèle.\n",
    "\n",
    "#### **2.10.1 AUTOTUNE**\n",
    "\n",
    "L’argument `AUTOTUNE` de TensorFlow permet d’ajuster dynamiquement la taille du buffer de préchargement des données. Le but est de déterminer automatiquement les paramètres optimaux pour le pipeline de données, en fonction des ressources disponibles sur la machine, comme la mémoire et les cœurs du processeur.\n",
    "\n",
    "\n",
    "#### **2.10.2. Mise en Cache des Données**\n",
    "\n",
    "L’instruction `.cache()` stocke les données prétraitées en mémoire après leur premier passage. Cela permet d’éviter de relire et de recharger les images du disque à chaque époque d’entraînement, une opération coûteuse en temps.\n",
    "\n",
    "#### **2.10.3. Mélange Aléatoire des Données**\n",
    "\n",
    "Le mélange des données avant chaque époque est un processus essentiel pour garantir que le modèle ne voit jamais les données dans un ordre fixe, ce qui améliorerait artificiellement ses performances et pourrait entraîner un surapprentissage.\n",
    "\n",
    "- **Pourquoi mélanger les données ?**\n",
    "  \n",
    "  Si les données sont toujours présentées au modèle dans le même ordre, il pourrait mémoriser des séquences spécifiques ou se concentrer de manière inégale sur certaines parties du dataset, ce qui compromettrait sa capacité à généraliser sur des données non vues. Le mélange aléatoire des données (`shuffle()`) permet d'assurer que chaque époque présente les données dans un ordre différent, réduisant ainsi la corrélation entre les échantillons successifs.\n",
    "\n",
    "- **Paramètre de taille du buffer (`1000`) :**\n",
    "  \n",
    "  Le nombre `1000` indique la taille du buffer utilisé pour mélanger les données. Ce buffer contient un sous-ensemble des données qui est mélangé avant d'être passé au modèle. Le choix de la taille du buffer est important : un buffer trop petit pourrait ne pas mélanger efficacement les données, tandis qu’un buffer trop grand pourrait être coûteux en mémoire. Le choix de `1000` est un compromis entre ces deux extrêmes, garantissant un mélange efficace sans surcharge mémoire.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "f04adf55b1dec394",
   "metadata": {},
   "source": [
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "train_set = train_set.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)\n",
    "test_set = test_set.cache().prefetch(buffer_size=AUTOTUNE)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "bf29d433",
   "metadata": {},
   "source": [
    "## **3. Modèle**\n",
    "\n",
    "Pour identifier l'architecture de modèle la plus performante, nous avons élaboré un plan d'expérience qui repose sur des ajustements progressifs du modèle initial. Cela inclut des modifications ciblées de l'architecture et des réglages minutieux des hyperparamètres clés.\n",
    "\n",
    "Chaque variante du modèle a été évaluée à l'aide de métriques de validation, puis comparée afin de déterminer laquelle généralise le mieux sur des données inédites. Ce processus itératif nous permet d'optimiser le modèle en trouvant un équilibre entre précision et complexité, garantissant ainsi des performances optimales."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a843be45",
   "metadata": {},
   "source": [
    "### **3.1. Création du modèle**\n",
    "\n",
    "Les réseaux de neurones convolutifs (CNN) sont largement reconnus comme l’architecture de référence pour les tâches de classification d'images en **deep learning**. Leur conception, qui exploite les relations spatiales entre les pixels, est parfaitement adaptée au traitement des données visuelles. Dans ce projet, la fonction `create_model` implémente un modèle CNN, optimisé pour classifier efficacement les images numérisées fournies.\n",
    "\n",
    "#### Pourquoi utiliser un CNN pour ce projet ?\n",
    "\n",
    "1. **Extraction automatique des caractéristiques**\n",
    "   Les CNN se distinguent par leur capacité à apprendre des représentations hiérarchiques directement à partir des données d'entrée, sans nécessiter de prétraitement manuel des caractéristiques. À travers les couches convolutionnelles, le modèle extrait automatiquement des motifs de plus en plus complexes : les premières couches capturent des motifs simples comme les bords ou les textures, tandis que les couches plus profondes apprennent des caractéristiques globales telles que la forme ou l'objet lui-même. Cette approche est particulièrement avantageuse pour notre projet, où les images peuvent être très variées en termes de contenu (photos, schémas, peintures, etc.).\n",
    "\n",
    "2. **Invariance aux translations et aux déformations**\n",
    "   L’un des grands avantages des CNN réside dans leur invariance spatiale. Grâce à l'utilisation de couches de **pooling** et de filtres partagés, les CNN sont capables de détecter des motifs visuels même lorsqu’ils subissent des transformations géométriques comme des rotations, des redimensionnements ou des translations. Pour ce projet, où les images numérisées peuvent être variées en termes d'orientation ou de perspective, cette capacité à identifier des motifs de manière robuste est essentielle.\n",
    "\n",
    "3. **Efficacité computationnelle**\n",
    "   Les CNN exploitent le partage de poids, ce qui permet d'optimiser le nombre de paramètres à entraîner. Contrairement aux réseaux entièrement connectés (fully connected), où chaque neurone est connecté à tous les autres, les CNN utilisent des filtres qui se déplacent localement sur l'image, réduisant ainsi la complexité. Cela permet d'entraîner des modèles plus profonds et plus puissants sans nécessiter une quantité excessive de mémoire ou de temps de calcul. Cette efficacité est critique dans un contexte de traitement à grande échelle, comme celui de ce projet.\n",
    "\n",
    "4. **Succès éprouvé dans la classification d'images**\n",
    "   Les CNN ont prouvé leur efficacité dans de nombreux benchmarks de classification d'images, y compris des jeux de données très complexes comme **ImageNet**. Les architectures CNN sont à la base des meilleures performances dans des compétitions internationales, confirmant leur capacité à généraliser sur des données visuelles variées. L’utilisation d’un CNN pour ce projet est donc justifiée pour garantir une haute précision et une robustesse dans la classification des images.\n",
    "\n",
    "#### Détails du Modèle CNN Implémenté\n",
    "\n",
    "La fonction `create_model` est structurée de la manière suivante :\n",
    "\n",
    "- **Couches Convolutionnelles (Conv2D)** : Chaque couche convolutionnelle applique plusieurs filtres qui balayent l’image pour détecter des motifs spécifiques. Les premières couches capturent des caractéristiques simples comme les bords et les textures, tandis que les couches plus profondes extraient des informations plus complexes.\n",
    "  \n",
    "- **Couches de Pooling (MaxPooling2D)** : Ces couches réduisent la dimension des cartes de caractéristiques tout en conservant les informations les plus importantes. Cela permet de diminuer le nombre de paramètres, rendant le modèle plus efficace sans perte significative de performance. De plus, le pooling introduit une invariance aux petites translations des objets dans les images.\n",
    "\n",
    "- **Couches Fully Connected (Dense)** : Ces couches, situées en fin de réseau, transforment les caractéristiques extraites par les couches convolutionnelles en un vecteur de classification. Ces couches apprennent à combiner les informations locales pour effectuer une classification globale de l'image.\n",
    "\n",
    "- **Fonctions d'activation** :\n",
    "  - **ReLU** (Rectified Linear Unit) est utilisée après chaque couche convolutionnelle. Cette fonction d'activation introduit de la non-linéarité, ce qui permet au modèle de capturer des relations complexes dans les données.\n",
    "  \n",
    "  - **Softmax** est utilisée en sortie pour convertir les prédictions en probabilités, permettant ainsi de classifier les images en fonction de leur classe.\n",
    "\n",
    "#### Choix de la Fonction de Perte : **Cross-Entropy Categorical**\n",
    "\n",
    "La **fonction de perte** utilisée pour ce modèle est la **cross-entropie catégorielle**. Elle est particulièrement adaptée aux tâches de classification multi-classes comme celle-ci, car elle mesure la distance entre la distribution des probabilités prédites par le modèle et les étiquettes réelles. En minimisant cette fonction de perte, le modèle est entraîné à améliorer la précision de ses prédictions, ajustant ses poids à chaque itération pour réduire l’erreur sur les données d'entraînement.\n",
    "\n",
    "#### Choix de l’Optimiseur : **Adam**\n",
    "\n",
    "L’optimiseur **Adam** a été sélectionné pour ce projet en raison de ses nombreux avantages par rapport aux algorithmes d'optimisation plus classiques comme **SGD (Stochastic Gradient Descent)**. Adam combine la descente de gradient avec moment et l'adaptation des taux d'apprentissage pour chaque paramètre, ce qui en fait un optimiseur efficace, en particulier dans des environnements où les gradients peuvent être bruités ou non stationnaires, comme c’est souvent le cas dans les tâches de classification d'images.\n",
    "\n",
    "- **Adaptation du taux d'apprentissage** : Contrairement à SGD qui utilise un taux d'apprentissage constant, Adam ajuste dynamiquement le taux d'apprentissage pour chaque paramètre, permettant ainsi une convergence plus rapide et plus stable.\n",
    "\n",
    "- **Résistance aux gradients bruyants** : Adam utilise deux moments (le moment de premier ordre pour la moyenne des gradients et le moment de second ordre pour la variance des gradients), ce qui permet d'atténuer l’effet des gradients bruyants ou mal estimés. Cela est particulièrement utile lorsque les données d'entraînement sont hétérogènes, comme dans le dataset de ce projet.\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "id": "3ca11573",
   "metadata": {},
   "source": [
    "def create_test_model(\n",
    "    batch_size,\n",
    "    loss_function,\n",
    "    optimizer,\n",
    "    dropout_rate,\n",
    "    regularization_value=None,\n",
    "    filter_factor=1,\n",
    "    use_data_augmentation=False,\n",
    "    add_additional_layer=False,\n",
    "    use_batch_normalization=False,\n",
    "    image_height=180,\n",
    "    image_width=180\n",
    "):    \n",
    "    # Define L2 regularizer if needed\n",
    "    regularizer = tf.keras.regularizers.l2(regularization_value) if regularization_value else None\n",
    "\n",
    "    # Initialize the model\n",
    "    model = tf.keras.models.Sequential()\n",
    "\n",
    "    # Normalize input pixels\n",
    "    model.add(tf.keras.layers.Rescaling(1./255, input_shape=(image_height, image_width, 3)))\n",
    "\n",
    "    # Add data augmentation layer if specified\n",
    "    if use_data_augmentation:\n",
    "        data_augmentation = tf.keras.Sequential([\n",
    "            tf.keras.layers.RandomFlip(\"horizontal\"),\n",
    "            tf.keras.layers.RandomRotation(0.1),\n",
    "            tf.keras.layers.RandomZoom(0.1),\n",
    "        ])\n",
    "        model.add(data_augmentation)\n",
    "\n",
    "    # First convolutional layer\n",
    "    model.add(tf.keras.layers.Conv2D(\n",
    "        filters=16 * filter_factor,\n",
    "        kernel_size=(3, 3),\n",
    "        padding=\"same\",\n",
    "        kernel_regularizer=regularizer\n",
    "    ))\n",
    "    \n",
    "    if use_batch_normalization:\n",
    "        model.add(tf.keras.layers.BatchNormalization())\n",
    "    model.add(tf.keras.layers.Activation(\"relu\"))\n",
    "    model.add(tf.keras.layers.MaxPooling2D((2, 2)))\n",
    "    model.add(tf.keras.layers.Dropout(dropout_rate))\n",
    "\n",
    "    # Second convolutional layer\n",
    "    model.add(tf.keras.layers.Conv2D(\n",
    "        filters=32 * filter_factor,\n",
    "        kernel_size=(3, 3),\n",
    "        padding=\"same\",\n",
    "        kernel_regularizer=regularizer\n",
    "    ))\n",
    "\n",
    "    if use_batch_normalization:\n",
    "        model.add(tf.keras.layers.BatchNormalization())\n",
    "    model.add(tf.keras.layers.Activation(\"relu\"))\n",
    "    model.add(tf.keras.layers.MaxPooling2D())\n",
    "    model.add(tf.keras.layers.Dropout(dropout_rate))\n",
    "\n",
    "    # Add an additional convolutional layer if specified\n",
    "    if add_additional_layer:\n",
    "        model.add(tf.keras.layers.Conv2D(\n",
    "            filters=64 * filter_factor,\n",
    "            kernel_size=(3, 3),\n",
    "            padding=\"same\",\n",
    "            kernel_regularizer=regularizer\n",
    "        ))\n",
    "\n",
    "        if use_batch_normalization:\n",
    "            model.add(tf.keras.layers.BatchNormalization())\n",
    "        model.add(tf.keras.layers.Activation(\"relu\"))\n",
    "        model.add(tf.keras.layers.MaxPooling2D())\n",
    "        model.add(tf.keras.layers.Dropout(dropout_rate))\n",
    "\n",
    "    # Flatten outputs for the dense layer\n",
    "    model.add(tf.keras.layers.Flatten())\n",
    "    \n",
    "    # Final dense layer\n",
    "    model.add(tf.keras.layers.Dense(128 * filter_factor, activation=\"relu\"))\n",
    "    model.add(tf.keras.layers.Dropout(dropout_rate))\n",
    "    model.add(tf.keras.layers.Dense(class_num, activation='softmax'))\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer=optimizer, loss=loss_function, metrics=['accuracy'])\n",
    "\n",
    "    # Display a model summary\n",
    "    model.summary()\n",
    "    \n",
    "    return model"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "b0dd800b",
   "metadata": {},
   "source": [
    "### **3.2. Définition des Callbacks**\n",
    "\n",
    "Les **callbacks** dans TensorFlow/Keras permettent d'intégrer des actions spécifiques pendant le processus d'entraînement du modèle. Dans ce projet, nous utilisons trois types de callbacks qui jouent un rôle crucial dans l'optimisation de l'entraînement et la surveillance des performances :\n",
    "\n",
    "#### 3.2.1. **ModelCheckpoint**\n",
    "\n",
    "Le **ModelCheckpoint** est utilisé pour sauvegarder périodiquement les poids du modèle. Plus précisément, il enregistre les poids du modèle après chaque epoch où une amélioration est observée sur les données de validation, en fonction d’une métrique donnée (par exemple, la perte ou l’accuracy sur l'ensemble de validation). Ce mécanisme est particulièrement important pour plusieurs raisons :\n",
    "\n",
    "- **Conserver les meilleures performances** : Seules les meilleures versions du modèle sont sauvegardées, évitant la dégradation due au surentraînement.\n",
    "\n",
    "- **Prévenir les pertes de progrès** en cas d'interruption.\n",
    "\n",
    "- **Limiter le surapprentissage** en enregistrant uniquement les poids correspondant aux meilleures performances.\n",
    "\n",
    "#### 3.2.2. **TensorBoard**\n",
    "\n",
    "Le **callback TensorBoard** permet de générer des journaux d'entraînement qui peuvent être visualisés avec l'outil **TensorBoard**. TensorBoard est un outil essentiel pour le diagnostic et la surveillance en temps réel de l’entraînement, car il offre plusieurs avantages techniques :\n",
    "\n",
    "- **Visualiser l’évolution des performances** (perte, précision) sur les données d’entraînement et de validation.\n",
    "\n",
    "- **Suivre les gradients et poids du modèle** pour détecter des anomalies comme le vanishing gradient.\n",
    "\n",
    "- **Analyser les temps d’entraînement** pour optimiser les performances.\n",
    "\n",
    "#### 3.2.3. **EarlyStopping**\n",
    "\n",
    "Le callback **EarlyStopping** est utilisé pour arrêter automatiquement l'entraînement lorsque les performances sur le jeu de validation cessent de s'améliorer après un certain nombre d'epoch consécutives (défini par le paramètre `patience`). Cette technique repose sur l'idée qu'après un certain point, continuer l'entraînement n'améliore plus les performances générales du modèle et risque même de le dégrader (surapprentissage). Les raisons principales d'utiliser EarlyStopping sont les suivantes :\n",
    "\n",
    "- **Éviter le surentraînement** et améliorer la généralisation.\n",
    "\n",
    "- **Réduire le temps d'entraînement** en stoppant avant saturation.\n",
    "\n",
    "- **Simplifier l’ajustement manuel** des époques d’entraînement.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "5cf4978e",
   "metadata": {},
   "source": [
    "def get_callbacks():    \n",
    "    # Create a callback that saves the model's weights at each epoch where validation loss improves\n",
    "    checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath=paths['checkpoint_path'] + \"/weights-epoch-{epoch:02d}-{val_loss:.2f}.weights.h5\",\n",
    "        save_weights_only=True,\n",
    "        save_best_only=True,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Create a TensorBoard callback to log training metrics, model graphs, and images for visualization\n",
    "    tensorboard_callback = tf.keras.callbacks.TensorBoard(\n",
    "        log_dir=paths['log_path'],\n",
    "        histogram_freq=1,\n",
    "        write_graph=True,\n",
    "        write_images=True,\n",
    "        update_freq='epoch',\n",
    "        profile_batch=0,\n",
    "        embeddings_freq=0\n",
    "    )\n",
    "    \n",
    "    # Set up early stopping to halt training if validation loss stops improving for a set number of epochs\n",
    "    early_callback = tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        min_delta=0,\n",
    "        patience=4,\n",
    "        verbose=0,\n",
    "        mode='auto',\n",
    "        baseline=None,\n",
    "        restore_best_weights=False,\n",
    "        start_from_epoch=0\n",
    "    )\n",
    "\n",
    "    return [checkpoint_callback, tensorboard_callback, early_callback]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "fdf8d800",
   "metadata": {},
   "source": [
    "Nous traçons des graphiques pour visualiser les performances du modèle pendant l'entraînement. Cela inclut l'évolution de l'accuracy et de la perte, à la fois sur les données d'entraînement et de validation, au fil des epoch. Ces graphiques nous aident à comprendre comment le modèle s'améliore et à identifier d'éventuels problèmes comme le surapprentissage."
   ]
  },
  {
   "cell_type": "code",
   "id": "44caf3fd",
   "metadata": {},
   "source": [
    "def plot_training_history(history, file_name):\n",
    "\n",
    "    plt.figure(figsize=(12, 4))\n",
    "\n",
    "    # Accuracy plot\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "    plt.title('Model Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "    # Loss plot\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['loss'], label='Training Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.title('Model Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save the plot to the specified path\n",
    "    plt.savefig(f\"{paths['plot_test_plan']}/{file_name}\")\n",
    "\n",
    "    # Display the plot\n",
    "    plt.show()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "6f689e56",
   "metadata": {},
   "source": [
    "### **3.3. Évolution du Modèle : Une Optimisation Itérative et Approfondie**\n",
    "\n",
    "#### 3.3.1. **Modèle Initial : Une Architecture de Base pour Réseaux Convolutifs**\n",
    "\n",
    "Le modèle initial était conçu comme un réseau de neurones convolutifs (CNN) standard, structuré pour effectuer la classification d’images. Il comprenait les composants suivants :\n",
    "\n",
    "- **Deux couches convolutionnelles**, chacune suivie d’une couche de **MaxPooling**. Les filtres étaient de taille croissante (16 et 32), permettant d'extraire des caractéristiques de plus en plus complexes à chaque couche.\n",
    "- **Dropout modéré fixé à 0.2** après chaque couche de pooling, utilisé comme une méthode de régularisation élémentaire pour éviter le sur-apprentissage.\n",
    "- **Pas de régularisation explicite (L2 ou L1)** au niveau des poids, ce qui laissait le modèle libre d'apprendre sans contrainte, mais le rendait vulnérable au sur-apprentissage avec l’augmentation de la complexité."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0161a595",
   "metadata": {},
   "source": [
    "Les schémas ci-dessous illustre cette architecture :\n",
    "![initial_model_1.png](./schemas/initial_model_1.png)\n",
    "![initial_model_2.png](./schemas/initial_model_2.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "22dbb3b1",
   "metadata": {},
   "source": [
    "# Define the loss function\n",
    "loss_function = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False)\n",
    "\n",
    "# Create the model and get the datasets\n",
    "first_model = create_test_model(\n",
    "    batch_size=batch_s,\n",
    "    loss_function=loss_function,\n",
    "    optimizer='adam',  # Choosing an optimizer, here Adam\n",
    "    dropout_rate=0.2,  # Dropout rate for slight regularization\n",
    "    regularization_value=None,  # No regularization\n",
    "    filter_factor=1,  # Default filter factor\n",
    "    use_data_augmentation=False,  # No data augmentation for this first trial\n",
    "    add_additional_layer=False,  # Only two layers\n",
    "    use_batch_normalization=False,  # No Batch Normalization for the first model\n",
    "    image_height=image_h,\n",
    "    image_width=image_w\n",
    ")\n",
    "# Train the model\n",
    "epochs = 10\n",
    "\n",
    "history_first_model = first_model.fit(\n",
    "    train_set,\n",
    "    validation_data=test_set,\n",
    "    epochs=epochs,\n",
    "    callbacks=get_callbacks()\n",
    ")\n",
    "\n",
    "# Visualize the model's performance\n",
    "plot_training_history(history_first_model, \"1_initial_model.png\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "![1_initial_model.png](./test_plan/1_initial_model.png)",
   "id": "779393c92c65f920"
  },
  {
   "cell_type": "markdown",
   "id": "da3099ce",
   "metadata": {},
   "source": [
    "#### 3.3.2. **Modèle à 3 couches : Vers une architecture plus complexe**\n",
    "\n",
    "Le second modèle introduit une complexité supplémentaire par rapport au modèle initial, avec les ajustements suivants :\n",
    "\n",
    "- **Trois couches convolutionnelles**, chacune suivie d’une couche de **MaxPooling**. Les filtres ont des tailles croissantes (16, 32, puis 64), ce qui permet d'extraire des caractéristiques de plus en plus complexes au fil des couches.\n",
    "- **Un Dropout modéré fixé à 0.2** après chaque couche de pooling, utilisé comme une technique de régularisation pour limiter le surapprentissage.\n",
    "- **Toujours aucune régularisation explicite (L2 ou L1)** au niveau des poids, mais l'augmentation de la profondeur du réseau apporte davantage de capacités d'apprentissage, tout en augmentant les risques de surapprentissage si le modèle n'est pas bien contrôlé."
   ]
  },
  {
   "cell_type": "code",
   "id": "657f51f5",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Define the loss function\n",
    "loss_function = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False)\n",
    "\n",
    "# Create the second model with an additional third layer\n",
    "second_model = create_test_model(\n",
    "    batch_size=batch_s,\n",
    "    loss_function=loss_function,\n",
    "    optimizer='adam',\n",
    "    dropout_rate=0.2,\n",
    "    regularization_value=None,\n",
    "    filter_factor=1,\n",
    "    use_data_augmentation=False,\n",
    "    add_additional_layer=True,\n",
    "    use_batch_normalization=False,\n",
    "    image_height=image_h,\n",
    "    image_width=image_w\n",
    ")\n",
    "\n",
    "# Train the second model\n",
    "epochs = 10\n",
    "\n",
    "history_second_model = second_model.fit(\n",
    "    train_set,\n",
    "    validation_data=test_set,\n",
    "    epochs=epochs,\n",
    "    callbacks=get_callbacks()\n",
    ")\n",
    "\n",
    "# Visualize the performance of the second model\n",
    "plot_training_history(history_second_model, \"2_extra_layer_model.png\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "![2_extra_layer_model.png](./test_plan/2_extra_layer_model.png)",
   "id": "1468dac9a2dbe8fc"
  },
  {
   "cell_type": "markdown",
   "id": "7ea5802d",
   "metadata": {},
   "source": [
    "#### 3.3.3. **Augmentation des Données**\n",
    "\n",
    "L’augmentation des données (**data augmentation**) est une technique essentielle pour améliorer la robustesse et la capacité de généralisation du modèle, en particulier lorsque le dataset est de taille limitée ou que certaines classes sont sous-représentées. Cette méthode repose sur l'application de transformations aléatoires aux images d'entraînement, créant ainsi des variantes artificielles tout en conservant les propriétés essentielles des données. Cela permet au modèle d’apprendre à identifier les caractéristiques des images dans des conditions variées et réalistes.\n",
    "\n",
    "#### Pourquoi l'augmentation des données est-elle nécessaire ?\n",
    "\n",
    "1. **Amélioration de la généralisation**\n",
    "   \n",
    "   Un des principaux défis en deep learning est d'assurer que le modèle ne se contente pas de mémoriser les données d'entraînement, mais qu'il puisse aussi généraliser à des données qu’il n’a jamais vues. L'augmentation des données simule cette variabilité en créant des versions modifiées des images existantes, ce qui augmente virtuellement la taille du dataset et diversifie les exemples que le modèle voit durant l’entraînement.\n",
    "\n",
    "2. **Prévention du surapprentissage (overfitting)**\n",
    "   \n",
    "   Lorsque le dataset est limité, il est facile pour un modèle puissant comme un CNN de surapprendre les détails spécifiques des images d'entraînement, conduisant à une mauvaise performance sur les données de validation ou en production. En introduisant des transformations aléatoires, l’augmentation des données rend le modèle moins susceptible de mémoriser les exemples spécifiques et l'oblige à apprendre des représentations plus robustes et généralisables.\n",
    "\n",
    "3. **Enrichissement du dataset**\n",
    "   \n",
    "   L'augmentation des données permet d'introduire une diversité dans les images, en simulant des conditions que le modèle pourrait rencontrer dans des situations réelles, comme des variations de lumière, des rotations, des zooms, etc. Cela est particulièrement utile dans le cas d'un dataset hétérogène ou lorsque certaines classes d'images sont peu représentées.\n",
    "\n",
    "#### Techniques Utilisées pour l'Augmentation des Données\n",
    "\n",
    "Dans ce projet, plusieurs techniques d'augmentation des données sont mises en œuvre, chacune ayant pour objectif de simuler des transformations réalistes des images tout en préservant leur signification sémantique :\n",
    "\n",
    "1. **RandomFlip (Renversement horizontal)**\n",
    "   \n",
    "   Le renversement horizontal est l'une des méthodes d'augmentation les plus simples et les plus efficaces. Il permet de doubler virtuellement la taille du dataset en générant des versions symétriques des images. Cette technique est particulièrement utile dans les tâches où l'orientation gauche/droite d'un objet ou d'une scène ne change pas la classe à laquelle il appartient (par exemple, des paysages ou des bâtiments). En introduisant des images renversées, le modèle apprend à être moins sensible à la symétrie des objets, améliorant ainsi sa capacité de généralisation.\n",
    "\n",
    "2. **RandomRotation (Rotation aléatoire)**\n",
    "   \n",
    "   La rotation aléatoire des images, dans une plage de ±10%, permet de rendre le modèle invariant à l'orientation des objets. Ceci est crucial dans un contexte de classification d'images, où l'angle de prise de vue peut varier. La rotation permet au modèle de reconnaître les objets sous des angles différents, ce qui est essentiel dans des scénarios où les images peuvent ne pas être parfaitement alignées (comme dans le cas de documents numérisés de manière non uniforme).\n",
    "\n",
    "3. **RandomZoom (Zoom aléatoire)**\n",
    "   \n",
    "   Le zoom aléatoire de ±10% permet de simuler des variations dans la distance entre l'objet et la caméra. Cela aide le modèle à apprendre à reconnaître les objets ou les motifs à différentes échelles, en renforçant sa robustesse face à des images où l'objet occupe plus ou moins d’espace dans le cadre. Cette transformation est particulièrement utile pour éviter que le modèle ne devienne trop dépendant d'une certaine échelle lors de l'apprentissage.\n",
    "\n",
    "#### Avantages de l'augmentation des données utilisée\n",
    "\n",
    "Les trois techniques d'augmentation utilisées dans ce projet visent à enrichir le dataset en simulant des variations courantes dans des contextes réels, tout en maintenant la pertinence sémantique des images. L'augmentation des données est essentielle pour :\n",
    "\n",
    "- **Réduire le surapprentissage** : En créant des images modifiées à partir des exemples d'entraînement, le modèle apprend à reconnaître les objets dans diverses conditions, ce qui réduit la dépendance à des exemples spécifiques et améliore sa capacité de généralisation.\n",
    "  \n",
    "- **Améliorer la robustesse aux transformations** : Les transformations appliquées (flip, rotation, zoom) simulent des conditions réalistes que le modèle pourrait rencontrer en production, rendant le modèle plus robuste aux variations d'orientation et d'échelle dans les images.\n",
    "\n",
    "- **Augmenter virtuellement la taille du dataset** : Avec seulement trois transformations, le dataset est virtuellement agrandi, ce qui permet au modèle de s’entraîner sur un ensemble de données plus diversifié sans avoir besoin de collecter des images supplémentaires."
   ]
  },
  {
   "cell_type": "code",
   "id": "31bbdafe",
   "metadata": {},
   "source": [
    "data_augmentation = tf.keras.Sequential(\n",
    "    [\n",
    "        tf.keras.layers.RandomFlip(\"horizontal\"),\n",
    "        tf.keras.layers.RandomRotation(0.1),\n",
    "        tf.keras.layers.RandomZoom(0.1),\n",
    "    ]\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "07538104",
   "metadata": {},
   "source": [
    "# Define the loss function\n",
    "loss_function = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False)\n",
    "\n",
    "# Create the third model with an additional third layer and data augmentation\n",
    "third_model = create_test_model(\n",
    "    batch_size=batch_s,\n",
    "    loss_function=loss_function,\n",
    "    optimizer='adam',\n",
    "    dropout_rate=0.2,\n",
    "    regularization_value=None,\n",
    "    filter_factor=1,\n",
    "    use_data_augmentation=True,\n",
    "    add_additional_layer=True,\n",
    "    use_batch_normalization=False,\n",
    "    image_height=image_h,\n",
    "    image_width=image_w\n",
    ")\n",
    "\n",
    "# Train the third model\n",
    "epochs = 10\n",
    "\n",
    "history_third_model = third_model.fit(\n",
    "    train_set,\n",
    "    validation_data=test_set,\n",
    "    epochs=epochs,\n",
    "    callbacks=get_callbacks()\n",
    ")\n",
    "\n",
    "# Visualize the performance of the third model\n",
    "plot_training_history(history_third_model, \"3_data_augmentation_model.png\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "![3_data_augmentation_model.png](./test_plan/3_data_augmentation_model.png)",
   "id": "756beb55fa8aef31"
  },
  {
   "cell_type": "markdown",
   "id": "53db31a6",
   "metadata": {},
   "source": [
    "#### 3.3.4. ****Introduction de Batch Normalization : Un Impact Mitigé****\n",
    "\n",
    "L'une des premières optimisations apportées fut l'ajout de ****Batch Normalization**** après chaque couche convolutionnelle. Cette technique est généralement utilisée pour stabiliser et accélérer l'apprentissage, notamment en :\n",
    "\n",
    "- ****Normalisant les activations*** au sein des couches, ce qui permet de réduire la sensibilité aux variations de distribution des données internes.\n",
    "- ****Permettant des taux d’apprentissage plus élevés*** tout en améliorant la convergence.\n",
    "\n",
    "Cependant, dans ce cas précis, les performances de validation ont montré des ****fluctuations plus importantes**** après l'ajout de Batch Normalization, avec une accuracy qui devenait instable à partir de certaines epoch. Les fluctuations observées indiquaient que, bien que la normalisation ait amélioré l'apprentissage sur le set d'entraînement, elle a également rendu l'apprentissage plus instable sur le set de validation. Ces oscillations ont suggéré que la normalisation seule ne suffisait pas à stabiliser l'ensemble du processus d'entraînement."
   ]
  },
  {
   "cell_type": "code",
   "id": "9ea0c908",
   "metadata": {},
   "source": [
    "# Define the loss function\n",
    "loss_function = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False)\n",
    "\n",
    "# Create the fourth model with an additional third layer, data augmentation, and Batch Normalization\n",
    "fourth_model = create_test_model(\n",
    "    batch_size=batch_s,\n",
    "    loss_function=loss_function,\n",
    "    optimizer='adam',\n",
    "    dropout_rate=0.2,\n",
    "    regularization_value=None,\n",
    "    filter_factor=1,\n",
    "    use_data_augmentation=True,\n",
    "    add_additional_layer=True, \n",
    "    use_batch_normalization=True,\n",
    "    image_height=image_h,\n",
    "    image_width=image_w\n",
    ")\n",
    "\n",
    "# Train the fourth model\n",
    "epochs = 10\n",
    "\n",
    "history_fourth_model = fourth_model.fit(\n",
    "    train_set,\n",
    "    validation_data=test_set,\n",
    "    epochs=epochs,\n",
    "    callbacks=get_callbacks()\n",
    ")\n",
    "\n",
    "# Visualize the performance of the fourth model\n",
    "plot_training_history(history_fourth_model, \"4_batch_normalization_model.png\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "caeda88f7a7f602d",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "343b09c1",
   "metadata": {},
   "source": [
    "#### 3.3.5. ****Ajout de la Régularisation L2 : Un Effet Temporaire****\n",
    "\n",
    "Pour mieux contrôler la complexité du modèle, une ****régularisation L2**** (λ = 0.001) a été ajoutée aux couches convolutionnelles. La régularisation L2 est souvent utilisée pour pénaliser les poids excessifs et favoriser une généralisation plus stable. Cependant, dans notre cas, bien que cette régularisation ait permis une légère amélioration de l'accuracy de validation dans les premières epoch, elle n'a pas permis de stabiliser complètement le modèle :\n",
    "\n",
    "- ****Fluctuations persistantes*** : Après plusieurs epoch, les performances sur le set de validation ont continué à osciller, avec des chutes et des hausses inattendues. Cela indiquait que la régularisation L2 ne corrigeait pas entièrement les problèmes de stabilité observés, et en augmentant la régularisation (λ = 0.002), le modèle a même vu ses performances freiner.\n",
    "\n",
    "Les tests ont montré que la ****régularisation L2****, bien qu'efficace dans certains contextes, n'était pas nécessairement adaptée pour ce modèle et ce dataset spécifique. Cela a conduit à la décision de retirer progressivement cette régularisation dans les versions ultérieures du modèle."
   ]
  },
  {
   "cell_type": "code",
   "id": "ab6e57e5",
   "metadata": {},
   "source": [
    "# Define the loss function\n",
    "loss_function = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False)\n",
    "\n",
    "# Define the L2 regularization value\n",
    "regularization_value = 0.001 \n",
    "\n",
    "# Create the fifth model with L2 regularization\n",
    "fifth_model = create_test_model(\n",
    "    batch_size=batch_s,\n",
    "    loss_function=loss_function,\n",
    "    optimizer='adam',\n",
    "    dropout_rate=0.2,\n",
    "    regularization_value=regularization_value, \n",
    "    filter_factor=1,\n",
    "    use_data_augmentation=True, \n",
    "    add_additional_layer=True, \n",
    "    use_batch_normalization=True, \n",
    "    image_height=image_h,\n",
    "    image_width=image_w\n",
    ")\n",
    "\n",
    "# Train the fifth model\n",
    "epochs = 10\n",
    "\n",
    "history_fifth_model = fifth_model.fit(\n",
    "    train_set,\n",
    "    validation_data=test_set,\n",
    "    epochs=epochs,\n",
    "    callbacks=get_callbacks()\n",
    ")\n",
    "\n",
    "# Visualize the performance of the fifth model\n",
    "plot_training_history(history_fifth_model, \"5_regularization_model.png\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "1440c4ee",
   "metadata": {},
   "source": [
    "#### 3.3.6. ****Introduction de ReduceLROnPlateau : Une Amélioration de la Convergence****\n",
    "\n",
    "Pour résoudre les problèmes de fluctuation récurrents, nous avons introduit le callback ****ReduceLROnPlateau****. Ce mécanisme permet de réduire dynamiquement le taux d'apprentissage lorsque les performances de validation stagnent ou se détériorent, afin de permettre une convergence plus stable. Ce callback a permis :\n",
    "\n",
    "- ***De réduire les oscillations*** : En ajustant automatiquement le taux d’apprentissage, le modèle a pu ajuster plus finement ses poids, ce qui a contribué à diminuer les fluctuations observées précédemment.\n",
    "- ***Amélioration de la convergence à long terme*** : Le modèle a montré une meilleure stabilité dans les dernières epoch d’entraînement, notamment grâce à des ajustements plus progressifs des poids.\n",
    "\n",
    "L’introduction de ce callback a permis de renforcer la robustesse du modèle et de mieux gérer les moments où l’apprentissage devenait instable."
   ]
  },
  {
   "cell_type": "code",
   "id": "d097779b",
   "metadata": {},
   "source": [
    "# Define the loss function\n",
    "loss_function = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False)\n",
    "\n",
    "# Define the L2 regularization value\n",
    "regularization_value = 0.001\n",
    "\n",
    "# Create the sixth model with L2 regularization, data augmentation, and Batch Normalization\n",
    "sixth_model = create_test_model(\n",
    "    batch_size=batch_s,\n",
    "    loss_function=loss_function,\n",
    "    optimizer='adam',\n",
    "    dropout_rate=0.2,\n",
    "    regularization_value=regularization_value,\n",
    "    filter_factor=1,\n",
    "    use_data_augmentation=True,\n",
    "    add_additional_layer=True,\n",
    "    use_batch_normalization=True,\n",
    "    image_height=image_h,\n",
    "    image_width=image_w\n",
    ")\n",
    "\n",
    "# Define the ReduceLROnPlateau callback\n",
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_loss',      # Monitor validation loss\n",
    "    factor=0.5,              # Reduce learning rate by half\n",
    "    patience=2,              # Wait for 2 epochs before reducing the rate\n",
    "    min_lr=1e-6,             # Minimum learning rate\n",
    "    verbose=1                # Display a message when learning rate is reduced\n",
    ")\n",
    "\n",
    "# Train the sixth model with the existing callbacks and ReduceLROnPlateau\n",
    "epochs = 10\n",
    "\n",
    "history_sixth_model = sixth_model.fit(\n",
    "    train_set,\n",
    "    validation_data=test_set,\n",
    "    epochs=epochs,\n",
    "    callbacks=get_callbacks() + [reduce_lr]  # Add ReduceLROnPlateau to callbacks\n",
    ")\n",
    "\n",
    "# Visualize the performance of the sixth model\n",
    "plot_training_history(history_sixth_model, \"6_reduce_lr_model.png\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "4937555e",
   "metadata": {},
   "source": [
    "#### 3.3.7. **Augmentation de la Complexité et Ajustement des Paramètres**\n",
    "\n",
    "Après avoir stabilisé le processus d'apprentissage grâce à ReduceLROnPlateau, nous avons exploré une augmentation progressive de la complexité du modèle :\n",
    "\n",
    "- **Doublement du nombre de filtres** à chaque couche convolutionnelle : Nous sommes passés de 16, 32, 64 filtres à 32, 64, 128, ce qui a permis d'extraire des caractéristiques plus riches et complexes à chaque étape du réseau.\n",
    "- **Ajustement du Dropout à 0.4** : Nous avons testé un Dropout plus élevé pour renforcer la régularisation. Cependant, un Dropout trop important a limité la capacité d'apprentissage, entraînant une dégradation des performances.\n",
    "\n",
    "Ces ajustements ont permis une amélioration à court terme de l'apprentissage, mais il est apparu qu'une régularisation excessive (Dropout ou L2) ralentissait l'apprentissage sans améliorer significativement la généralisation."
   ]
  },
  {
   "cell_type": "code",
   "id": "9ae16d35",
   "metadata": {},
   "source": [
    "# Define the loss function\n",
    "loss_function = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False)\n",
    "\n",
    "# Define the L2 regularization value\n",
    "regularization_value = 0.001\n",
    "\n",
    "# Create the seventh model with doubled filters and a dropout rate of 0.4\n",
    "seventh_model = create_test_model(\n",
    "    batch_size=batch_s,\n",
    "    loss_function=loss_function,\n",
    "    optimizer='adam',\n",
    "    dropout_rate=0.4,\n",
    "    regularization_value=regularization_value,\n",
    "    filter_factor=2,\n",
    "    use_data_augmentation=True,\n",
    "    add_additional_layer=True,\n",
    "    use_batch_normalization=True,\n",
    "    image_height=image_h,\n",
    "    image_width=image_w\n",
    ")\n",
    "\n",
    "# Train the seventh model with the existing callbacks and ReduceLROnPlateau\n",
    "epochs = 10\n",
    "\n",
    "history_seventh_model = seventh_model.fit(\n",
    "    train_set,\n",
    "    validation_data=test_set,\n",
    "    epochs=epochs,\n",
    "    callbacks=get_callbacks() + [reduce_lr]\n",
    ")\n",
    "\n",
    "# Visualize the performance of the seventh model\n",
    "plot_training_history(history_seventh_model, \"7_filter_augmentation_model.png\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "889c7c8f",
   "metadata": {},
   "source": [
    "#### 3.3.8. ****Modèle Final : Une Simplicité Retrouvée****\n",
    "\n",
    "Le modèle final a finalement abandonné la ****régularisation L2**** et est revenu à un ****Dropout plus modéré de 0.2****, tout en conservant une architecture plus complexe avec des filtres plus nombreux (32, 64, 128) et un ****Dense Layer de 256 unités**** avant la sortie. Ce modèle présente les caractéristiques suivantes :\n",
    "\n",
    "- ***Trois couches convolutionnelles*** accompagnées de MaxPooling pour extraire des caractéristiques tout en réduisant la dimensionnalité.\n",
    "- ***Batch Normalization maintenu***, malgré les fluctuations initiales, pour accélérer l’apprentissage tout en minimisant les gradients instables.\n",
    "- ***Dropout à 0.2***, suffisant pour régulariser le modèle sans sacrifier sa capacité à bien apprendre.\n",
    "\n",
    "Ce modèle final a montré une ****accuracy de validation stable à 88.8%****, tout en conservant une convergence régulière, sans les oscillations observées dans les versions intermédiaires."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00bac3c2",
   "metadata": {},
   "source": [
    "Les schémas ci-dessous illustre l'architecture finale de notre modèle :\n",
    "\n",
    "![final_model_1.png](./schemas/final_model_1.png)\n",
    "![final_model_2.png](./schemas/final_model_2.png)"
   ]
  },
  {
   "cell_type": "code",
   "id": "cb4a2c8443a5b4de",
   "metadata": {},
   "source": [
    "batch_size = 24\n",
    "image_height = 224\n",
    "image_width = 224\n",
    "\n",
    "train_set, test_set = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    paths['train_data_path'],\n",
    "    validation_split=0.2,\n",
    "    subset=\"both\",\n",
    "    seed=42,\n",
    "    image_size=(image_h, image_w),\n",
    "    batch_size=batch_s,\n",
    "    labels=\"inferred\",\n",
    "    label_mode=\"int\"\n",
    ")\n",
    "\n",
    "train_set = train_set.map(preprocess_image)\n",
    "test_set = test_set.map(preprocess_image)\n",
    "\n",
    "train_set = train_set.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)\n",
    "test_set = test_set.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "def create_model(input_shape=(image_h, image_w, 3)):\n",
    "    model = tf.keras.models.Sequential()\n",
    "\n",
    "    model.add(tf.keras.layers.Rescaling(1./255, input_shape=input_shape))\n",
    "\n",
    "    model.add(data_augmentation)\n",
    "\n",
    "    model.add(tf.keras.layers.Conv2D(filters=32, kernel_size=(3, 3), activation=\"relu\", padding=\"same\"))\n",
    "    model.add(tf.keras.layers.MaxPooling2D((2, 2)))\n",
    "    model.add(tf.keras.layers.Dropout(0.2))\n",
    "\n",
    "\n",
    "    model.add(tf.keras.layers.Conv2D(filters=64, kernel_size=3, padding=\"same\", activation=\"relu\"))\n",
    "    model.add(tf.keras.layers.MaxPooling2D())\n",
    "    model.add(tf.keras.layers.Dropout(0.2))\n",
    "\n",
    "    model.add(tf.keras.layers.Conv2D(filters=128, kernel_size=3, padding=\"same\", activation=\"relu\"))\n",
    "    model.add(tf.keras.layers.MaxPooling2D())\n",
    "    model.add(tf.keras.layers.Dropout(0.2))\n",
    "\n",
    "    model.add(tf.keras.layers.Flatten())\n",
    "\n",
    "    model.add(tf.keras.layers.Dense(256, activation=\"relu\"))\n",
    "    model.add(tf.keras.layers.Dropout(0.2))\n",
    "    model.add(tf.keras.layers.Dense(class_num, activation='softmax'))\n",
    "\n",
    "    loss_function = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False)\n",
    "    model.compile(optimizer=\"adam\", loss=loss_function, metrics=['accuracy'])\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    return model\n",
    "\n",
    "model = create_model()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "73603632",
   "metadata": {},
   "source": [
    "#### ****Conclusion****\n",
    "\n",
    "Le processus d'évolution du modèle, du modèle initial à la version finale, a été guidé par une série d’optimisations itératives visant à stabiliser l'apprentissage tout en maximisant la généralisation. Les étapes d'ajout de ****Batch Normalization**** et de ****régularisation L2**** ont introduit des fluctuations qui ont nécessité l'intervention de techniques plus avancées, comme ****ReduceLROnPlateau****. En fin de compte, le modèle final a trouvé un équilibre optimal sans régularisation explicite mais avec un Dropout modéré et une architecture plus complexe, conduisant à des performances stables et robustes sur le set de validation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea6732d7",
   "metadata": {},
   "source": [
    "### **3.4. Entraînement du modèle**\n",
    "\n",
    "Maintenant que le modèle est défini et compilé, nous pouvons procéder à l'entraînement. Dans cette étape, nous entraînons le modèle en utilisant le dataset d'entraînement tout en évaluant ses performances sur le dataset de validation à chaque époque. Nous utilisons également les callbacks définis précédemment pour sauvegarder les meilleurs poids, suivre l'entraînement avec TensorBoard, et arrêter l'entraînement de manière anticipée si nécessaire.\n",
    "\n",
    "Les paramètres d'entraînement :\n",
    "\n",
    "- **Nombre d'epochs** : Le modèle est entraîné pendant 10 epoch, mais cela peut être ajusté en fonction des besoins du projet et des performances observées.\n",
    "- **Callbacks** : Les callbacks définis précédemment (sauvegarde des poids, TensorBoard, et EarlyStopping) sont activés pendant l'entraînement pour surveiller et optimiser le processus.\n",
    "- **Sauvegarde du modèle** : Une fois l'entraînement terminé, le modèle complet est sauvegardé sous forme de fichier `.keras`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "5c90452c68120684",
   "metadata": {},
   "source": [
    "def run_model(model, epochs, load_path=None):\n",
    "    if load_path is not None:\n",
    "        model.load_weights(load_path)\n",
    "        return False\n",
    "    else:\n",
    "        history = model.fit(\n",
    "            train_set,\n",
    "            epochs=epochs, \n",
    "            validation_data=test_set,\n",
    "            callbacks=get_callbacks()\n",
    "        )\n",
    "        model.save(f\"{paths['model_path']}/model.keras\")\n",
    "        return history\n",
    "\n",
    "history = run_model(\n",
    "    model,\n",
    "    epochs=50\n",
    ")\n",
    "\n",
    "plot_training_history(history, \"8_final_model.png\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "da83facd",
   "metadata": {},
   "source": [
    "Les résultats montrent une amélioration continue de la précision sur les données d'entraînement au fil des epoch, ce qui indique que le modèle apprend bien les caractéristiques des données. Cependant, la précision de validation commence à stagner après l’époque 5, suggérant un début de sur-apprentissage. L’utilisation de techniques de régularisation supplémentaires, comme l’ajout de dropout ou l’augmentation des données, pourrait permettre d’atténuer ce phénomène."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3.5. Tri des images en fonction des prédictions**\n",
    "\n",
    "Cette section décrit une procédure automatisée de tri des images, basée sur les prédictions du modèle entraîné.\n",
    "\n",
    "1. **Prétraitement des images** : La fonction `preprocess_image` ajuste la taille des images (224x224 pixels) et les convertit en un format exploitable par le modèle pour la prédiction.\n",
    "   \n",
    "2. **Prédiction de la classe \"photo\"** : La fonction `is_photo` fait appel au modèle pour prédire si une image appartient à la classe \"photo\". Elle retourne un booléen indiquant si c'est le cas.\n",
    "\n",
    "3. **Tri des images** : La fonction `sort_images` parcourt un dossier d'images non triées, détermine pour chaque image si elle est une \"photo\", et la copie dans un dossier dédié si la prédiction est positive.\n",
    "\n",
    "Cette méthode permet de trier efficacement les images selon leur classe prédite par le modèle, assurant un classement automatique sans intervention manuelle."
   ],
   "id": "7a819fc26bcfa6a"
  },
  {
   "cell_type": "code",
   "id": "ae43ded273cfb5f9",
   "metadata": {},
   "source": [
    "def preprocess_image(img_path):\n",
    "    img = tf.keras.preprocessing.image.load_img(img_path, target_size=(image_h, image_w))\n",
    "    img_array = tf.keras.preprocessing.image.img_to_array(img)\n",
    "    img_array = tf.expand_dims(img_array, 0)\n",
    "    return img_array\n",
    "\n",
    "def is_photo(model, img_path):\n",
    "    img = preprocess_image(img_path)\n",
    "    prediction = model.predict(img)\n",
    "    class_name = class_names[np.argmax(prediction)]\n",
    "    return class_name == \"photo\"\n",
    "\n",
    "def sort_images(model):\n",
    "    number_photos = 0\n",
    "    for img_file in os.listdir(paths['unsorted_validation_data_path']):\n",
    "        img_path = os.path.join(paths['unsorted_validation_data_path'], img_file)\n",
    "\n",
    "        if os.path.isfile(img_path):\n",
    "            if is_photo(model, img_path):\n",
    "                number_photos += 1\n",
    "                shutil.copy2(img_path, f\"{paths['sorted_validation_data_path']}/{img_file}\")\n",
    "                print(f\"Image {img_file} detected as a photo. Copied to the photo folder.\")\n",
    "            else:\n",
    "                print(f\"Image {img_file} not detected as a photo.\")\n",
    "    print(f\"Total number of photos detected: {number_photos}\")\n",
    "\n",
    "                \n",
    "sort_images(model)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "fc3597e9",
   "metadata": {},
   "source": [
    "### Conclusion du projet de classification d'images\n",
    "\n",
    "Ce projet de classification d'images nous a permis d'explorer l'ensemble du processus de traitement des données et de création d'un modèle performant, de la préparation des données à l'entraînement et à l'évaluation. Nous avons commencé par préparer des données de manière rigoureuse, en utilisant différentes bibliothèques pour gérer les fichiers, créer des visualisations, et garantir la qualité des données. Cela a posé des bases solides pour la création d'un modèle de réseau de neurones efficace.\n",
    "\n",
    "Lors de la construction du modèle, nous avons conçu une architecture composée de plusieurs couches de convolution et de pooling, avec l'ajout de techniques de régularisation comme le dropout pour limiter le surapprentissage. Le modèle a été entraîné sur plusieurs epochs, et les résultats ont montré une amélioration continue de la précision sur les données d'entraînement, bien que la précision de validation ait commencé à stagner, suggérant un début de sur-apprentissage. Cela met en évidence l'importance d'utiliser des techniques de régularisation plus avancées, telles que l'augmentation de données, pour améliorer la généralisation.\n",
    "\n",
    "Ce projet illustre les défis inhérents à la construction de modèles de classification d'images, notamment en ce qui concerne l'équilibre entre l'apprentissage et la généralisation. Nous avons obtenu des résultats prometteurs, mais il reste des pistes d'amélioration, telles que l'optimisation des hyperparamètres et l'utilisation de modèles préentraînés plus avancés, pour continuer à améliorer la performance du modèle. Nous espérons que ces résultats serviront de base pour des travaux futurs, avec des possibilités d'application dans des contextes réels et diversifiés."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf3a3e39",
   "metadata": {},
   "source": [
    "### Conclusion du projet de classification d'images\n",
    "\n",
    "Ce projet de classification d'images nous a permis d'explorer l'ensemble du processus de traitement des données et de création d'un modèle performant. Cela inclut la préparation des données, l'entraînement et l'évaluation du modèle. Nous avons commencé par préparer des données de manière rigoureuse, en utilisant différentes bibliothèques pour gérer les fichiers, créer des visualisations, et garantir la qualité des données. Ces étapes ont posé des bases solides pour la création d'un modèle de réseau de neurones efficace, en garantissant la qualité et la pertinence des données utilisées.\n",
    "\n",
    "Lors de la construction du modèle, nous avons conçu une architecture composée de plusieurs couches de convolution et de pooling, avec l'ajout de techniques de régularisation comme le dropout, qui est efficace pour réduire le surapprentissage en désactivant aléatoirement certaines unités du réseau lors de l'entraînement. Le modèle a été entraîné sur plusieurs epochs, et les résultats ont montré une amélioration continue de la précision sur les données d'entraînement, bien que la précision de validation ait commencé à stagner, suggérant un début de sur-apprentissage. Cela met en évidence l'importance d'utiliser des techniques de régularisation plus avancées, telles que l'augmentation de données (par exemple, des transformations aléatoires des images), pour améliorer la généralisation.\n",
    "\n",
    "Ce projet illustre les défis inhérents à la construction de modèles de classification d'images, notamment en ce qui concerne l'équilibre entre l'apprentissage et la généralisation. Nous avons obtenu des résultats prometteurs, mais il reste des pistes d'amélioration, telles que l'optimisation des hyperparamètres pour affiner les performances du modèle, et l'utilisation de modèles préentraînés plus avancés pour bénéficier des avancées récentes en vision par ordinateur. Nous espérons que ces résultats serviront de base pour des travaux futurs, avec des possibilités d'application dans des contextes réels et diversifiés."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
